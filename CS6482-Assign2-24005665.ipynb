{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d182b1f",
   "metadata": {},
   "source": [
    "# Name and ID: Hoang Tu Bui - 24005665\n",
    "\n",
    "# The code executes till the end without any errors\n",
    "\n",
    "# Original source: \n",
    "\n",
    "1. https://medium.com/@liyinxuan0213/step-by-step-double-deep-q-networks-double-dqn-tutorial-from-atari-games-to-bioengineering-dec7e6373896\n",
    "1. https://stackoverflow.com/questions/26587527/cite-a-paper-using-github-markdown-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38889d05",
   "metadata": {},
   "source": [
    "# Why Reinforcement Learning is the ML paradigm of choice for this task?\n",
    "\n",
    "Reinforcement Learning (RL) is well-suited for Atari games like Breakout where an agent must learn to make decisions through trial and error, optimizing its actions based on feedback in the form of rewards. This paradigm is distinct from supervised learning, which relies on pre-labeled data, and unsupervised learning, which seeks to find patterns without any labeled responses ([Shaheen et al., 2025](#deepmind-ateri-2025)).\n",
    "\n",
    "In Breakout, the agent must learn to control a paddle to bounce a ball and break bricks, receiving rewards only when bricks are broken. This setup presents a challenge where the agent must learn the consequences of its actions over time, making RL an ideal approach . ​\n",
    "\n",
    "The success of RL in Breakout not only showcases its effectiveness in gaming but also highlights its potential in real-world applications where decision-making under uncertainty and learning from interaction are crucial.​\n",
    "\n",
    "In summary, RL's framework aligns seamlessly with the challenges presented by Breakout, making it the machine learning paradigm of choice for such tasks.​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a401fb",
   "metadata": {},
   "source": [
    "# The Environment\n",
    "\n",
    "1. The game selected: **Breakout**\n",
    "\n",
    "    The goal is to control a paddle in order to hit a ball toward brick walls at the top of the screen and to break them ([AtariAge - Atari 2600 Manuals (HTML) - Breakout (Atari), 1997-1998](#breakout-description)). The player starts with 5 lives and loses a life if the ball drops. The game terminates and resets when all lives are lost.\n",
    "\n",
    "1. The input from the environment: A 210x160 RGB image frame from the game screen, where each pixel value is an unsigned 8-bit integer ranging from 0 to 255 ([Implementation/Game environment](#game-environment)).\n",
    "\n",
    "1. The control settings: four Discrete actions ([Machado et al., 2018](#ale-breakout-doc)):\n",
    "    - 0:NOOP (No Operation)\n",
    "    - 1:FIRE\n",
    "    - 2:RIGHT\n",
    "    - 3:LEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90759672",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29d140",
   "metadata": {},
   "source": [
    "## Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "org_env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"Action space: {org_env.action_space}\")\n",
    "print(f\"Observation space: {org_env.observation_space}\")\n",
    "\n",
    "org_env.reset()\n",
    "rgb_arr = org_env.render()\n",
    "\n",
    "assert rgb_arr is not None, \"Rendering empty array\"\n",
    "plt.imshow(rgb_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76031686",
   "metadata": {},
   "source": [
    "<a id=\"obs-preprocess\"></a> \n",
    "<!-- I noticed that the observation includes redundant information — specifically, the scoreboard at the top, which takes up 20 pixels. Thus they will be cut of using Gymnasium Wrappers.  -->\n",
    "<!-- Finally, a Normalize wrapper scales pixel value from [0,255] to [0,1]. -->\n",
    "\n",
    "The preprocess applied to the observations include:\n",
    "\n",
    "1. RecordEpisodeStatistics: keeps track of cumulative rewards and episode lengths.\n",
    "1. ResizeObservation((84, 84)): resizes the input frame to 84x84 in order to ensure the input to be in fixed size and reduce computation\n",
    "1. GrayscaleObservation: reduces input dimensionality from 3 channels (rgb) to 1 (gray) while preserving essential information\n",
    "1. FrameStackObservation(4): Stacks the last 4 grayscale frames together to provide temporal context\n",
    "    <!-- 1. EpisodicLifeEnv: Mark the loss of life as end of episode but does not reset the environment. [Shaheen et al., 2025](#deepmind-ateri-2025) found out that it helps value estimation -->\n",
    "1. MaxAndSkipObservation(skip=4): reduces computation and removes flickering artifacts \n",
    "1. FireResetEnv: For certain games like Breakout, after life loss, the game is fixed until firing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    MaxAndSkipEnv,\n",
    "    FireResetEnv,\n",
    ")\n",
    "\n",
    "# record episode reward and length\n",
    "env = gym.wrappers.RecordEpisodeStatistics(org_env)\n",
    "# Resize from 210x160 to 84x84\n",
    "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "# Convert rgb to grayscale\n",
    "env = gym.wrappers.GrayscaleObservation(env)\n",
    "# Stack 4 frames for temporal information\n",
    "env = gym.wrappers.FrameStackObservation(env, 4)\n",
    "# Only change action every 4th frame\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "# Reset with FIRE action\n",
    "env = FireResetEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6102dd",
   "metadata": {},
   "source": [
    "### Observation from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "# plot 4 channels of the observation\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(obs.squeeze()[i], cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Agent's view\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba8091",
   "metadata": {},
   "source": [
    "## VANILLA DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4b5db",
   "metadata": {},
   "source": [
    "### Capture and pre-processing of the data\n",
    "\n",
    "Explained in section [Game environment](#obs-preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f55824",
   "metadata": {},
   "source": [
    "### Network structure\n",
    "\n",
    "The Deep Q-Network (DQN) implemented here is a convolutional neural network designed to process stacks of image frames, typical in Atari-based reinforcement learning environments. The architecture follows the structure used in the original DeepMind DQN paper by [Mnih et al. (2015)](#deepmind-ateri-2015).\n",
    "The input to the network is a tensor of shape (4, 84, 84), representing four stacked grayscale frames.\n",
    "\n",
    "The layers are as follows:\n",
    "\n",
    "1. **Convolutional Layer 1**\n",
    "\n",
    "   * `in_channels = 4`, `out_channels = 32`, `kernel_size = 8`, `stride = 4`\n",
    "   * Output shape: `(32, 20, 20)`\n",
    "\n",
    "2. **ReLU Activation**\n",
    "\n",
    "3. **Convolutional Layer 2**\n",
    "\n",
    "   * `in_channels = 32`, `out_channels = 64`, `kernel_size = 4`, `stride = 2`\n",
    "   * Output shape: `(64, 9, 9)`\n",
    "\n",
    "4. **ReLU Activation**\n",
    "\n",
    "5. **Convolutional Layer 3**\n",
    "\n",
    "   * `in_channels = 64`, `out_channels = 64`, `kernel_size = 3`, `stride = 1`\n",
    "   * Output shape: `(64, 7, 7)`\n",
    "\n",
    "6. **ReLU Activation**\n",
    "\n",
    "7. **Flatten Layer**\n",
    "\n",
    "   * Flattens the output from shape `(64, 7, 7)` to a vector of length `3136` (i.e., `64*7*7`)\n",
    "\n",
    "8. **Fully Connected Layer 1**\n",
    "\n",
    "   * `in_features = 3136`, `out_features = 512`\n",
    "\n",
    "9. **ReLU Activation**\n",
    "\n",
    "10. **Fully Connected Output Layer**\n",
    "\n",
    "    * `in_features = 512`, `out_features = n_actions`\n",
    "    * Outputs one Q-value per discrete action.\n",
    "\n",
    "\n",
    "**In short**, the network extracts spatial features from the input frames using convolutional layers, then maps them through fully connected layers to a Q-value for each possible action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)  # Normalize to [0, 1]\n",
    "\n",
    "\n",
    "print(DQN(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca4e03",
   "metadata": {},
   "source": [
    "### Q learning update applied to the weights\n",
    "\n",
    "In this implementation, Deep Q-Learning is used to train a neural network to approximate the optimal action-value function $Q^*(s, a)$. The network parameters are updated using samples from a Replay Buffer, with the target computed from a fixed target network.\n",
    "\n",
    "At every train_freq steps (e.g. every 4 steps), if enough transitions are stored, the following update is applied:\n",
    "\n",
    "1. The weights are updated to minimize the **Huber loss** between the predicted Q-value and the target:\n",
    "\n",
    "    $$\n",
    "    \\text{Loss} = \\text{Huber}(Q(s_t, a_t) - y_t)\n",
    "    $$\n",
    "\n",
    "    where the **target** $y_t$ is defined as:\n",
    "\n",
    "    $$\n",
    "    y_t = r_t + \\gamma \\cdot \\max_{a'} Q_{\\text{target}}(s_{t+1}, a') \\cdot (1 - \\text{done})\n",
    "    $$\n",
    "\n",
    "    Here:\n",
    "\n",
    "    * $Q(s_t, a_t)$: is the estimated Q-value from the online network for the selected action\n",
    "    * $Q_{\\text{target}}(s_{t+1}, a')$: is the max predicted Q-value from the target network\n",
    "    * $r_t$: is the clipped reward at time $t$\n",
    "    * $\\gamma$: is the discount factor\n",
    "    * $\\text{done}$: is a binary flag indicating episode termination\n",
    "\n",
    "1. Mechanics\n",
    "\n",
    "    1. Sample a minibatch of transitions $(s, a, r, s', \\text{done})$ from the replay buffer\n",
    "    2. Compute current Q-values for $s$ using the online network\n",
    "    3. Compute target Q-values for $s'$ using the target network (no gradients)\n",
    "    4. Compute loss between current and target Q-values using `F.huber_loss`\n",
    "    5. Backpropagate the loss and update the online network weights via Adam optimizer\n",
    "    6. Every $C$ steps, **synchronize the target network** with the online network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29271a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gymnasium import Env\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "import torch\n",
    "from torch.optim.adam import Adam\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calEpsilon(epoch, epsilon_start, epsilon_end, exploration_steps):\n",
    "    return max(\n",
    "        (epsilon_end - epsilon_start) / exploration_steps * epoch + epsilon_start,\n",
    "        epsilon_end,\n",
    "    )\n",
    "\n",
    "\n",
    "def DQN_train(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    device=\"cuda\",\n",
    "    buffer_size=10_000,\n",
    "    n_epoch=30_000,\n",
    "    train_freq=4,\n",
    "    batch_size=96,\n",
    "    gamma=0.99,\n",
    "    replay_start_size=50,\n",
    "    epsilon_start=1,\n",
    "    epsilon_end=0.1,\n",
    "    exploration_steps=1_000,\n",
    "    C=10,  # Target network update frequency\n",
    "    learning_rate=1.25e-4,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = ReplayBuffer(\n",
    "        buffer_size,\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        device,\n",
    "        optimize_memory_usage=True,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "\n",
    "    # Initialize target action-value function Q_hat\n",
    "    targetNetwork = copy.deepcopy(model).to(device)\n",
    "    # Initialize action-value function Q with random weights\n",
    "    onlineNetwork = model.to(device)\n",
    "    targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "    optimizer = Adam(onlineNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    total_rewards_list = []\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "    total_loss_list = []\n",
    "    loss_means = []\n",
    "    losses = []\n",
    "    best_reward = 0\n",
    "    bestState = {}\n",
    "\n",
    "    while epoch <= n_epoch:\n",
    "\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        # Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):  # Noop and fire to reset environment\n",
    "            obs, reward, terminated, truncated, info = env.step(1)\n",
    "\n",
    "        while not dead:\n",
    "            current_life = info[\"lives\"]\n",
    "\n",
    "            epsilon = calEpsilon(epoch, epsilon_start, epsilon_end, exploration_steps)\n",
    "            if random.random() < epsilon:  # With probability ε select a random action a\n",
    "                action = np.array(env.action_space.sample())\n",
    "\n",
    "            else:  # Otherwise select a = max_a Q∗(φ(st), a; θ)\n",
    "                q_values = onlineNetwork(torch.Tensor(obs).unsqueeze(0).to(device))\n",
    "                action = np.array(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "            # Execute action a in emulator and observe reward rt and image xt+1\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            # print(f\"info: {info}\")\n",
    "            done = np.array(info[\"lives\"] < current_life)\n",
    "\n",
    "            # Set st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            real_next_obs = next_obs.copy()\n",
    "\n",
    "            reward = float(reward)\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)  # Reward clipping\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            rb.add(obs, real_next_obs, action, reward, done, [info])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if epoch > replay_start_size and epoch % train_freq == 0:\n",
    "                # Sample random minibatch of transitions (φj , aj , rj , φj +1 ) from D\n",
    "                data = rb.sample(batch_size)\n",
    "                with torch.no_grad():\n",
    "                    max_target_q_value, _ = targetNetwork(data.next_observations).max(\n",
    "                        dim=1\n",
    "                    )\n",
    "                    y = data.rewards.flatten() + gamma * max_target_q_value * (\n",
    "                        1 - data.dones.flatten()\n",
    "                    )\n",
    "                current_q_value = (\n",
    "                    onlineNetwork(data.observations).gather(1, data.actions).squeeze()\n",
    "                )\n",
    "\n",
    "                loss = F.huber_loss(y, current_q_value)\n",
    "\n",
    "                # Perform a gradient descent\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Every C steps reset Q_hat=Q\n",
    "            if epoch % C == 0:\n",
    "                targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "            epoch += 1\n",
    "            if (epoch % 10_000 == 0) and epoch > 0:\n",
    "                smoothed_reward = np.mean(rewards) if rewards else 0\n",
    "                smoothed_rewards.append(smoothed_reward)\n",
    "                total_rewards_list.append(rewards)\n",
    "                rewards = []\n",
    "\n",
    "                loss_mean = np.mean(losses) if losses else 0\n",
    "                loss_means.append(loss_mean)\n",
    "                total_loss_list.append(losses)\n",
    "                losses = []\n",
    "\n",
    "            if verbose and (epoch % 100_000 == 0):\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss_mean}, Smoothed Reward: {smoothed_reward}\"\n",
    "                )\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "        if total_rewards > best_reward:\n",
    "            best_reward = total_rewards\n",
    "            bestState = onlineNetwork.state_dict()\n",
    "\n",
    "    return bestState, total_rewards_list, total_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"Action space is not discrete\"\n",
    "model = DQN(env.action_space.n)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "trainConfigs = {\n",
    "    \"buffer_size\": 100_000,\n",
    "    \"n_epoch\": 5_000_000,\n",
    "    \"batch_size\": 96,\n",
    "    \"gamma\": 0.99,\n",
    "    \"replay_start_size\": 5_000,\n",
    "    \"epsilon_start\": 1,\n",
    "    \"epsilon_end\": 0.1,\n",
    "    \"exploration_steps\": 100_000,\n",
    "    \"C\": 1_000,\n",
    "    \"learning_rate\": 1.25e-4,\n",
    "}\n",
    "\n",
    "(\n",
    "    modelState,\n",
    "    rewardss,\n",
    "    losseses,\n",
    ") = DQN_train(\n",
    "    env,\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    **trainConfigs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ad097",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7329e1e",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77847363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwrap the rewardss\n",
    "unwrappedRewards = [item for sublist in rewardss for item in sublist]\n",
    "\n",
    "# plot rewards over episodes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedRewards, label=\"Episode reward\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedRewards, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Moving average reward\",\n",
    ")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"DQN training rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9567f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedLosses = [item for sublist in losseses for item in sublist]\n",
    "# plot losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedLosses, label=\"Loss\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedLosses, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Loss (moving average)\",\n",
    ")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b2f4d",
   "metadata": {},
   "source": [
    "#### How does one evaluate the performance of the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(model: nn.Module, env: Env, device, n_episodes, render):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        dead = False\n",
    "\n",
    "        # start with fire option\n",
    "        action = 1\n",
    "\n",
    "        while not dead:\n",
    "            obs, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            if len(frames) < render:\n",
    "                frames.append(env.render())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.tensor(obs).unsqueeze(0).to(device)\n",
    "\n",
    "                q_values = model(obs_tensor)\n",
    "                action = np.array(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                action = 1\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "model.load_state_dict(modelState)\n",
    "\n",
    "# Play the game\n",
    "frames = play(model, env, \"cpu\", n_episodes=3, render=1000)\n",
    "\n",
    "# Keep 3_rd frames\n",
    "frames = [frames[i] for i in range(0, len(frames), 3)]\n",
    "\n",
    "print(f\"Drawing {len(frames)} frames\")\n",
    "fig = plt.figure()\n",
    "plt.axis(\"off\")\n",
    "im = plt.imshow(frames[0])\n",
    "\n",
    "\n",
    "def update_frame(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_frame, frames=frames, interval=50)\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8b1cb",
   "metadata": {},
   "source": [
    "#### Is the agent learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3025ad",
   "metadata": {},
   "source": [
    "## DOUBLE DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de83c2",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "One of the key problems with Vanilla DQN is its tendency to overestimate Q-values during training. This happens because the max operator used in the Q-learning update selects the highest estimated action value, which tends to be overly optimistic due to noise and approximation errors in the value function ([van Hasselt et al., 2016](#double-dqn)).\n",
    "This overestimation can lead to unstable learning and suboptimal policies, especially in environments with noisy or sparse rewards. As training progresses, these compounded overestimations can mislead the policy into favoring incorrect actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df8ee1",
   "metadata": {},
   "source": [
    "### Theoretical solution\n",
    "\n",
    "Double DQN addresses this overestimation bias by **decoupling action selection from action evaluation**. Instead of using the same network to both select and evaluate the next action, Double DQN uses:\n",
    "\n",
    "* The **online network** to select the best action (argmax),\n",
    "* The **target network** to evaluate the Q-value of that action.\n",
    "\n",
    "Mathematically, the target value becomes:\n",
    "\n",
    "$$\n",
    "Q_{\\text{target}}(s,a) = r + \\gamma \\, Q_{\\text{target}}\\bigl(s', \\arg\\max_{a'} Q_{\\text{online}}(s',a')\\bigr)\n",
    "$$\n",
    "\n",
    "This separation reduces the chance of overestimating Q-values because the evaluation is less biased by the same noisy estimates used for selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleDQN_train(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    device=\"cuda\",\n",
    "    buffer_size=10_000,\n",
    "    n_epoch=30_000,\n",
    "    train_freq=4,\n",
    "    batch_size=96,\n",
    "    gamma=0.99,\n",
    "    replay_start_size=50,\n",
    "    epsilon_start=1,\n",
    "    epsilon_end=0.1,\n",
    "    exploration_steps=1_000,\n",
    "    C=10,  # Target network update frequency\n",
    "    learning_rate=1.25e-4,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = ReplayBuffer(\n",
    "        buffer_size,\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        device,\n",
    "        optimize_memory_usage=True,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "\n",
    "    # Initialize target action-value function Q_hat\n",
    "    targetNetwork = copy.deepcopy(model).to(device)\n",
    "    # Initialize action-value function Q with random weights\n",
    "    onlineNetwork = model.to(device)\n",
    "    targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "    optimizer = Adam(onlineNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    total_rewards_list = []\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "    total_loss_list = []\n",
    "    loss_means = []\n",
    "    losses = []\n",
    "    best_reward = 0\n",
    "    bestState = {}\n",
    "\n",
    "    while epoch <= n_epoch:\n",
    "\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        # Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):  # Noop and fire to reset environment\n",
    "            obs, reward, terminated, truncated, info = env.step(1)\n",
    "\n",
    "        while not dead:\n",
    "            current_life = info[\"lives\"]\n",
    "\n",
    "            epsilon = max(\n",
    "                (epsilon_end - epsilon_start) / exploration_steps * epoch\n",
    "                + epsilon_start,\n",
    "                epsilon_end,\n",
    "            )\n",
    "            if random.random() < epsilon:  # With probability ε select a random action a\n",
    "                action = np.array(env.action_space.sample())\n",
    "\n",
    "            else:  # Otherwise select a = max_a Q∗(φ(st), a; θ)\n",
    "                q_values = onlineNetwork(torch.Tensor(obs).unsqueeze(0).to(device))\n",
    "                action = np.array(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "            # Execute action a in emulator and observe reward rt and image xt+1\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            # print(f\"info: {info}\")\n",
    "            done = np.array(info[\"lives\"] < current_life)\n",
    "\n",
    "            # Set st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            real_next_obs = next_obs.copy()\n",
    "\n",
    "            reward = float(reward)\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)  # Reward clipping\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            rb.add(obs, real_next_obs, action, reward, done, [info])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if epoch > replay_start_size and epoch % train_freq == 0:\n",
    "                # Sample random minibatch of transitions (φj , aj , rj , φj +1 ) from D\n",
    "                data = rb.sample(batch_size)\n",
    "                with torch.no_grad():\n",
    "                    # Use the online network to select the action\n",
    "                    bestActions = onlineNetwork(data.next_observations).argmax(\n",
    "                        dim=1, keepdim=True\n",
    "                    )\n",
    "                    # Use the target network to evaluate the action\n",
    "                    nextQ_val = (\n",
    "                        targetNetwork(data.next_observations)\n",
    "                        .gather(1, bestActions)\n",
    "                        .squeeze()\n",
    "                    )\n",
    "                y = data.rewards.flatten() + (\n",
    "                    gamma * nextQ_val * (1 - data.dones.flatten())\n",
    "                )\n",
    "                current_q_value = (\n",
    "                    onlineNetwork(data.observations).gather(1, data.actions).squeeze()\n",
    "                )\n",
    "\n",
    "                loss = F.huber_loss(y, current_q_value)\n",
    "\n",
    "                # Perform a gradient descent\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Every C steps reset Q_hat=Q\n",
    "            if epoch % C == 0:\n",
    "                targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "            epoch += 1\n",
    "            if (epoch % 10_000 == 0) and epoch > 0:\n",
    "                smoothed_reward = np.mean(rewards) if rewards else 0\n",
    "                smoothed_rewards.append(smoothed_reward)\n",
    "                total_rewards_list.append(rewards)\n",
    "                rewards = []\n",
    "\n",
    "                loss_mean = np.mean(losses) if losses else 0\n",
    "                loss_means.append(loss_mean)\n",
    "                total_loss_list.append(losses)\n",
    "                losses = []\n",
    "\n",
    "            if verbose and (epoch % 100_000 == 0):\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss_mean}, Smoothed Reward: {smoothed_reward}\"\n",
    "                )\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "        if total_rewards > best_reward:\n",
    "            best_reward = total_rewards\n",
    "            bestState = onlineNetwork.state_dict()\n",
    "\n",
    "    return bestState, total_rewards_list, total_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"Action space is not discrete\"\n",
    "model = DQN(env.action_space.n)\n",
    "\n",
    "\n",
    "modelState, rewardss, losseses = doubleDQN_train(\n",
    "    env,\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    **trainConfigs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab549b56",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c405054",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedRewards = [item for sublist in rewardss for item in sublist]\n",
    "# plot rewards over episodes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedRewards, label=\"Episode reward\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedRewards, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Moving average reward\",\n",
    ")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Double DQN training rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1092cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedLosses = [item for sublist in losseses for item in sublist]\n",
    "# plot losses over steps\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedLosses, label=\"Loss\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedLosses, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Loss (moving average)\",\n",
    ")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "model.load_state_dict(modelState)\n",
    "\n",
    "# Play the game\n",
    "frames = play(model, env, \"cpu\", n_episodes=3, render=1000)\n",
    "\n",
    "# Keep 3_rd frames\n",
    "frames = [frames[i] for i in range(0, len(frames), 3)]\n",
    "\n",
    "print(f\"Drawing {len(frames)} frames\")\n",
    "fig = plt.figure()\n",
    "plt.axis(\"off\")\n",
    "im = plt.imshow(frames[0])\n",
    "\n",
    "\n",
    "def update_frame(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_frame, frames=frames, interval=50)\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26805827",
   "metadata": {},
   "source": [
    "# Compare DQN and Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9676b",
   "metadata": {},
   "source": [
    "# Added Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23336b97",
   "metadata": {},
   "source": [
    "## Hyperparamerter Optimization\n",
    "\n",
    "[Ken Zheng (2024)](#hyperparam-2024) investigates the impact of hyperparameter tuning on the performance of popular deep reinforcement learning (DRL) algorithms, specifically DDPG, TRPO, and PPO, across three continuous control environments from MuJoCo: HalfCheetah, Hopper, and Walker2d. The motivation stems from the widely acknowledged sensitivity of DRL algorithms to hyperparameter settings, yet many practitioners still rely on default configurations.\n",
    "\n",
    "To address this, the authors employ BOHB (Bayesian Optimization with Hyperband), an efficient hyperparameter optimization method that balances exploration and exploitation while being budget-aware. They define a search space of nine hyperparameters per algorithm and conduct extensive evaluations to understand how tuning affects final episodic return and sample efficiency. The study compares optimized configurations against default baselines and demonstrates that optimized parameters can significantly boost performance, especially for DDPG and TRPO.\n",
    "\n",
    "A key finding is that even modest tuning budgets lead to meaningful gains, and blindly using defaults can result in suboptimal performance. However, they also show diminishing returns beyond a certain tuning effort, and that tuning results can be environment-specific. The work emphasizes the importance of systematic tuning in DRL and provides insights into which parameters and search strategies are most impactful.\n",
    "\n",
    "Below are the comparision of their paramerters and mine:\n",
    "\n",
    "| **Parameter**            | **Paper's Tuned Value**                             | **My Value**    | **Notes / Comparison**                                                                  |\n",
    "| ------------------------ | --------------------------------------------------- | ----------------- | --------------------------------------------------------------------------------------- |\n",
    "| `batch_size`             | `84`                                                | `96`              | Similar. Marginal difference — likely low impact unless memory is a concern.            |\n",
    "| `gamma`                  | `0.95298`                                           | `0.99`            | I discount future rewards more — they found a slightly more short-term focus better.  |\n",
    "| `learning_rate`          | `6.6566e-5`                                         | `1.25e-4`         | Mine makes training faster; theirs is more conservative and possibly more stable.          |\n",
    "| `replay_start_size`      | `50000`                                             | `5000`              |  they delay learning until buffer is filled more. Prevents early bias. |\n",
    "| `target_update`          | `2500`                                              | `1000` (my `C`)   | Their target network updates less frequently. This might add more stability.           |\n",
    "| `exploration_steps`      | - | `100_000`            | Their exploration comprises of 2 linear phrases 1->0.1->0.01, over 1 million then 22 million of frames while mine is one linear decaying over 100_000 frames.   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnedConfigs = {\n",
    "    \"buffer_size\": 100_000,\n",
    "    \"n_epoch\": 5_000_000,\n",
    "    \"batch_size\": 84,\n",
    "    \"gamma\": 0.95298,\n",
    "    \"replay_start_size\": 50_000,\n",
    "    \"epsilon_start\": 0.147,\n",
    "    \"epsilon_end\": 0.1,\n",
    "    \"exploration_steps\": 100_000,\n",
    "    \"C\": 2500,\n",
    "    \"learning_rate\": 6.6566e-5,\n",
    "}\n",
    "\n",
    "\n",
    "# their way of calculating decay epsilon\n",
    "# full epsilon range is from 1.0 → 0.1 → 0.01, transitioning across:\n",
    "# Step 2 to 1,000,000: Decay from 1.0 → 0.1\n",
    "# Step 1,000,000 to 22,000,000: Decay from 0.1 → 0.01\n",
    "# After 22M steps: Fixed ε = 0.01\n",
    "\n",
    "\n",
    "def calEpsilon(epoch, epsilon_start, epsilon_end, exploration_steps):\n",
    "    current_step = epoch\n",
    "    startpoint = 2\n",
    "    end = 0.1\n",
    "    kneepoint = 1000000\n",
    "    final_eps = 0.01\n",
    "    final_knee_point = 22000000\n",
    "\n",
    "    if current_step < startpoint:\n",
    "        return 1\n",
    "    mid_seg = end + np.maximum(\n",
    "        0, (1 - end) - (1 - end) / kneepoint * (current_step - startpoint)\n",
    "    )\n",
    "    if not final_eps:\n",
    "        return mid_seg\n",
    "    else:\n",
    "        if final_eps and final_knee_point and (current_step < kneepoint):\n",
    "            return mid_seg\n",
    "        else:\n",
    "            return final_eps + (end - final_eps) / (final_knee_point - kneepoint) * (\n",
    "                final_knee_point - current_step\n",
    "            )\n",
    "\n",
    "\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"Action space is not discrete\"\n",
    "model = DQN(env.action_space.n)\n",
    "\n",
    "modelState, rewardss, losseses = doubleDQN_train(\n",
    "    env,\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    **tunnedConfigs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedRewards = [item for sublist in rewardss for item in sublist]\n",
    "# plot rewards over episodes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedRewards, label=\"Episode reward\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedRewards, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Moving average reward\",\n",
    ")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Double DQN training rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedLosses = [item for sublist in losseses for item in sublist]\n",
    "# plot losses over steps\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedLosses, label=\"Loss\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedLosses, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Loss (moving average)\",\n",
    ")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f95ec7",
   "metadata": {},
   "source": [
    "## Prioritised Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd36406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data import PrioritizedReplayBuffer, ListStorage\n",
    "\n",
    "\n",
    "def prioritisedDoubleDQN_train(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    device=\"cuda\",\n",
    "    buffer_size=10_000,\n",
    "    n_epoch=30_000,\n",
    "    train_freq=4,\n",
    "    batch_size=96,\n",
    "    gamma=0.99,\n",
    "    replay_start_size=50,\n",
    "    epsilon_start=1,\n",
    "    epsilon_end=0.1,\n",
    "    exploration_steps=1_000,\n",
    "    C=10,  # Target network update frequency\n",
    "    learning_rate=1.25e-4,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = PrioritizedReplayBuffer(\n",
    "        storage=ListStorage(buffer_size),\n",
    "        alpha=0.7,\n",
    "        beta=0.5,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Initialize target action-value function Q_hat\n",
    "    targetNetwork = copy.deepcopy(model).to(device)\n",
    "    # Initialize action-value function Q with random weights\n",
    "    onlineNetwork = model.to(device)\n",
    "    targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "    optimizer = Adam(onlineNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    total_rewards_list = []\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "    total_loss_list = []\n",
    "    loss_means = []\n",
    "    losses = []\n",
    "    best_reward = 0\n",
    "    bestState = {}\n",
    "\n",
    "    while epoch <= n_epoch:\n",
    "\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        # Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):  # Noop and fire to reset environment\n",
    "            obs, reward, terminated, truncated, info = env.step(1)\n",
    "\n",
    "        while not dead:\n",
    "            current_life = info[\"lives\"]\n",
    "\n",
    "            epsilon = max(\n",
    "                (epsilon_end - epsilon_start) / exploration_steps * epoch\n",
    "                + epsilon_start,\n",
    "                epsilon_end,\n",
    "            )\n",
    "            if random.random() < epsilon:  # With probability ε select a random action a\n",
    "                action = np.array(env.action_space.sample())\n",
    "\n",
    "            else:  # Otherwise select a = max_a Q∗(φ(st), a; θ)\n",
    "                q_values = onlineNetwork(torch.Tensor(obs).unsqueeze(0).to(device))\n",
    "                action = np.array(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "            # Execute action a in emulator and observe reward rt and image xt+1\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            # print(f\"info: {info}\")\n",
    "            done = np.array(info[\"lives\"] < current_life)\n",
    "\n",
    "            # Set st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            real_next_obs = next_obs.copy()\n",
    "\n",
    "            reward = float(reward)\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)  # Reward clipping\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            rbData = {\n",
    "                \"obs\": torch.tensor(obs),\n",
    "                \"next_obs\": torch.tensor(real_next_obs),\n",
    "                \"action\": torch.tensor(action),\n",
    "                \"reward\": torch.tensor(reward),\n",
    "                \"done\": torch.tensor(done),\n",
    "                # \"info\": torch.tensor(info),\n",
    "            }\n",
    "            rb.add(rbData)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if epoch > replay_start_size and epoch % train_freq == 0:\n",
    "                # Sample random minibatch of transitions (φj , aj , rj , φj +1 ) from D\n",
    "                rbData, rbInfo = rb.sample(batch_size, True)\n",
    "                for k, v in rbData.items():\n",
    "                    rbData[k] = v.float().to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Use the online network to select the action\n",
    "                    bestActions = onlineNetwork(rbData[\"next_obs\"]).argmax(\n",
    "                        dim=1, keepdim=True\n",
    "                    )\n",
    "                    # Use the target network to evaluate the action\n",
    "                    nextQ_val = (\n",
    "                        targetNetwork(rbData[\"next_obs\"])\n",
    "                        .gather(1, bestActions)\n",
    "                        .squeeze()\n",
    "                    )\n",
    "                y = rbData[\"reward\"].flatten() + (\n",
    "                    gamma * nextQ_val * (1 - rbData[\"done\"].flatten())\n",
    "                )\n",
    "                current_q_value = (\n",
    "                    onlineNetwork(rbData[\"obs\"])\n",
    "                    .gather(1, rbData[\"action\"].to(torch.int64).unsqueeze(-1))\n",
    "                    .squeeze()\n",
    "                )\n",
    "\n",
    "                loss = F.huber_loss(y, current_q_value)\n",
    "\n",
    "                # cal the priority\n",
    "                tdErr = (y - current_q_value).abs()\n",
    "                rb.update_priority(rbInfo[\"index\"], tdErr)\n",
    "\n",
    "                # Perform a gradient descent\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Every C steps reset Q_hat=Q\n",
    "            if epoch % C == 0:\n",
    "                targetNetwork.load_state_dict(onlineNetwork.state_dict())\n",
    "\n",
    "            epoch += 1\n",
    "            if (epoch % 10_000 == 0) and epoch > 0:\n",
    "                smoothed_reward = np.mean(rewards) if rewards else 0\n",
    "                smoothed_rewards.append(smoothed_reward)\n",
    "                total_rewards_list.append(rewards)\n",
    "                rewards = []\n",
    "\n",
    "                loss_mean = np.mean(losses) if losses else 0\n",
    "                loss_means.append(loss_mean)\n",
    "                total_loss_list.append(losses)\n",
    "                losses = []\n",
    "\n",
    "            if verbose and (epoch % 100_000 == 0):\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss_mean}, Smoothed Reward: {smoothed_reward}\"\n",
    "                )\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "        if total_rewards > best_reward:\n",
    "            best_reward = total_rewards\n",
    "            bestState = onlineNetwork.state_dict()\n",
    "\n",
    "    return bestState, total_rewards_list, total_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7564744",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"Action space is not discrete\"\n",
    "model = DQN(env.action_space.n)\n",
    "\n",
    "modelState, rewardss, losseses = prioritisedDoubleDQN_train(\n",
    "    env,\n",
    "    model,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    **trainConfigs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca734e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedRewards = [item for sublist in rewardss for item in sublist]\n",
    "# plot rewards over episodes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedRewards, label=\"Episode reward\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedRewards, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Moving average reward\",\n",
    ")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Double DQN training rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad77ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrappedLosses = [item for sublist in losseses for item in sublist]\n",
    "# plot losses over steps\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(unwrappedLosses, label=\"Loss\")\n",
    "# add moving average (1000)\n",
    "plt.plot(\n",
    "    np.convolve(unwrappedLosses, np.ones(1000) / 1000, mode=\"valid\"),\n",
    "    label=\"Loss (moving average)\",\n",
    ")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d136f8",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "[Mnih et al. (2015)](#deepmind-ateri-2015) discussed performance across different Atari games and explained why Vanilla DQN works well or poorly in certain games. \n",
    "For games like Breakout, Pong, Robotank, ... where the agent reaches **Superhuman** performance because the environment sends frequent and immediate rewards.\n",
    "In contrast, games like Seaquest and Pitfall feature sparse rewards, requiring long sequences of correct actions before any feedback is received — leading the DQN agent to perform worse than a human, and sometimes even worse than a random policy.\n",
    "\n",
    "One fundamental weakness of Vanilla DQN is its tendency to overestimate action values due to the use of the max operator in Q-learning updates. \n",
    "This overestimation bias significantly impacts games like Seaquest, where noisy reward signals can mislead the value function. \n",
    "To address this, [van Hasselt et al. (2016)](#double-dqn) introduced Double DQN, which decouples action selection and evaluation by using separate networks. This significantly reduces overestimation bias and leads to more stable and accurate learning, especially in environments where DQN struggles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a601a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. <a id=\"breakout-description\"></a>\n",
    "AtariAge—Atari 2600 Manuals (HTML)—Breakout (Atari). (1997-1998). Retrieved May 5, 2025, from https://atariage.com/manual_html_page.php?SoftwareID=889\n",
    "\n",
    "1. <a id=\"ale-breakout-doc\"></a>\n",
    "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M. J., & Bowling, M. (2018). Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. Journal of Artificial Intelligence Research, 61, 523–562.\n",
    "https://ale.farama.org/environments/breakout/\n",
    "\n",
    "1. <a id=\"deepmind-ateri-2025\"></a>\n",
    "Shaheen, A., Badr, A., Abohendy, A., Alsaadawy, H., & Alsayad, N. (2025). Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations (No. arXiv:2502.10303). arXiv. https://doi.org/10.48550/arXiv.2502.10303\n",
    "\n",
    "1. <a id=\"deepmind-ateri-2015\"></a>\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n",
    "\n",
    "1. <a id=\"hyperparam-2024\"></a>\n",
    "Zheng, Ken. (2024). Hyperparameter Optimization for Deep Reinforcement Learning: An Atari Breakout Case Study. 10.58445/rars.1635. \n",
    "\n",
    "1. <a id=\"double-dqn\"></a>\n",
    "van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double Q-Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 30(1). https://doi.org/10.1609/aaai.v30i1.10295\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
