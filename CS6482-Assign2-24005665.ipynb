{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d182b1f",
   "metadata": {},
   "source": [
    "# Name and ID: Hoang Tu Bui - 24005665\n",
    "\n",
    "# The code executes till the end without any errors\n",
    "\n",
    "# Original source: \n",
    "\n",
    "1. https://medium.com/@liyinxuan0213/step-by-step-double-deep-q-networks-double-dqn-tutorial-from-atari-games-to-bioengineering-dec7e6373896\n",
    "1. https://stackoverflow.com/questions/26587527/cite-a-paper-using-github-markdown-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38889d05",
   "metadata": {},
   "source": [
    "# Why Reinforcement Learning is the ML paradigm of choice for this task?\n",
    "\n",
    "Reinforcement Learning (RL) is well-suited for Atari games like Breakout where an agent must learn to make decisions through trial and error, optimizing its actions based on feedback in the form of rewards. This paradigm is distinct from supervised learning, which relies on pre-labeled data, and unsupervised learning, which seeks to find patterns without any labeled responses ([Shaheen et al., 2025](#shaheen2025reinforcementlearningstrategybasedatari)).\n",
    "\n",
    "In Breakout, the agent must learn to control a paddle to bounce a ball and break bricks, receiving rewards only when bricks are broken. This setup presents a challenge where the agent must learn the consequences of its actions over time, making RL an ideal approach . ​\n",
    "\n",
    "The success of RL in Breakout not only showcases its effectiveness in gaming but also highlights its potential in real-world applications where decision-making under uncertainty and learning from interaction are crucial.​\n",
    "\n",
    "In summary, RL's framework aligns seamlessly with the challenges presented by Breakout, making it the machine learning paradigm of choice for such tasks.​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a401fb",
   "metadata": {},
   "source": [
    "# The Environment\n",
    "\n",
    "1. The game selected: **Breakout**\n",
    "\n",
    "    The goal is to control a paddle in order to hit a ball toward brick walls at the top of the screen and to break them ([AtariAge - Atari 2600 Manuals (HTML) - Breakout (Atari), n.d.](#breakout-description)). The player starts with 5 lives and loses a life if the ball drops. The game terminates and resets when all lives are lost.\n",
    "\n",
    "1. The input from the environment:\n",
    "\n",
    "1. The control settings: four Discrete actions:\n",
    "    - 0:NOOP (No Operation)\n",
    "    - 1:FIRE\n",
    "    - 2:RIGHT\n",
    "    - 3:LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5581c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Box(0, 255, (4, 84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "env = gym.wrappers.GrayscaleObservation(env)\n",
    "env = gym.wrappers.FrameStackObservation(env, 4)\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "spec = env.spec\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, nb_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, nb_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29271a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deep_Q_Learning(\n",
    "    env,\n",
    "    buffer_size=1_000_000,\n",
    "    nb_epochs=30_000_000,\n",
    "    train_frequency=4,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    replay_start_size=50_000,\n",
    "    epsilon_start=1,\n",
    "    epsilon_end=0.1,\n",
    "    exploration_steps=1_000_000,\n",
    "    device=\"cuda\",\n",
    "    C=10_000,\n",
    "    learning_rate=1.25e-4,\n",
    "):\n",
    "\n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = ReplayBuffer(\n",
    "        buffer_size,\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        device,\n",
    "        optimize_memory_usage=True,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "\n",
    "    # Initialize action-value function Q with random weights\n",
    "    q_network = DQN(env.action_space.n).to(device)\n",
    "    # Initialize target action-value function Q_hat\n",
    "    target_network = DQN(env.action_space.n).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch = 0\n",
    "    total_rewards_list = []\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "    total_loss_list = []\n",
    "    loss_means = []\n",
    "    losses = []\n",
    "    best_reward = 0\n",
    "\n",
    "    progress_bar = tqdm(total=nb_epochs)\n",
    "    while epoch <= nb_epochs:\n",
    "\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        # Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):  # Noop and fire to reset environment\n",
    "            obs, reward, terminated, truncated, info = env.step(1)\n",
    "\n",
    "        while not dead:\n",
    "            current_life = info[\"lives\"]\n",
    "\n",
    "            epsilon = max(\n",
    "                (epsilon_end - epsilon_start) / exploration_steps * epoch\n",
    "                + epsilon_start,\n",
    "                epsilon_end,\n",
    "            )\n",
    "            if random.random() < epsilon:  # With probability ε select a random action a\n",
    "                action = np.array(env.action_space.sample())\n",
    "                # print(\"random\")\n",
    "\n",
    "            else:  # Otherwise select a = max_a Q∗(φ(st), a; θ)\n",
    "                q_values = q_network(torch.Tensor(obs).unsqueeze(0).to(device))\n",
    "                action = np.array(torch.argmax(q_values, dim=1).item())\n",
    "                # print(\"not random\")\n",
    "\n",
    "            # Execute action a in emulator and observe reward rt and image xt+1\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            # print(f\"info: {info}\")\n",
    "            done = np.array(info[\"lives\"] < current_life)\n",
    "\n",
    "            # Set st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            real_next_obs = next_obs.copy()\n",
    "\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)  # Reward clipping\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            rb.add(obs, real_next_obs, action, reward, done, info)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if epoch > replay_start_size and epoch % train_frequency == 0:\n",
    "                # Sample random minibatch of transitions (φj , aj , rj , φj +1 ) from D\n",
    "                data = rb.sample(batch_size)\n",
    "                with torch.no_grad():\n",
    "                    max_target_q_value, _ = target_network(data.next_observations).max(\n",
    "                        dim=1\n",
    "                    )\n",
    "                    y = data.rewards.flatten() + gamma * max_target_q_value * (\n",
    "                        1 - data.dones.flatten()\n",
    "                    )\n",
    "                current_q_value = (\n",
    "                    q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                )\n",
    "\n",
    "                loss = F.huber_loss(y, current_q_value)\n",
    "\n",
    "                # Perform a gradient descent step according to equation 3\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Every C steps reset Q_hat=Q\n",
    "            if epoch % C == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            epoch += 1\n",
    "            if (epoch % 10_000 == 0) and epoch > 0:\n",
    "                smoothed_reward = np.mean(rewards) if rewards else 0\n",
    "                smoothed_rewards.append(smoothed_reward)\n",
    "                total_rewards_list.append(rewards)\n",
    "                rewards = []\n",
    "\n",
    "                loss_mean = np.mean(losses) if losses else 0\n",
    "                loss_means.append(loss_mean)\n",
    "                total_loss_list.append(losses)\n",
    "                losses = []\n",
    "\n",
    "            if (epoch % 100_000 == 0) and epoch > 0:\n",
    "                plt.plot(smoothed_rewards)\n",
    "                plt.title(\"Average Reward on Breakout\")\n",
    "                plt.xlabel(\"Training Epochs [units of 10,000]\")\n",
    "                plt.ylabel(\"Average Reward per Episode\")\n",
    "                if not os.path.exists(\"./Imgs\"):\n",
    "                    os.makedirs(\"./Imgs\")\n",
    "                plt.savefig(f\"./Imgs/average_reward_on_breakout_{epoch}.png\")\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                plt.plot(loss_means)\n",
    "                plt.title(\"Average Loss on Breakout\")\n",
    "                plt.xlabel(\"Training Epochs [units of 10,000]\")\n",
    "                plt.ylabel(\"Average Loss per Episode\")\n",
    "                if not os.path.exists(\"./Imgs\"):\n",
    "                    os.makedirs(\"./Imgs\")\n",
    "                plt.savefig(f\"./Imgs/average_loss_on_breakout_{epoch}.png\")\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss_mean}, Smoothed Reward: {smoothed_reward}\"\n",
    "                )\n",
    "\n",
    "            if epoch % 500_000 == 0 and epoch > 0:\n",
    "                # if epoch % 10_000 == 0 and epoch > 0:\n",
    "                checkpoint_path = f\"./checkpoints/ddqn_checkpoint_{epoch}.pth\"\n",
    "                if not os.path.exists(\"./checkpoints\"):\n",
    "                    os.makedirs(\"./checkpoints\")\n",
    "                if not os.path.exists(\"./checkpoint_data\"):\n",
    "                    os.makedirs(\"./checkpoint_data\")\n",
    "                torch.save(q_network.state_dict(), checkpoint_path)\n",
    "\n",
    "                # Save smoothed rewards\n",
    "                with open(\n",
    "                    f\"./checkpoint_data/total_rewards_list_{epoch}.pkl\", \"wb\"\n",
    "                ) as f:\n",
    "                    pickle.dump(total_rewards_list, f)\n",
    "\n",
    "                # Save losses\n",
    "                with open(f\"./checkpoint_data/total_loss_list_{epoch}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(total_loss_list, f)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "        if total_rewards > best_reward:\n",
    "            best_reward = total_rewards\n",
    "            if not os.path.exists(\"./best_models\"):\n",
    "                os.makedirs(\"./best_models\")\n",
    "            torch.save(q_network.cpu(), f\"./best_models/best_model_{best_reward}\")\n",
    "            q_network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9676b",
   "metadata": {},
   "source": [
    "# Test ref \n",
    "\"...the **go to** statement should be abolished...\" ([Dijkstra, 1968](#1)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a601a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. <a id=\"1\"></a> \n",
    "Dijkstra, E. W. (1968). \n",
    "Go to statement considered harmful. \n",
    "Communications of the ACM, 11(3), 147-148.\n",
    "\n",
    "1. <a id=\"breakout-description\"></a>\n",
    "AtariAge—Atari 2600 Manuals (HTML)—Breakout (Atari). (n.d.). Retrieved May 5, 2025, from https://atariage.com/manual_html_page.php?SoftwareID=889\n",
    "\n",
    "1. <a id=\"shaheen2025reinforcementlearningstrategybasedatari\"></a>\n",
    "Shaheen, A., Badr, A., Abohendy, A., Alsaadawy, H., & Alsayad, N. (2025). Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations (No. arXiv:2502.10303). arXiv. https://doi.org/10.48550/arXiv.2502.10303"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
