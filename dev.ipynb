{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/micromamba/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 98179\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 4909\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 4923\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "dataName = \"slegroux/tiny-imagenet-200-clean\"\n",
    "dataPath = Path(dataName)\n",
    "if dataPath.exists() is False:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds: DatasetDict = load_dataset(dataName)  # type: ignore\n",
    "    ds.save_to_disk(dataPath)  # type: ignore\n",
    "\n",
    "else:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    ds: DatasetDict = load_from_disk(dataPath)  # type: ignore\n",
    "\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][16199][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"][900][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size = (64, 64)\n",
      "Mode = RGB\n",
      "Format = JPEG\n"
     ]
    }
   ],
   "source": [
    "img = ds[\"train\"][0][\"image\"]\n",
    "\n",
    "print(f\"Size = {img.size}\")\n",
    "print(f\"Mode = {img.mode}\")\n",
    "print(f\"Format = {img.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Prepare data for training and evaluating.\n",
    "\n",
    "Also augment the train data by introduce random effects like cropting, rotating, flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as ImgF\n",
    "\n",
    "\n",
    "# ds[\"train\"][0][\"image\"] is a PIL.JpegImagePlugin.JpegImageFile\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset of images and its labels. Return image and label. If transform is provided, apply it to the image.\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    IMG_SIZE = 64, 64\n",
    "\n",
    "    def _transformBuild(self):\n",
    "        transformations = []\n",
    "        transformations.append(v2.Resize(size=ImageDataset.IMG_SIZE))\n",
    "        if self.randRot:\n",
    "            transformations.append(v2.RandomRotation(15))  # type: ignore\n",
    "        if self.randCrop:\n",
    "            transformations.append(\n",
    "                v2.RandomResizedCrop(size=ImageDataset.IMG_SIZE, scale=(0.8, 1.0))\n",
    "            )\n",
    "        if self.randFlipH:\n",
    "            transformations.append(v2.RandomHorizontalFlip())\n",
    "        if self.randFlipV:\n",
    "            transformations.append(v2.RandomVerticalFlip())\n",
    "\n",
    "        transformations.append(v2.ToImage())\n",
    "        transformations.append(v2.ToDtype(torch.float32, True))\n",
    "\n",
    "        transformations.append(self.normaliser)\n",
    "\n",
    "        return v2.Compose(transformations)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        randCrop=False,\n",
    "        randRot=False,\n",
    "        randFlipH=False,\n",
    "        randFlipV=False,\n",
    "        normaliser=None,\n",
    "    ):\n",
    "        self.randCrop = randCrop\n",
    "        self.randRot = randRot\n",
    "        self.randFlipH = randFlipH\n",
    "        self.randFlipV = randFlipV\n",
    "\n",
    "        self.length = len(data)\n",
    "        self.imgSize = self.IMG_SIZE\n",
    "\n",
    "        self.images = [d[\"image\"] for d in data]\n",
    "        self.labels = [d[\"label\"] for d in data]\n",
    "\n",
    "        # calculate normaliser if not provided\n",
    "        if normaliser is None:\n",
    "            c = ImgF.to_tensor(self.images[0]).size(0)\n",
    "            mean = torch.zeros(c)\n",
    "            std = torch.zeros(c)\n",
    "\n",
    "            for img in self.images:\n",
    "                img = ImgF.to_tensor(img)\n",
    "                mean += img.mean(dim=(1, 2))\n",
    "                std += img.std(dim=(1, 2))\n",
    "\n",
    "            mean /= self.length\n",
    "            std /= self.length\n",
    "\n",
    "            normaliser = v2.Normalize(mean.tolist(), std.tolist())\n",
    "        self.normaliser = normaliser\n",
    "\n",
    "        # build the transformer\n",
    "        self.imgTransformer = self._transformBuild()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx\n",
    "\n",
    "        return self.imgTransformer(self.images[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# img255Scaler = v2.Lambda(lambda x: x / 255.0)\n",
    "# trainSet = ImageDataset(ds[\"train\"], True, True, True, True, img255Scaler)\n",
    "# test if augmentations are overdoing it\n",
    "trainSet = ImageDataset(ds[\"train\"]) #, True, True, True, True)\n",
    "trainSetAugmented = ImageDataset(ds[\"train\"], True, True, True, True)\n",
    "valSet = ImageDataset(ds[\"validation\"], normaliser=trainSet.normaliser)\n",
    "testSet = ImageDataset(ds[\"test\"], normaliser=trainSet.normaliser)\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "trainLoaderAugmented = DataLoader(trainSetAugmented, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSize = trainSet[0][0].size()\n",
    "sampleSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate if the image is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.099147..2.6807408].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = trainSet[0][0].permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = ds[\"train\"][0][\"image\"]\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, device, dataLoader):\n",
    "    \"\"\"Infer the predictions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for inference.\n",
    "        device (torch.device): The device to use.\n",
    "        dataLoader (DataLoader): The data to infer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The predictions and the actual labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    predProba = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            actual.append(labels)\n",
    "            predProba.append(outputs)\n",
    "\n",
    "    return torch.cat(predProba), torch.cat(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    trainLoader,\n",
    "    valLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=100,\n",
    "    earlyStopping=10,\n",
    "):\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        device (torch.device): The device to use.\n",
    "        trainLoader (DataLoader): The training data.\n",
    "        valLoader (DataLoader): The validation data.\n",
    "        criterion (_type_): The loss function.\n",
    "        optimizer (_type_): The optimizer.\n",
    "        n_epochs (int, optional): Number of epochs to train for. Defaults to 100.\n",
    "        earlyStopping (int, optional): Number of epochs to wait before stopping training if no improvement is made\n",
    "            on the validation loss. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    optimScheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.2)\n",
    "\n",
    "    # Training loop\n",
    "    earlyStopping = 10\n",
    "    bestValLoss = float(\"inf\")\n",
    "    bestModelState = None\n",
    "    patience = 0\n",
    "\n",
    "    trainLosses = []\n",
    "    valLosses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        trainLoss = 0\n",
    "        trainAcc = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainLoader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            trainLoss += loss.item()\n",
    "            trainAcc += acc.item()\n",
    "\n",
    "            # Print every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{n_epochs}], Step [{batch_idx}/{len(trainLoader)}], Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        trainLoss /= len(trainLoader)\n",
    "        trainAcc /= len(trainLoader)\n",
    "        trainLosses.append(trainLoss)\n",
    "\n",
    "        # Validation loss\n",
    "        pred, actual = infer(model, device, valLoader)\n",
    "        valLoss = criterion(pred, actual)\n",
    "        valAcc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "\n",
    "        valLosses.append(valLoss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}], Train Loss: {trainLoss:.4f}, Val Loss: {valLoss:.4f}, Train Acc: {trainAcc:.4f}, Val Acc: {valAcc:.4f}\"\n",
    "        )\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Early stopping\n",
    "        if valLoss < bestValLoss:\n",
    "            bestValLoss = valLoss\n",
    "            bestModelState = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > earlyStopping:\n",
    "                break\n",
    "\n",
    "        optimScheduler.step(valLoss)\n",
    "\n",
    "    # Save the model\n",
    "    # torch.save(bestModelState, \"model.pth\")\n",
    "    model.load_state_dict(bestModelState)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2d, BatchNorm2d and activation function.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, in_chn, out_chn, activation, kernel_size=3, alpha=1, stride=1, group=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2  # for same padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chn,\n",
    "            out_chn,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "            groups=group,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_chn)\n",
    "        self.activate = activation() if activation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activate is not None:\n",
    "            x = self.activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeAndExcite(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_chn, reduction=4):\n",
    "        super().__init__()\n",
    "        reduction_chn = in_chn // reduction\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(\n",
    "                1\n",
    "            ),  # 1,16,x,y->1,1,16,1 each channel is reduce to one value\n",
    "            nn.Flatten(),  #  1,16\n",
    "            nn.Linear(in_chn, reduction_chn, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduction_chn, in_chn, bias=False),\n",
    "            nn.Hardsigmoid(),\n",
    "            nn.Unflatten(1, (in_chn, 1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        out_chn,\n",
    "        expansion_channel,\n",
    "        activation,\n",
    "        se_reduction=4,\n",
    "        se_flag=False,\n",
    "        kernel=3,\n",
    "        stride=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #         expansion_channel=int(in_chn*expansiton_factor)\n",
    "        self.use_residual = (stride == 1) and (in_chn == out_chn)\n",
    "\n",
    "        self.residual_block_layers: list[nn.Module] = [\n",
    "            ConvBlock(\n",
    "                in_chn, expansion_channel, activation=activation, kernel_size=1\n",
    "            ),  # expansion_part\n",
    "            ConvBlock(\n",
    "                expansion_channel,\n",
    "                expansion_channel,\n",
    "                activation=activation,\n",
    "                group=expansion_channel,\n",
    "                stride=stride,\n",
    "                kernel_size=kernel,\n",
    "            ),  # depthwise conv\n",
    "        ]\n",
    "\n",
    "        if se_flag is True:\n",
    "            self.residual_block_layers.extend(\n",
    "                [SqueezeAndExcite(expansion_channel, reduction=se_reduction)]\n",
    "            )\n",
    "\n",
    "        self.residual_block_layers.extend(\n",
    "            [ConvBlock(expansion_channel, out_chn, activation=None, kernel_size=1)]\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*self.residual_block_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + x if self.use_residual else self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Hardswish, ReLU\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"large\":\n",
    "            layers_config = [\n",
    "                # kernel, exp, out, SE, NL, stride\n",
    "                [3, 16, 16, False, ReLU, 1],\n",
    "                [3, 64, 24, False, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 1],\n",
    "                [5, 72, 40, True, ReLU, 2],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [3, 240, 80, False, Hardswish, 2],\n",
    "                [3, 200, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 480, 112, True, Hardswish, 1],\n",
    "                [3, 672, 112, True, Hardswish, 1],\n",
    "                [5, 672, 160, True, Hardswish, 2],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 960, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, Hardswish, 2],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 120, 48, True, Hardswish, 1],\n",
    "                [5, 144, 48, True, Hardswish, 1],\n",
    "                [5, 288, 96, True, Hardswish, 2],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "        \n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Step [0/1535], Loss: 5.2892, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [100/1535], Loss: 5.3428, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [200/1535], Loss: 7.3250, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [300/1535], Loss: 5.3347, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [400/1535], Loss: 5.3258, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [500/1535], Loss: 5.2997, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [600/1535], Loss: 5.3125, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [700/1535], Loss: 5.2933, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [800/1535], Loss: 5.3044, Accuracy: 0.0312\n",
      "Epoch [1/100], Step [900/1535], Loss: 5.2748, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1000/1535], Loss: 5.3505, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1100/1535], Loss: 5.3622, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1200/1535], Loss: 5.3204, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1300/1535], Loss: 5.2990, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1400/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1500/1535], Loss: 5.3436, Accuracy: 0.0000\n",
      "Epoch [1/100], Train Loss: 5.5529, Val Loss: 5.3220, Train Acc: 0.0053, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/1535], Loss: 5.3241, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [100/1535], Loss: 5.3061, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [200/1535], Loss: 5.3197, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [300/1535], Loss: 5.3433, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [400/1535], Loss: 5.3408, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [500/1535], Loss: 5.3452, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [600/1535], Loss: 5.2858, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [700/1535], Loss: 5.3027, Accuracy: 0.0312\n",
      "Epoch [2/100], Step [800/1535], Loss: 5.3326, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [900/1535], Loss: 5.3139, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1000/1535], Loss: 5.3382, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1100/1535], Loss: 5.3160, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [1200/1535], Loss: 5.2907, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1300/1535], Loss: 5.2758, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1400/1535], Loss: 5.3301, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1500/1535], Loss: 5.3249, Accuracy: 0.0000\n",
      "Epoch [2/100], Train Loss: 5.3210, Val Loss: 5.3172, Train Acc: 0.0052, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/1535], Loss: 5.3516, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [100/1535], Loss: 5.2775, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [200/1535], Loss: 5.3052, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [300/1535], Loss: 5.3219, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [400/1535], Loss: 5.2858, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [500/1535], Loss: 5.3592, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [600/1535], Loss: 5.2600, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [700/1535], Loss: 5.3209, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [800/1535], Loss: 5.3087, Accuracy: 0.0312\n",
      "Epoch [3/100], Step [900/1535], Loss: 5.3062, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1000/1535], Loss: 5.3293, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [1100/1535], Loss: 5.3152, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1200/1535], Loss: 5.3230, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [1300/1535], Loss: 5.3575, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1400/1535], Loss: 5.3395, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1500/1535], Loss: 5.3475, Accuracy: 0.0000\n",
      "Epoch [3/100], Train Loss: 5.3217, Val Loss: 5.3182, Train Acc: 0.0049, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/1535], Loss: 5.3392, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [100/1535], Loss: 5.2952, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [200/1535], Loss: 5.3353, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [300/1535], Loss: 5.3010, Accuracy: 0.0156\n",
      "Epoch [4/100], Step [400/1535], Loss: 5.3434, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [500/1535], Loss: 5.3058, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [600/1535], Loss: 5.3771, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [700/1535], Loss: 5.2880, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [800/1535], Loss: 5.3130, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [900/1535], Loss: 5.3343, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1000/1535], Loss: 5.3519, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1100/1535], Loss: 5.2821, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1200/1535], Loss: 5.3129, Accuracy: 0.0156\n",
      "Epoch [4/100], Step [1300/1535], Loss: 5.3215, Accuracy: 0.0156\n",
      "Epoch [4/100], Step [1400/1535], Loss: 5.3134, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1500/1535], Loss: 5.3853, Accuracy: 0.0000\n",
      "Epoch [4/100], Train Loss: 5.3210, Val Loss: 5.3190, Train Acc: 0.0049, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/1535], Loss: 5.2813, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [100/1535], Loss: 5.3317, Accuracy: 0.0156\n",
      "Epoch [5/100], Step [200/1535], Loss: 5.3155, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [300/1535], Loss: 5.3459, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [400/1535], Loss: 5.3119, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [500/1535], Loss: 5.3247, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [600/1535], Loss: 5.3366, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [700/1535], Loss: 5.3076, Accuracy: 0.0156\n",
      "Epoch [5/100], Step [800/1535], Loss: 5.3054, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [900/1535], Loss: 5.3353, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1000/1535], Loss: 5.3314, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1100/1535], Loss: 5.2917, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1200/1535], Loss: 5.2867, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1300/1535], Loss: 5.3540, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1400/1535], Loss: 5.3329, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1500/1535], Loss: 5.2814, Accuracy: 0.0000\n",
      "Epoch [5/100], Train Loss: 5.3213, Val Loss: 5.3204, Train Acc: 0.0047, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/1535], Loss: 5.2867, Accuracy: 0.0312\n",
      "Epoch [6/100], Step [100/1535], Loss: 5.3346, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [200/1535], Loss: 5.3340, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [300/1535], Loss: 5.3535, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [400/1535], Loss: 5.2949, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [500/1535], Loss: 5.3107, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [600/1535], Loss: 5.3091, Accuracy: 0.0156\n",
      "Epoch [6/100], Step [700/1535], Loss: 5.3017, Accuracy: 0.0156\n",
      "Epoch [6/100], Step [800/1535], Loss: 5.3291, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [900/1535], Loss: 5.3076, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [1000/1535], Loss: 5.3198, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [1100/1535], Loss: 5.3418, Accuracy: 0.0156\n",
      "Epoch [6/100], Step [1200/1535], Loss: 5.2931, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [1300/1535], Loss: 5.2758, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [1400/1535], Loss: 5.3254, Accuracy: 0.0000\n",
      "Epoch [6/100], Step [1500/1535], Loss: 5.3373, Accuracy: 0.0000\n",
      "Epoch [6/100], Train Loss: 5.3326, Val Loss: 4512.4956, Train Acc: 0.0051, Val Acc: 0.0047\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/1535], Loss: 13.3242, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [100/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [200/1535], Loss: 5.3066, Accuracy: 0.0312\n",
      "Epoch [7/100], Step [300/1535], Loss: 5.3023, Accuracy: 0.0156\n",
      "Epoch [7/100], Step [400/1535], Loss: 5.3141, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [500/1535], Loss: 5.3191, Accuracy: 0.0156\n",
      "Epoch [7/100], Step [600/1535], Loss: 5.3037, Accuracy: 0.0156\n",
      "Epoch [7/100], Step [700/1535], Loss: 5.3042, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [800/1535], Loss: 5.2861, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [900/1535], Loss: 5.3026, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [1000/1535], Loss: 5.2986, Accuracy: 0.0156\n",
      "Epoch [7/100], Step [1100/1535], Loss: 5.2913, Accuracy: 0.0156\n",
      "Epoch [7/100], Step [1200/1535], Loss: 5.3027, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [1300/1535], Loss: 5.2900, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [1400/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [7/100], Step [1500/1535], Loss: 5.3081, Accuracy: 0.0000\n",
      "Epoch [7/100], Train Loss: 5.3129, Val Loss: 5.3357, Train Acc: 0.0051, Val Acc: 0.0052\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/1535], Loss: 5.2833, Accuracy: 0.0156\n",
      "Epoch [8/100], Step [100/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [200/1535], Loss: 5.3075, Accuracy: 0.0156\n",
      "Epoch [8/100], Step [300/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [400/1535], Loss: 5.3221, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [500/1535], Loss: 5.2971, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [600/1535], Loss: 5.3046, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [700/1535], Loss: 5.3076, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [800/1535], Loss: 5.3028, Accuracy: 0.0156\n",
      "Epoch [8/100], Step [900/1535], Loss: 5.3172, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1000/1535], Loss: 5.3081, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1100/1535], Loss: 5.2806, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1200/1535], Loss: 5.2920, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1300/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1400/1535], Loss: 5.2933, Accuracy: 0.0000\n",
      "Epoch [8/100], Step [1500/1535], Loss: 5.3137, Accuracy: 0.0000\n",
      "Epoch [8/100], Train Loss: 5.3032, Val Loss: 5.3033, Train Acc: 0.0050, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/1535], Loss: 5.2911, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [100/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [200/1535], Loss: 5.2909, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [300/1535], Loss: 5.3125, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [400/1535], Loss: 5.3050, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [500/1535], Loss: 5.3127, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [600/1535], Loss: 5.3241, Accuracy: 0.0156\n",
      "Epoch [9/100], Step [700/1535], Loss: 5.3087, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [800/1535], Loss: 5.3012, Accuracy: 0.0156\n",
      "Epoch [9/100], Step [900/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1000/1535], Loss: 5.2772, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1100/1535], Loss: 5.3028, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1200/1535], Loss: 5.2952, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1300/1535], Loss: 5.2854, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1400/1535], Loss: 5.2907, Accuracy: 0.0000\n",
      "Epoch [9/100], Step [1500/1535], Loss: 5.3329, Accuracy: 0.0000\n",
      "Epoch [9/100], Train Loss: 5.3031, Val Loss: 5.3026, Train Acc: 0.0045, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/1535], Loss: 5.3166, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [100/1535], Loss: 5.3090, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [200/1535], Loss: 5.2849, Accuracy: 0.0312\n",
      "Epoch [10/100], Step [300/1535], Loss: 5.3116, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [400/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [10/100], Step [500/1535], Loss: 5.3034, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [600/1535], Loss: 5.2996, Accuracy: 0.0156\n",
      "Epoch [10/100], Step [700/1535], Loss: 5.3267, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [800/1535], Loss: 5.3084, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [900/1535], Loss: 5.3084, Accuracy: 0.0156\n",
      "Epoch [10/100], Step [1000/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [1100/1535], Loss: 5.3122, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [1200/1535], Loss: 5.3288, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [1300/1535], Loss: 5.3027, Accuracy: 0.0156\n",
      "Epoch [10/100], Step [1400/1535], Loss: 5.3060, Accuracy: 0.0000\n",
      "Epoch [10/100], Step [1500/1535], Loss: 5.3114, Accuracy: 0.0156\n",
      "Epoch [10/100], Train Loss: 5.3035, Val Loss: 5.3021, Train Acc: 0.0048, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [100/1535], Loss: 5.3020, Accuracy: 0.0156\n",
      "Epoch [11/100], Step [200/1535], Loss: 5.3110, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [300/1535], Loss: 5.3077, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [400/1535], Loss: 5.3202, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [500/1535], Loss: 5.3121, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [600/1535], Loss: 5.2880, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [700/1535], Loss: 5.3154, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [800/1535], Loss: 5.2839, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [900/1535], Loss: 5.3092, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [1000/1535], Loss: 5.3239, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [1100/1535], Loss: 5.2922, Accuracy: 0.0156\n",
      "Epoch [11/100], Step [1200/1535], Loss: 5.3238, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [1300/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [1400/1535], Loss: 5.3163, Accuracy: 0.0000\n",
      "Epoch [11/100], Step [1500/1535], Loss: 5.2895, Accuracy: 0.0312\n",
      "Epoch [11/100], Train Loss: 5.3037, Val Loss: 5.3012, Train Acc: 0.0044, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/1535], Loss: 5.3040, Accuracy: 0.0156\n",
      "Epoch [12/100], Step [100/1535], Loss: 5.3107, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [200/1535], Loss: 5.2982, Accuracy: 0.0156\n",
      "Epoch [12/100], Step [300/1535], Loss: 5.2949, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [400/1535], Loss: 5.3126, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [500/1535], Loss: 5.3062, Accuracy: 0.0156\n",
      "Epoch [12/100], Step [600/1535], Loss: 5.3183, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [700/1535], Loss: 5.3055, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [800/1535], Loss: 5.2895, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [900/1535], Loss: 5.3214, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [1000/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [12/100], Step [1100/1535], Loss: 5.3170, Accuracy: 0.0156\n",
      "Epoch [12/100], Step [1200/1535], Loss: 5.3264, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [1300/1535], Loss: 5.3182, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [1400/1535], Loss: 5.3071, Accuracy: 0.0000\n",
      "Epoch [12/100], Step [1500/1535], Loss: 5.2900, Accuracy: 0.0000\n",
      "Epoch [12/100], Train Loss: 5.3034, Val Loss: 5.3013, Train Acc: 0.0047, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/1535], Loss: 5.3047, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [100/1535], Loss: 5.3100, Accuracy: 0.0156\n",
      "Epoch [13/100], Step [200/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [300/1535], Loss: 5.2819, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [400/1535], Loss: 5.3110, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [500/1535], Loss: 5.3243, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [600/1535], Loss: 5.2851, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [700/1535], Loss: 5.2976, Accuracy: 0.0156\n",
      "Epoch [13/100], Step [800/1535], Loss: 5.3032, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [900/1535], Loss: 5.3163, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [1000/1535], Loss: 5.2895, Accuracy: 0.0156\n",
      "Epoch [13/100], Step [1100/1535], Loss: 5.3075, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [1200/1535], Loss: 5.3140, Accuracy: 0.0156\n",
      "Epoch [13/100], Step [1300/1535], Loss: 5.2929, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [1400/1535], Loss: 5.2940, Accuracy: 0.0000\n",
      "Epoch [13/100], Step [1500/1535], Loss: 5.3199, Accuracy: 0.0312\n",
      "Epoch [13/100], Train Loss: 5.3034, Val Loss: 5.3019, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/1535], Loss: 5.2921, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [100/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [200/1535], Loss: 5.3076, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [300/1535], Loss: 5.3140, Accuracy: 0.0156\n",
      "Epoch [14/100], Step [400/1535], Loss: 5.2886, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [500/1535], Loss: 5.3053, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [600/1535], Loss: 5.2884, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [700/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [800/1535], Loss: 5.3221, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [900/1535], Loss: 5.3181, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [1000/1535], Loss: 5.2968, Accuracy: 0.0312\n",
      "Epoch [14/100], Step [1100/1535], Loss: 5.2945, Accuracy: 0.0156\n",
      "Epoch [14/100], Step [1200/1535], Loss: 5.2908, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [1300/1535], Loss: 5.3132, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [1400/1535], Loss: 5.2859, Accuracy: 0.0000\n",
      "Epoch [14/100], Step [1500/1535], Loss: 5.2976, Accuracy: 0.0312\n",
      "Epoch [14/100], Train Loss: 5.3034, Val Loss: 5.3024, Train Acc: 0.0046, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [100/1535], Loss: 5.2852, Accuracy: 0.0312\n",
      "Epoch [15/100], Step [200/1535], Loss: 5.3066, Accuracy: 0.0156\n",
      "Epoch [15/100], Step [300/1535], Loss: 5.2889, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [400/1535], Loss: 5.3120, Accuracy: 0.0156\n",
      "Epoch [15/100], Step [500/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [600/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [700/1535], Loss: 5.3269, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [800/1535], Loss: 5.3151, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [900/1535], Loss: 5.3083, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [1000/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [1100/1535], Loss: 5.2853, Accuracy: 0.0156\n",
      "Epoch [15/100], Step [1200/1535], Loss: 5.3162, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [1300/1535], Loss: 5.2767, Accuracy: 0.0156\n",
      "Epoch [15/100], Step [1400/1535], Loss: 5.3048, Accuracy: 0.0000\n",
      "Epoch [15/100], Step [1500/1535], Loss: 5.2915, Accuracy: 0.0156\n",
      "Epoch [15/100], Train Loss: 5.3034, Val Loss: 5.3029, Train Acc: 0.0046, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/1535], Loss: 5.2921, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [100/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [200/1535], Loss: 5.2873, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [300/1535], Loss: 5.3081, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [400/1535], Loss: 5.3125, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [500/1535], Loss: 5.3134, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [600/1535], Loss: 5.2890, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [700/1535], Loss: 5.2960, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [800/1535], Loss: 5.3050, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [900/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [1000/1535], Loss: 5.3075, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [1100/1535], Loss: 5.2932, Accuracy: 0.0156\n",
      "Epoch [16/100], Step [1200/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [1300/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [1400/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [16/100], Step [1500/1535], Loss: 5.3042, Accuracy: 0.0156\n",
      "Epoch [16/100], Train Loss: 5.3004, Val Loss: 5.3019, Train Acc: 0.0047, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/1535], Loss: 5.2965, Accuracy: 0.0156\n",
      "Epoch [17/100], Step [100/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [200/1535], Loss: 5.2929, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [300/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [400/1535], Loss: 5.3061, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [500/1535], Loss: 5.3037, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [600/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [700/1535], Loss: 5.2883, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [800/1535], Loss: 5.3042, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [900/1535], Loss: 5.2987, Accuracy: 0.0156\n",
      "Epoch [17/100], Step [1000/1535], Loss: 5.3024, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [1100/1535], Loss: 5.3066, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [1200/1535], Loss: 5.2971, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [1300/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [1400/1535], Loss: 5.3069, Accuracy: 0.0000\n",
      "Epoch [17/100], Step [1500/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [17/100], Train Loss: 5.2996, Val Loss: 5.2994, Train Acc: 0.0045, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/1535], Loss: 5.2925, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [100/1535], Loss: 5.2963, Accuracy: 0.0156\n",
      "Epoch [18/100], Step [200/1535], Loss: 5.2938, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [300/1535], Loss: 5.2931, Accuracy: 0.0156\n",
      "Epoch [18/100], Step [400/1535], Loss: 5.3074, Accuracy: 0.0156\n",
      "Epoch [18/100], Step [500/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [600/1535], Loss: 5.3058, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [700/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [800/1535], Loss: 5.3018, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [900/1535], Loss: 5.3005, Accuracy: 0.0156\n",
      "Epoch [18/100], Step [1000/1535], Loss: 5.3019, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [1100/1535], Loss: 5.2962, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [1200/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [1300/1535], Loss: 5.3039, Accuracy: 0.0000\n",
      "Epoch [18/100], Step [1400/1535], Loss: 5.2996, Accuracy: 0.0156\n",
      "Epoch [18/100], Step [1500/1535], Loss: 5.3019, Accuracy: 0.0000\n",
      "Epoch [18/100], Train Loss: 5.2995, Val Loss: 5.3006, Train Acc: 0.0045, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/1535], Loss: 5.2994, Accuracy: 0.0156\n",
      "Epoch [19/100], Step [100/1535], Loss: 5.3001, Accuracy: 0.0156\n",
      "Epoch [19/100], Step [200/1535], Loss: 5.3013, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [300/1535], Loss: 5.2904, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [400/1535], Loss: 5.2984, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [500/1535], Loss: 5.2864, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [600/1535], Loss: 5.3051, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [700/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [800/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [900/1535], Loss: 5.2934, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [1000/1535], Loss: 5.3020, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [1100/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [1200/1535], Loss: 5.2998, Accuracy: 0.0156\n",
      "Epoch [19/100], Step [1300/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [19/100], Step [1400/1535], Loss: 5.2971, Accuracy: 0.0156\n",
      "Epoch [19/100], Step [1500/1535], Loss: 5.3031, Accuracy: 0.0000\n",
      "Epoch [19/100], Train Loss: 5.2995, Val Loss: 5.2987, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/1535], Loss: 5.2971, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [100/1535], Loss: 5.2950, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [200/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [300/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [400/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [500/1535], Loss: 5.3062, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [600/1535], Loss: 5.2995, Accuracy: 0.0156\n",
      "Epoch [20/100], Step [700/1535], Loss: 5.2948, Accuracy: 0.0156\n",
      "Epoch [20/100], Step [800/1535], Loss: 5.3006, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [900/1535], Loss: 5.3064, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [1000/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [1100/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [1200/1535], Loss: 5.3016, Accuracy: 0.0156\n",
      "Epoch [20/100], Step [1300/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [1400/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [20/100], Step [1500/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [20/100], Train Loss: 5.2995, Val Loss: 5.2997, Train Acc: 0.0047, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [100/1535], Loss: 5.2966, Accuracy: 0.0156\n",
      "Epoch [21/100], Step [200/1535], Loss: 5.3052, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [300/1535], Loss: 5.2959, Accuracy: 0.0156\n",
      "Epoch [21/100], Step [400/1535], Loss: 5.3084, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [500/1535], Loss: 5.2938, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [600/1535], Loss: 5.3041, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [700/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [800/1535], Loss: 5.2935, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [900/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [21/100], Step [1000/1535], Loss: 5.3052, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [1100/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [1200/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [1300/1535], Loss: 5.3015, Accuracy: 0.0312\n",
      "Epoch [21/100], Step [1400/1535], Loss: 5.2950, Accuracy: 0.0000\n",
      "Epoch [21/100], Step [1500/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [21/100], Train Loss: 5.2995, Val Loss: 5.2988, Train Acc: 0.0046, Val Acc: 0.0050\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/1535], Loss: 5.3020, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [100/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [200/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [300/1535], Loss: 5.2997, Accuracy: 0.0156\n",
      "Epoch [22/100], Step [400/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [500/1535], Loss: 5.3007, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [600/1535], Loss: 5.2995, Accuracy: 0.0156\n",
      "Epoch [22/100], Step [700/1535], Loss: 5.2959, Accuracy: 0.0156\n",
      "Epoch [22/100], Step [800/1535], Loss: 5.3055, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [900/1535], Loss: 5.3032, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [1000/1535], Loss: 5.2950, Accuracy: 0.0156\n",
      "Epoch [22/100], Step [1100/1535], Loss: 5.3026, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [1200/1535], Loss: 5.3049, Accuracy: 0.0156\n",
      "Epoch [22/100], Step [1300/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [1400/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [22/100], Step [1500/1535], Loss: 5.3054, Accuracy: 0.0000\n",
      "Epoch [22/100], Train Loss: 5.2995, Val Loss: 5.2994, Train Acc: 0.0041, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [100/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [200/1535], Loss: 5.2988, Accuracy: 0.0156\n",
      "Epoch [23/100], Step [300/1535], Loss: 5.2949, Accuracy: 0.0156\n",
      "Epoch [23/100], Step [400/1535], Loss: 5.2943, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [500/1535], Loss: 5.3058, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [600/1535], Loss: 5.2977, Accuracy: 0.0156\n",
      "Epoch [23/100], Step [700/1535], Loss: 5.3061, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [800/1535], Loss: 5.2965, Accuracy: 0.0156\n",
      "Epoch [23/100], Step [900/1535], Loss: 5.3006, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [1000/1535], Loss: 5.3018, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [1100/1535], Loss: 5.3069, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [1200/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [23/100], Step [1300/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [1400/1535], Loss: 5.3014, Accuracy: 0.0000\n",
      "Epoch [23/100], Step [1500/1535], Loss: 5.3024, Accuracy: 0.0000\n",
      "Epoch [23/100], Train Loss: 5.2995, Val Loss: 5.3000, Train Acc: 0.0046, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/1535], Loss: 5.2941, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [100/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [200/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [300/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [400/1535], Loss: 5.3009, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [500/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [600/1535], Loss: 5.2948, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [700/1535], Loss: 5.3013, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [800/1535], Loss: 5.2931, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [900/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [1000/1535], Loss: 5.3016, Accuracy: 0.0156\n",
      "Epoch [24/100], Step [1100/1535], Loss: 5.2952, Accuracy: 0.0000\n",
      "Epoch [24/100], Step [1200/1535], Loss: 5.2986, Accuracy: 0.0156\n",
      "Epoch [24/100], Step [1300/1535], Loss: 5.3020, Accuracy: 0.0156\n",
      "Epoch [24/100], Step [1400/1535], Loss: 5.3017, Accuracy: 0.0156\n",
      "Epoch [24/100], Step [1500/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [24/100], Train Loss: 5.2985, Val Loss: 5.3014, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/1535], Loss: 5.3033, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [100/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [200/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [300/1535], Loss: 5.2927, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [400/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [500/1535], Loss: 5.3006, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [600/1535], Loss: 5.2918, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [700/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [800/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [900/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1000/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1100/1535], Loss: 5.2964, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1200/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1300/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1400/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [25/100], Step [1500/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [25/100], Train Loss: 5.2985, Val Loss: 5.2992, Train Acc: 0.0048, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [100/1535], Loss: 5.2965, Accuracy: 0.0156\n",
      "Epoch [26/100], Step [200/1535], Loss: 5.3018, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [300/1535], Loss: 5.3009, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [400/1535], Loss: 5.2980, Accuracy: 0.0156\n",
      "Epoch [26/100], Step [500/1535], Loss: 5.3009, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [600/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [700/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [800/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [900/1535], Loss: 5.2967, Accuracy: 0.0156\n",
      "Epoch [26/100], Step [1000/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [1100/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [1200/1535], Loss: 5.3006, Accuracy: 0.0156\n",
      "Epoch [26/100], Step [1300/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [1400/1535], Loss: 5.2934, Accuracy: 0.0000\n",
      "Epoch [26/100], Step [1500/1535], Loss: 5.2942, Accuracy: 0.0156\n",
      "Epoch [26/100], Train Loss: 5.2984, Val Loss: 5.3007, Train Acc: 0.0048, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [100/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [200/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [300/1535], Loss: 5.2949, Accuracy: 0.0156\n",
      "Epoch [27/100], Step [400/1535], Loss: 5.3001, Accuracy: 0.0156\n",
      "Epoch [27/100], Step [500/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [600/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [700/1535], Loss: 5.3001, Accuracy: 0.0156\n",
      "Epoch [27/100], Step [800/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [900/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [1000/1535], Loss: 5.2972, Accuracy: 0.0156\n",
      "Epoch [27/100], Step [1100/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [1200/1535], Loss: 5.3021, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [1300/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [1400/1535], Loss: 5.3033, Accuracy: 0.0000\n",
      "Epoch [27/100], Step [1500/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [27/100], Train Loss: 5.2984, Val Loss: 5.2990, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [100/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [200/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [300/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [400/1535], Loss: 5.2984, Accuracy: 0.0156\n",
      "Epoch [28/100], Step [500/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [600/1535], Loss: 5.3009, Accuracy: 0.0156\n",
      "Epoch [28/100], Step [700/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [800/1535], Loss: 5.2994, Accuracy: 0.0156\n",
      "Epoch [28/100], Step [900/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [1000/1535], Loss: 5.2986, Accuracy: 0.0156\n",
      "Epoch [28/100], Step [1100/1535], Loss: 5.2942, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [1200/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [1300/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [1400/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [28/100], Step [1500/1535], Loss: 5.3060, Accuracy: 0.0000\n",
      "Epoch [28/100], Train Loss: 5.2982, Val Loss: 5.2989, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/1535], Loss: 5.2918, Accuracy: 0.0312\n",
      "Epoch [29/100], Step [100/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [200/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [300/1535], Loss: 5.2951, Accuracy: 0.0156\n",
      "Epoch [29/100], Step [400/1535], Loss: 5.2954, Accuracy: 0.0312\n",
      "Epoch [29/100], Step [500/1535], Loss: 5.3019, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [600/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [700/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [800/1535], Loss: 5.2978, Accuracy: 0.0156\n",
      "Epoch [29/100], Step [900/1535], Loss: 5.2927, Accuracy: 0.0156\n",
      "Epoch [29/100], Step [1000/1535], Loss: 5.3011, Accuracy: 0.0156\n",
      "Epoch [29/100], Step [1100/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [1200/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [1300/1535], Loss: 5.2930, Accuracy: 0.0000\n",
      "Epoch [29/100], Step [1400/1535], Loss: 5.3033, Accuracy: 0.0156\n",
      "Epoch [29/100], Step [1500/1535], Loss: 5.2900, Accuracy: 0.0156\n",
      "Epoch [29/100], Train Loss: 5.2982, Val Loss: 5.2996, Train Acc: 0.0048, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [100/1535], Loss: 5.3043, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [200/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [300/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [400/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [500/1535], Loss: 5.2937, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [600/1535], Loss: 5.2994, Accuracy: 0.0156\n",
      "Epoch [30/100], Step [700/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [800/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [900/1535], Loss: 5.2982, Accuracy: 0.0156\n",
      "Epoch [30/100], Step [1000/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [1100/1535], Loss: 5.2929, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [1200/1535], Loss: 5.2951, Accuracy: 0.0156\n",
      "Epoch [30/100], Step [1300/1535], Loss: 5.2940, Accuracy: 0.0000\n",
      "Epoch [30/100], Step [1400/1535], Loss: 5.2974, Accuracy: 0.0312\n",
      "Epoch [30/100], Step [1500/1535], Loss: 5.2971, Accuracy: 0.0156\n",
      "Epoch [30/100], Train Loss: 5.2982, Val Loss: 5.2987, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [100/1535], Loss: 5.2946, Accuracy: 0.0156\n",
      "Epoch [31/100], Step [200/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [300/1535], Loss: 5.2993, Accuracy: 0.0156\n",
      "Epoch [31/100], Step [400/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [500/1535], Loss: 5.2993, Accuracy: 0.0312\n",
      "Epoch [31/100], Step [600/1535], Loss: 5.2983, Accuracy: 0.0156\n",
      "Epoch [31/100], Step [700/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [800/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [900/1535], Loss: 5.2961, Accuracy: 0.0156\n",
      "Epoch [31/100], Step [1000/1535], Loss: 5.2971, Accuracy: 0.0156\n",
      "Epoch [31/100], Step [1100/1535], Loss: 5.2998, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [1200/1535], Loss: 5.2914, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [1300/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [1400/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [31/100], Step [1500/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [31/100], Train Loss: 5.2982, Val Loss: 5.2987, Train Acc: 0.0050, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/1535], Loss: 5.3001, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [100/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [200/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [32/100], Step [300/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [400/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [500/1535], Loss: 5.3003, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [600/1535], Loss: 5.3026, Accuracy: 0.0156\n",
      "Epoch [32/100], Step [700/1535], Loss: 5.2955, Accuracy: 0.0156\n",
      "Epoch [32/100], Step [800/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [900/1535], Loss: 5.2954, Accuracy: 0.0156\n",
      "Epoch [32/100], Step [1000/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [1100/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [1200/1535], Loss: 5.2942, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [1300/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [32/100], Step [1400/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [32/100], Step [1500/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [32/100], Train Loss: 5.2981, Val Loss: 5.3010, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [100/1535], Loss: 5.2964, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [200/1535], Loss: 5.2977, Accuracy: 0.0156\n",
      "Epoch [33/100], Step [300/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [400/1535], Loss: 5.2947, Accuracy: 0.0156\n",
      "Epoch [33/100], Step [500/1535], Loss: 5.2935, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [600/1535], Loss: 5.2980, Accuracy: 0.0156\n",
      "Epoch [33/100], Step [700/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [800/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [900/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [1000/1535], Loss: 5.3069, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [1100/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [1200/1535], Loss: 5.2970, Accuracy: 0.0156\n",
      "Epoch [33/100], Step [1300/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [33/100], Step [1400/1535], Loss: 5.3027, Accuracy: 0.0000\n",
      "Epoch [33/100], Step [1500/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [33/100], Train Loss: 5.2981, Val Loss: 5.3003, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/1535], Loss: 5.3034, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [100/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [200/1535], Loss: 5.2926, Accuracy: 0.0312\n",
      "Epoch [34/100], Step [300/1535], Loss: 5.2948, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [400/1535], Loss: 5.2946, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [500/1535], Loss: 5.2982, Accuracy: 0.0312\n",
      "Epoch [34/100], Step [600/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [700/1535], Loss: 5.3024, Accuracy: 0.0156\n",
      "Epoch [34/100], Step [800/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [900/1535], Loss: 5.2961, Accuracy: 0.0156\n",
      "Epoch [34/100], Step [1000/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [1100/1535], Loss: 5.2996, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [1200/1535], Loss: 5.3025, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [1300/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [1400/1535], Loss: 5.2948, Accuracy: 0.0000\n",
      "Epoch [34/100], Step [1500/1535], Loss: 5.3011, Accuracy: 0.0156\n",
      "Epoch [34/100], Train Loss: 5.2981, Val Loss: 5.3006, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [100/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [200/1535], Loss: 5.2967, Accuracy: 0.0156\n",
      "Epoch [35/100], Step [300/1535], Loss: 5.3035, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [400/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [500/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [600/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [700/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [800/1535], Loss: 5.2958, Accuracy: 0.0156\n",
      "Epoch [35/100], Step [900/1535], Loss: 5.3045, Accuracy: 0.0156\n",
      "Epoch [35/100], Step [1000/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [1100/1535], Loss: 5.2950, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [1200/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [1300/1535], Loss: 5.2982, Accuracy: 0.0156\n",
      "Epoch [35/100], Step [1400/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [35/100], Step [1500/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [35/100], Train Loss: 5.2981, Val Loss: 5.2995, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/1535], Loss: 5.3031, Accuracy: 0.0156\n",
      "Epoch [36/100], Step [100/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [200/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [300/1535], Loss: 5.2960, Accuracy: 0.0156\n",
      "Epoch [36/100], Step [400/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [500/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [600/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [700/1535], Loss: 5.3004, Accuracy: 0.0156\n",
      "Epoch [36/100], Step [800/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [900/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1000/1535], Loss: 5.2936, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1100/1535], Loss: 5.3015, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1200/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1300/1535], Loss: 5.3026, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1400/1535], Loss: 5.3025, Accuracy: 0.0000\n",
      "Epoch [36/100], Step [1500/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [36/100], Train Loss: 5.2981, Val Loss: 5.2984, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/1535], Loss: 5.3005, Accuracy: 0.0156\n",
      "Epoch [37/100], Step [100/1535], Loss: 5.2962, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [200/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [300/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [400/1535], Loss: 5.2937, Accuracy: 0.0156\n",
      "Epoch [37/100], Step [500/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [600/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [700/1535], Loss: 5.3019, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [800/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [900/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [1000/1535], Loss: 5.3003, Accuracy: 0.0156\n",
      "Epoch [37/100], Step [1100/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [1200/1535], Loss: 5.2997, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [1300/1535], Loss: 5.3009, Accuracy: 0.0156\n",
      "Epoch [37/100], Step [1400/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [37/100], Step [1500/1535], Loss: 5.2975, Accuracy: 0.0156\n",
      "Epoch [37/100], Train Loss: 5.2981, Val Loss: 5.2991, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/1535], Loss: 5.2979, Accuracy: 0.0156\n",
      "Epoch [38/100], Step [100/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [200/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [300/1535], Loss: 5.2985, Accuracy: 0.0156\n",
      "Epoch [38/100], Step [400/1535], Loss: 5.2960, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [500/1535], Loss: 5.2948, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [600/1535], Loss: 5.3027, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [700/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [800/1535], Loss: 5.2966, Accuracy: 0.0156\n",
      "Epoch [38/100], Step [900/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [1000/1535], Loss: 5.2984, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [1100/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [1200/1535], Loss: 5.2968, Accuracy: 0.0156\n",
      "Epoch [38/100], Step [1300/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [1400/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [38/100], Step [1500/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [38/100], Train Loss: 5.2981, Val Loss: 5.2986, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [100/1535], Loss: 5.2983, Accuracy: 0.0156\n",
      "Epoch [39/100], Step [200/1535], Loss: 5.2967, Accuracy: 0.0156\n",
      "Epoch [39/100], Step [300/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [400/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [500/1535], Loss: 5.2980, Accuracy: 0.0156\n",
      "Epoch [39/100], Step [600/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [700/1535], Loss: 5.2940, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [800/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [900/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [1000/1535], Loss: 5.2976, Accuracy: 0.0156\n",
      "Epoch [39/100], Step [1100/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [1200/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [1300/1535], Loss: 5.2997, Accuracy: 0.0000\n",
      "Epoch [39/100], Step [1400/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [39/100], Step [1500/1535], Loss: 5.3007, Accuracy: 0.0000\n",
      "Epoch [39/100], Train Loss: 5.2981, Val Loss: 5.3009, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [100/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [200/1535], Loss: 5.3019, Accuracy: 0.0156\n",
      "Epoch [40/100], Step [300/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [400/1535], Loss: 5.3015, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [500/1535], Loss: 5.3022, Accuracy: 0.0156\n",
      "Epoch [40/100], Step [600/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [700/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [800/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [900/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [1000/1535], Loss: 5.3029, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [1100/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [1200/1535], Loss: 5.2962, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [1300/1535], Loss: 5.2958, Accuracy: 0.0312\n",
      "Epoch [40/100], Step [1400/1535], Loss: 5.3021, Accuracy: 0.0000\n",
      "Epoch [40/100], Step [1500/1535], Loss: 5.2964, Accuracy: 0.0000\n",
      "Epoch [40/100], Train Loss: 5.2981, Val Loss: 5.2995, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [100/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [200/1535], Loss: 5.2944, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [300/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [400/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [500/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [600/1535], Loss: 5.3011, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [700/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [800/1535], Loss: 5.3049, Accuracy: 0.0312\n",
      "Epoch [41/100], Step [900/1535], Loss: 5.3014, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [1000/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [1100/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [41/100], Step [1200/1535], Loss: 5.3018, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [1300/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [1400/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [41/100], Step [1500/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [41/100], Train Loss: 5.2981, Val Loss: 5.2992, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/1535], Loss: 5.3021, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [100/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [200/1535], Loss: 5.3035, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [300/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [400/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [500/1535], Loss: 5.2962, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [600/1535], Loss: 5.3007, Accuracy: 0.0156\n",
      "Epoch [42/100], Step [700/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [800/1535], Loss: 5.2925, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [900/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [1000/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [1100/1535], Loss: 5.2961, Accuracy: 0.0156\n",
      "Epoch [42/100], Step [1200/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [42/100], Step [1300/1535], Loss: 5.3008, Accuracy: 0.0156\n",
      "Epoch [42/100], Step [1400/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [42/100], Step [1500/1535], Loss: 5.2952, Accuracy: 0.0156\n",
      "Epoch [42/100], Train Loss: 5.2981, Val Loss: 5.2988, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/1535], Loss: 5.2999, Accuracy: 0.0156\n",
      "Epoch [43/100], Step [100/1535], Loss: 5.3003, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [200/1535], Loss: 5.2945, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [300/1535], Loss: 5.2947, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [400/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [500/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [600/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [700/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [800/1535], Loss: 5.2939, Accuracy: 0.0156\n",
      "Epoch [43/100], Step [900/1535], Loss: 5.3004, Accuracy: 0.0156\n",
      "Epoch [43/100], Step [1000/1535], Loss: 5.2979, Accuracy: 0.0156\n",
      "Epoch [43/100], Step [1100/1535], Loss: 5.2949, Accuracy: 0.0156\n",
      "Epoch [43/100], Step [1200/1535], Loss: 5.2943, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [1300/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [1400/1535], Loss: 5.2997, Accuracy: 0.0000\n",
      "Epoch [43/100], Step [1500/1535], Loss: 5.2952, Accuracy: 0.0156\n",
      "Epoch [43/100], Train Loss: 5.2981, Val Loss: 5.2983, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [100/1535], Loss: 5.2950, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [200/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [300/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [400/1535], Loss: 5.2998, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [500/1535], Loss: 5.2969, Accuracy: 0.0156\n",
      "Epoch [44/100], Step [600/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [700/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [800/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [900/1535], Loss: 5.2962, Accuracy: 0.0156\n",
      "Epoch [44/100], Step [1000/1535], Loss: 5.2936, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [1100/1535], Loss: 5.3024, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [1200/1535], Loss: 5.2944, Accuracy: 0.0156\n",
      "Epoch [44/100], Step [1300/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [1400/1535], Loss: 5.2939, Accuracy: 0.0000\n",
      "Epoch [44/100], Step [1500/1535], Loss: 5.2984, Accuracy: 0.0000\n",
      "Epoch [44/100], Train Loss: 5.2981, Val Loss: 5.3001, Train Acc: 0.0053, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [100/1535], Loss: 5.2943, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [200/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [300/1535], Loss: 5.2972, Accuracy: 0.0156\n",
      "Epoch [45/100], Step [400/1535], Loss: 5.2956, Accuracy: 0.0156\n",
      "Epoch [45/100], Step [500/1535], Loss: 5.2950, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [600/1535], Loss: 5.2959, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [700/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [800/1535], Loss: 5.3040, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [900/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1000/1535], Loss: 5.3023, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1100/1535], Loss: 5.2961, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1200/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1300/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1400/1535], Loss: 5.2949, Accuracy: 0.0000\n",
      "Epoch [45/100], Step [1500/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [45/100], Train Loss: 5.2981, Val Loss: 5.2996, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/1535], Loss: 5.2981, Accuracy: 0.0312\n",
      "Epoch [46/100], Step [100/1535], Loss: 5.2948, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [200/1535], Loss: 5.2962, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [300/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [400/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [500/1535], Loss: 5.2947, Accuracy: 0.0156\n",
      "Epoch [46/100], Step [600/1535], Loss: 5.3006, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [700/1535], Loss: 5.2954, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [800/1535], Loss: 5.2959, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [900/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1000/1535], Loss: 5.2991, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1100/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1200/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1300/1535], Loss: 5.2964, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1400/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [46/100], Step [1500/1535], Loss: 5.2995, Accuracy: 0.0156\n",
      "Epoch [46/100], Train Loss: 5.2981, Val Loss: 5.2998, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/1535], Loss: 5.2997, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [100/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [200/1535], Loss: 5.2943, Accuracy: 0.0312\n",
      "Epoch [47/100], Step [300/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [400/1535], Loss: 5.2916, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [500/1535], Loss: 5.2940, Accuracy: 0.0156\n",
      "Epoch [47/100], Step [600/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [700/1535], Loss: 5.2990, Accuracy: 0.0156\n",
      "Epoch [47/100], Step [800/1535], Loss: 5.2979, Accuracy: 0.0156\n",
      "Epoch [47/100], Step [900/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1000/1535], Loss: 5.2945, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1100/1535], Loss: 5.2947, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1200/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1300/1535], Loss: 5.3028, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1400/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [47/100], Step [1500/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [47/100], Train Loss: 5.2981, Val Loss: 5.3009, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/1535], Loss: 5.2941, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [100/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [200/1535], Loss: 5.2964, Accuracy: 0.0156\n",
      "Epoch [48/100], Step [300/1535], Loss: 5.3031, Accuracy: 0.0156\n",
      "Epoch [48/100], Step [400/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [500/1535], Loss: 5.3007, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [600/1535], Loss: 5.2963, Accuracy: 0.0156\n",
      "Epoch [48/100], Step [700/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [800/1535], Loss: 5.2954, Accuracy: 0.0156\n",
      "Epoch [48/100], Step [900/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [1000/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [1100/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [1200/1535], Loss: 5.2995, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [1300/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [48/100], Step [1400/1535], Loss: 5.3001, Accuracy: 0.0156\n",
      "Epoch [48/100], Step [1500/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [48/100], Train Loss: 5.2981, Val Loss: 5.2981, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/1535], Loss: 5.3014, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [100/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [200/1535], Loss: 5.2946, Accuracy: 0.0156\n",
      "Epoch [49/100], Step [300/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [400/1535], Loss: 5.3010, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [500/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [600/1535], Loss: 5.2989, Accuracy: 0.0156\n",
      "Epoch [49/100], Step [700/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [800/1535], Loss: 5.3029, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [900/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [1000/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [1100/1535], Loss: 5.2933, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [1200/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [1300/1535], Loss: 5.3008, Accuracy: 0.0000\n",
      "Epoch [49/100], Step [1400/1535], Loss: 5.3031, Accuracy: 0.0156\n",
      "Epoch [49/100], Step [1500/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [49/100], Train Loss: 5.2981, Val Loss: 5.3000, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [100/1535], Loss: 5.2959, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [200/1535], Loss: 5.3031, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [300/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [400/1535], Loss: 5.2978, Accuracy: 0.0156\n",
      "Epoch [50/100], Step [500/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [600/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [700/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [800/1535], Loss: 5.2983, Accuracy: 0.0156\n",
      "Epoch [50/100], Step [900/1535], Loss: 5.3019, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [1000/1535], Loss: 5.2984, Accuracy: 0.0312\n",
      "Epoch [50/100], Step [1100/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [1200/1535], Loss: 5.3027, Accuracy: 0.0156\n",
      "Epoch [50/100], Step [1300/1535], Loss: 5.3012, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [1400/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [50/100], Step [1500/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [50/100], Train Loss: 5.2981, Val Loss: 5.2981, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/1535], Loss: 5.3010, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [100/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [200/1535], Loss: 5.2952, Accuracy: 0.0156\n",
      "Epoch [51/100], Step [300/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [400/1535], Loss: 5.3013, Accuracy: 0.0156\n",
      "Epoch [51/100], Step [500/1535], Loss: 5.2992, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [600/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [700/1535], Loss: 5.2951, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [800/1535], Loss: 5.2939, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [900/1535], Loss: 5.2944, Accuracy: 0.0156\n",
      "Epoch [51/100], Step [1000/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [1100/1535], Loss: 5.2961, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [1200/1535], Loss: 5.2940, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [1300/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [51/100], Step [1400/1535], Loss: 5.2973, Accuracy: 0.0156\n",
      "Epoch [51/100], Step [1500/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [51/100], Train Loss: 5.2981, Val Loss: 5.2993, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/1535], Loss: 5.2972, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [100/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [200/1535], Loss: 5.2931, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [300/1535], Loss: 5.2991, Accuracy: 0.0156\n",
      "Epoch [52/100], Step [400/1535], Loss: 5.2915, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [500/1535], Loss: 5.3021, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [600/1535], Loss: 5.2993, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [700/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [800/1535], Loss: 5.2945, Accuracy: 0.0156\n",
      "Epoch [52/100], Step [900/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [1000/1535], Loss: 5.2997, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [1100/1535], Loss: 5.2981, Accuracy: 0.0156\n",
      "Epoch [52/100], Step [1200/1535], Loss: 5.3022, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [1300/1535], Loss: 5.3024, Accuracy: 0.0000\n",
      "Epoch [52/100], Step [1400/1535], Loss: 5.2958, Accuracy: 0.0156\n",
      "Epoch [52/100], Step [1500/1535], Loss: 5.2937, Accuracy: 0.0000\n",
      "Epoch [52/100], Train Loss: 5.2981, Val Loss: 5.2999, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/1535], Loss: 5.3034, Accuracy: 0.0156\n",
      "Epoch [53/100], Step [100/1535], Loss: 5.2959, Accuracy: 0.0156\n",
      "Epoch [53/100], Step [200/1535], Loss: 5.3029, Accuracy: 0.0156\n",
      "Epoch [53/100], Step [300/1535], Loss: 5.3026, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [400/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [500/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [600/1535], Loss: 5.3015, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [700/1535], Loss: 5.2975, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [800/1535], Loss: 5.3047, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [900/1535], Loss: 5.2975, Accuracy: 0.0156\n",
      "Epoch [53/100], Step [1000/1535], Loss: 5.2967, Accuracy: 0.0156\n",
      "Epoch [53/100], Step [1100/1535], Loss: 5.2956, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [1200/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [1300/1535], Loss: 5.3001, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [1400/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [53/100], Step [1500/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [53/100], Train Loss: 5.2981, Val Loss: 5.2992, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [54/100], Step [0/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [100/1535], Loss: 5.2935, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [200/1535], Loss: 5.2996, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [300/1535], Loss: 5.2970, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [400/1535], Loss: 5.2967, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [500/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [600/1535], Loss: 5.3042, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [700/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [800/1535], Loss: 5.2944, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [900/1535], Loss: 5.3010, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [1000/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [1100/1535], Loss: 5.2980, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [1200/1535], Loss: 5.3017, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [1300/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [54/100], Step [1400/1535], Loss: 5.2960, Accuracy: 0.0156\n",
      "Epoch [54/100], Step [1500/1535], Loss: 5.2985, Accuracy: 0.0156\n",
      "Epoch [54/100], Train Loss: 5.2981, Val Loss: 5.2993, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [55/100], Step [0/1535], Loss: 5.2990, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [100/1535], Loss: 5.2947, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [200/1535], Loss: 5.2970, Accuracy: 0.0156\n",
      "Epoch [55/100], Step [300/1535], Loss: 5.3027, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [400/1535], Loss: 5.2971, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [500/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [600/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [700/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [800/1535], Loss: 5.2976, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [900/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [1000/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [1100/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [1200/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [1300/1535], Loss: 5.2991, Accuracy: 0.0156\n",
      "Epoch [55/100], Step [1400/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [55/100], Step [1500/1535], Loss: 5.2940, Accuracy: 0.0000\n",
      "Epoch [55/100], Train Loss: 5.2981, Val Loss: 5.2987, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [56/100], Step [0/1535], Loss: 5.2961, Accuracy: 0.0156\n",
      "Epoch [56/100], Step [100/1535], Loss: 5.2992, Accuracy: 0.0156\n",
      "Epoch [56/100], Step [200/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [300/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [400/1535], Loss: 5.2988, Accuracy: 0.0156\n",
      "Epoch [56/100], Step [500/1535], Loss: 5.2974, Accuracy: 0.0156\n",
      "Epoch [56/100], Step [600/1535], Loss: 5.2937, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [700/1535], Loss: 5.2952, Accuracy: 0.0312\n",
      "Epoch [56/100], Step [800/1535], Loss: 5.3022, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [900/1535], Loss: 5.2989, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [1000/1535], Loss: 5.2983, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [1100/1535], Loss: 5.2948, Accuracy: 0.0156\n",
      "Epoch [56/100], Step [1200/1535], Loss: 5.2968, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [1300/1535], Loss: 5.3018, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [1400/1535], Loss: 5.2928, Accuracy: 0.0000\n",
      "Epoch [56/100], Step [1500/1535], Loss: 5.2986, Accuracy: 0.0156\n",
      "Epoch [56/100], Train Loss: 5.2981, Val Loss: 5.2997, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [57/100], Step [0/1535], Loss: 5.2941, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [100/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [200/1535], Loss: 5.3030, Accuracy: 0.0156\n",
      "Epoch [57/100], Step [300/1535], Loss: 5.2998, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [400/1535], Loss: 5.2988, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [500/1535], Loss: 5.2966, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [600/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [700/1535], Loss: 5.2988, Accuracy: 0.0156\n",
      "Epoch [57/100], Step [800/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [900/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [1000/1535], Loss: 5.3017, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [1100/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [1200/1535], Loss: 5.3001, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [1300/1535], Loss: 5.2971, Accuracy: 0.0000\n",
      "Epoch [57/100], Step [1400/1535], Loss: 5.2977, Accuracy: 0.0312\n",
      "Epoch [57/100], Step [1500/1535], Loss: 5.3015, Accuracy: 0.0000\n",
      "Epoch [57/100], Train Loss: 5.2981, Val Loss: 5.2986, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [58/100], Step [0/1535], Loss: 5.3011, Accuracy: 0.0156\n",
      "Epoch [58/100], Step [100/1535], Loss: 5.2976, Accuracy: 0.0156\n",
      "Epoch [58/100], Step [200/1535], Loss: 5.2969, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [300/1535], Loss: 5.2971, Accuracy: 0.0156\n",
      "Epoch [58/100], Step [400/1535], Loss: 5.2999, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [500/1535], Loss: 5.2953, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [600/1535], Loss: 5.3004, Accuracy: 0.0156\n",
      "Epoch [58/100], Step [700/1535], Loss: 5.2947, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [800/1535], Loss: 5.2987, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [900/1535], Loss: 5.2955, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1000/1535], Loss: 5.2986, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1100/1535], Loss: 5.3041, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1200/1535], Loss: 5.2977, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1300/1535], Loss: 5.3005, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1400/1535], Loss: 5.3044, Accuracy: 0.0000\n",
      "Epoch [58/100], Step [1500/1535], Loss: 5.2990, Accuracy: 0.0156\n",
      "Epoch [58/100], Train Loss: 5.2981, Val Loss: 5.2991, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [59/100], Step [0/1535], Loss: 5.2974, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [100/1535], Loss: 5.2977, Accuracy: 0.0156\n",
      "Epoch [59/100], Step [200/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [300/1535], Loss: 5.2993, Accuracy: 0.0156\n",
      "Epoch [59/100], Step [400/1535], Loss: 5.2966, Accuracy: 0.0156\n",
      "Epoch [59/100], Step [500/1535], Loss: 5.2970, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [600/1535], Loss: 5.2964, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [700/1535], Loss: 5.3051, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [800/1535], Loss: 5.2984, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [900/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [1000/1535], Loss: 5.2978, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [1100/1535], Loss: 5.2939, Accuracy: 0.0156\n",
      "Epoch [59/100], Step [1200/1535], Loss: 5.2972, Accuracy: 0.0156\n",
      "Epoch [59/100], Step [1300/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [1400/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [59/100], Step [1500/1535], Loss: 5.2958, Accuracy: 0.0000\n",
      "Epoch [59/100], Train Loss: 5.2981, Val Loss: 5.2994, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [60/100], Step [0/1535], Loss: 5.2981, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [100/1535], Loss: 5.3003, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [200/1535], Loss: 5.2987, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [300/1535], Loss: 5.2973, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [400/1535], Loss: 5.2939, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [500/1535], Loss: 5.2968, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [600/1535], Loss: 5.2964, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [700/1535], Loss: 5.2953, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [800/1535], Loss: 5.2987, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [900/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [1000/1535], Loss: 5.2966, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [1100/1535], Loss: 5.3020, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [1200/1535], Loss: 5.2984, Accuracy: 0.0156\n",
      "Epoch [60/100], Step [1300/1535], Loss: 5.3037, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [1400/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [60/100], Step [1500/1535], Loss: 5.2965, Accuracy: 0.0000\n",
      "Epoch [60/100], Train Loss: 5.2981, Val Loss: 5.2987, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [61/100], Step [0/1535], Loss: 5.2987, Accuracy: 0.0156\n",
      "Epoch [61/100], Step [100/1535], Loss: 5.2980, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [200/1535], Loss: 5.2979, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [300/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [400/1535], Loss: 5.2963, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [500/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [600/1535], Loss: 5.3016, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [700/1535], Loss: 5.2970, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [800/1535], Loss: 5.2994, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [900/1535], Loss: 5.3002, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [1000/1535], Loss: 5.2998, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [1100/1535], Loss: 5.3054, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [1200/1535], Loss: 5.2973, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [1300/1535], Loss: 5.2957, Accuracy: 0.0000\n",
      "Epoch [61/100], Step [1400/1535], Loss: 5.2963, Accuracy: 0.0156\n",
      "Epoch [61/100], Step [1500/1535], Loss: 5.3004, Accuracy: 0.0000\n",
      "Epoch [61/100], Train Loss: 5.2981, Val Loss: 5.3000, Train Acc: 0.0051, Val Acc: 0.0051\n",
      "==================================================\n",
      "CPU times: user 2d 13h 17min 45s, sys: 2h 9min 6s, total: 2d 15h 26min 51s\n",
      "Wall time: 8h 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=200,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "n_epochs = 100\n",
    "earlyStopping = 10\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/1535], Loss: 5.2985, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [100/1535], Loss: 6.6283, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [200/1535], Loss: 5.7079, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [300/1535], Loss: 5.5896, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [400/1535], Loss: 58.5395, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [500/1535], Loss: 5.9508, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [600/1535], Loss: 5.5107, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [700/1535], Loss: 5.3493, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [800/1535], Loss: 5.3300, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [900/1535], Loss: 5.3063, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1000/1535], Loss: 5.3229, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1100/1535], Loss: 5.2967, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1200/1535], Loss: 5.2907, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1300/1535], Loss: 5.3468, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1400/1535], Loss: 5.3022, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1500/1535], Loss: 5.3075, Accuracy: 0.0000\n",
      "Epoch [1/100], Train Loss: 10.2371, Val Loss: 5.3101, Train Acc: 0.0051, Val Acc: 0.0049\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/1535], Loss: 5.3240, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [100/1535], Loss: 5.3040, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [200/1535], Loss: 5.3365, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [300/1535], Loss: 5.3236, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [400/1535], Loss: 5.3222, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [500/1535], Loss: 5.2694, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [600/1535], Loss: 5.3046, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [700/1535], Loss: 5.3305, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [800/1535], Loss: 5.3416, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [900/1535], Loss: 5.3254, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1000/1535], Loss: 5.3065, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1100/1535], Loss: 5.3656, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1200/1535], Loss: 5.3461, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [1300/1535], Loss: 5.3302, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [1400/1535], Loss: 5.3032, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1500/1535], Loss: 5.3764, Accuracy: 0.0000\n",
      "Epoch [2/100], Train Loss: 5.3164, Val Loss: 5.3171, Train Acc: 0.0049, Val Acc: 0.0047\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/1535], Loss: 5.3057, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [100/1535], Loss: 5.3438, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [200/1535], Loss: 5.2872, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [300/1535], Loss: 5.3133, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [400/1535], Loss: 5.3183, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [500/1535], Loss: 5.2966, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [600/1535], Loss: 5.3225, Accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MobileNetV3(\n\u001b[1;32m      2\u001b[0m     in_chn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      3\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)\u001b[0m\n\u001b[1;32m     46\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     51\u001b[0m acc \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 111\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(x)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mBottleNeck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_residual \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1733\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiled_call_impl\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=200,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "n_epochs = 100\n",
    "earlyStopping = 10\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 s, sys: 1.24 s, total: 38.5 s\n",
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred, actual = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0051\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "print(f\"Test Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using GELU instead of hard swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/1535], Loss: 5.3079, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [100/1535], Loss: 5.2943, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [200/1535], Loss: 5.3277, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [300/1535], Loss: 5.2952, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [400/1535], Loss: 5.3301, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [500/1535], Loss: 5.2876, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [600/1535], Loss: 5.3209, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [700/1535], Loss: 5.2923, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [800/1535], Loss: 5.3562, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [900/1535], Loss: 5.3304, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [1000/1535], Loss: 5.3679, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1100/1535], Loss: 5.3051, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1200/1535], Loss: 5.3390, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1300/1535], Loss: 5.3397, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1400/1535], Loss: 5.3176, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1500/1535], Loss: 5.3473, Accuracy: 0.0000\n",
      "Epoch [1/100], Train Loss: 5.4999, Val Loss: 96.5215, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/1535], Loss: 5.3293, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [100/1535], Loss: 5.2957, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [200/1535], Loss: 5.3274, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [300/1535], Loss: 5.2787, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [400/1535], Loss: 5.3066, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [500/1535], Loss: 5.2825, Accuracy: 0.0469\n",
      "Epoch [2/100], Step [600/1535], Loss: 5.2882, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [700/1535], Loss: 5.3225, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [800/1535], Loss: 5.3025, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [900/1535], Loss: 5.3406, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1000/1535], Loss: 5.2949, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1100/1535], Loss: 5.3846, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1200/1535], Loss: 5.2881, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1300/1535], Loss: 5.3419, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1400/1535], Loss: 5.3100, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1500/1535], Loss: 5.3089, Accuracy: 0.0312\n",
      "Epoch [2/100], Train Loss: 5.3209, Val Loss: 101.7768, Train Acc: 0.0050, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/1535], Loss: 5.2846, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [100/1535], Loss: 5.3504, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [200/1535], Loss: 5.2937, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [300/1535], Loss: 5.3577, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [400/1535], Loss: 5.3200, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [500/1535], Loss: 5.3121, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [600/1535], Loss: 5.2899, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [700/1535], Loss: 5.3075, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [800/1535], Loss: 5.3381, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [900/1535], Loss: 5.3135, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1000/1535], Loss: 5.3554, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1100/1535], Loss: 5.3149, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1200/1535], Loss: 5.3619, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1300/1535], Loss: 5.2916, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1400/1535], Loss: 5.2803, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1500/1535], Loss: 5.3648, Accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     11\u001b[0m earlyStopping \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)\u001b[0m\n\u001b[1;32m     67\u001b[0m trainLosses\u001b[38;5;241m.\u001b[39mappend(trainLoss)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m pred, actual \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m valLoss \u001b[38;5;241m=\u001b[39m criterion(pred, actual)\n\u001b[1;32m     72\u001b[0m valAcc \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m actual)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(model, device, dataLoader)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataLoader:\n\u001b[0;32m---> 20\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     23\u001b[0m         actual\u001b[38;5;241m.\u001b[39mappend(labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import GELU\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"large\":\n",
    "            layers_config = [\n",
    "                # kernel, exp, out, SE, NL, stride\n",
    "                [3, 16, 16, False, ReLU, 1],\n",
    "                [3, 64, 24, False, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 1],\n",
    "                [5, 72, 40, True, ReLU, 2],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [3, 240, 80, False, GELU, 2],\n",
    "                [3, 200, 80, False, GELU, 1],\n",
    "                [3, 184, 80, False, GELU, 1],\n",
    "                [3, 184, 80, False, GELU, 1],\n",
    "                [3, 480, 112, True, GELU, 1],\n",
    "                [3, 672, 112, True, GELU, 1],\n",
    "                [5, 672, 160, True, GELU, 2],\n",
    "                [5, 960, 160, True, GELU, 1],\n",
    "                [5, 960, 160, True, GELU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 960, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, GELU, 2],\n",
    "                [5, 240, 40, True, GELU, 1],\n",
    "                [5, 240, 40, True, GELU, 1],\n",
    "                [5, 120, 48, True, GELU, 1],\n",
    "                [5, 144, 48, True, GELU, 1],\n",
    "                [5, 288, 96, True, GELU, 2],\n",
    "                [5, 576, 96, True, GELU, 1],\n",
    "                [5, 576, 96, True, GELU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "\n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/1535], Loss: 6.9043, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [100/1535], Loss: 5.3616, Accuracy: 0.0312\n",
      "Epoch [1/100], Step [200/1535], Loss: 5.3457, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [300/1535], Loss: 5.3874, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [400/1535], Loss: 5.3041, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [500/1535], Loss: 5.3067, Accuracy: 0.0156\n",
      "Epoch [1/100], Step [600/1535], Loss: 5.3407, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [700/1535], Loss: 5.3456, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [800/1535], Loss: 5.3323, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [900/1535], Loss: 5.3255, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1000/1535], Loss: 5.3380, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1100/1535], Loss: 5.3052, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1200/1535], Loss: 5.3710, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1300/1535], Loss: 5.3432, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1400/1535], Loss: 5.2982, Accuracy: 0.0000\n",
      "Epoch [1/100], Step [1500/1535], Loss: 5.3098, Accuracy: 0.0156\n",
      "Epoch [1/100], Train Loss: 5.6531, Val Loss: 5.3570, Train Acc: 0.0053, Val Acc: 0.0041\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/1535], Loss: 5.2816, Accuracy: 0.0156\n",
      "Epoch [2/100], Step [100/1535], Loss: 5.3034, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [200/1535], Loss: 5.3224, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [300/1535], Loss: 5.3183, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [400/1535], Loss: 5.2618, Accuracy: 0.0312\n",
      "Epoch [2/100], Step [500/1535], Loss: 5.3446, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [600/1535], Loss: 5.3739, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [700/1535], Loss: 5.2825, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [800/1535], Loss: 5.3406, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [900/1535], Loss: 5.3155, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1000/1535], Loss: 5.3030, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1100/1535], Loss: 5.3186, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1200/1535], Loss: 5.3229, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1300/1535], Loss: 5.3023, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1400/1535], Loss: 5.3203, Accuracy: 0.0000\n",
      "Epoch [2/100], Step [1500/1535], Loss: 5.3227, Accuracy: 0.0000\n",
      "Epoch [2/100], Train Loss: 5.3246, Val Loss: 5.3412, Train Acc: 0.0049, Val Acc: 0.0043\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/1535], Loss: 5.2931, Accuracy: 0.0312\n",
      "Epoch [3/100], Step [100/1535], Loss: 5.3345, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [200/1535], Loss: 5.3112, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [300/1535], Loss: 5.2973, Accuracy: 0.0156\n",
      "Epoch [3/100], Step [400/1535], Loss: 5.2854, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [500/1535], Loss: 5.3420, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [600/1535], Loss: 5.3443, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [700/1535], Loss: 5.3268, Accuracy: 0.0312\n",
      "Epoch [3/100], Step [800/1535], Loss: 5.3228, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [900/1535], Loss: 5.3278, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1000/1535], Loss: 5.3505, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1100/1535], Loss: 5.3217, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1200/1535], Loss: 5.3396, Accuracy: 0.0312\n",
      "Epoch [3/100], Step [1300/1535], Loss: 5.3172, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1400/1535], Loss: 5.3000, Accuracy: 0.0000\n",
      "Epoch [3/100], Step [1500/1535], Loss: 5.3155, Accuracy: 0.0000\n",
      "Epoch [3/100], Train Loss: 5.4897, Val Loss: 5.3196, Train Acc: 0.0054, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/1535], Loss: 5.3189, Accuracy: 0.0312\n",
      "Epoch [4/100], Step [100/1535], Loss: 5.3264, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [200/1535], Loss: 5.3271, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [300/1535], Loss: 5.3283, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [400/1535], Loss: 5.3318, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [500/1535], Loss: 5.3142, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [600/1535], Loss: 5.3862, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [700/1535], Loss: 5.3276, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [800/1535], Loss: 5.3225, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [900/1535], Loss: 5.3352, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1000/1535], Loss: 5.3362, Accuracy: 0.0156\n",
      "Epoch [4/100], Step [1100/1535], Loss: 5.2756, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1200/1535], Loss: 5.3555, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1300/1535], Loss: 5.3030, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1400/1535], Loss: 5.2929, Accuracy: 0.0000\n",
      "Epoch [4/100], Step [1500/1535], Loss: 5.3709, Accuracy: 0.0000\n",
      "Epoch [4/100], Train Loss: 5.3204, Val Loss: 5.3204, Train Acc: 0.0049, Val Acc: 0.0051\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/1535], Loss: 5.2942, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [100/1535], Loss: 5.2996, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [200/1535], Loss: 5.3077, Accuracy: 0.0156\n",
      "Epoch [5/100], Step [300/1535], Loss: 5.2969, Accuracy: 0.0312\n",
      "Epoch [5/100], Step [400/1535], Loss: 5.3068, Accuracy: 0.0156\n",
      "Epoch [5/100], Step [500/1535], Loss: 5.3074, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [600/1535], Loss: 5.3404, Accuracy: 0.0312\n",
      "Epoch [5/100], Step [700/1535], Loss: 5.3553, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [800/1535], Loss: 5.3297, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [900/1535], Loss: 5.3108, Accuracy: 0.0312\n",
      "Epoch [5/100], Step [1000/1535], Loss: 5.3388, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1100/1535], Loss: 5.3658, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1200/1535], Loss: 5.2983, Accuracy: 0.0156\n",
      "Epoch [5/100], Step [1300/1535], Loss: 5.3498, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1400/1535], Loss: 5.3233, Accuracy: 0.0000\n",
      "Epoch [5/100], Step [1500/1535], Loss: 5.3547, Accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     10\u001b[0m earlyStopping \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)\u001b[0m\n\u001b[1;32m     67\u001b[0m trainLosses\u001b[38;5;241m.\u001b[39mappend(trainLoss)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m pred, actual \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m valLoss \u001b[38;5;241m=\u001b[39m criterion(pred, actual)\n\u001b[1;32m     72\u001b[0m valAcc \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m actual)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(model, device, dataLoader)\u001b[0m\n\u001b[1;32m     16\u001b[0m actual \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataLoader:\n\u001b[1;32m     20\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     82\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_misc.py:165\u001b[0m, in \u001b[0;36mNormalize._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     34\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_utils.py:31\u001b[0m, in \u001b[0;36m_kernel_tv_tensor_wrapper.<locals>.wrapper\u001b[0;34m(inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(kernel)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(inpt, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# If you're wondering whether we could / should get rid of this wrapper,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# lost after the first operation due to our own __torch_function__\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# logic.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tv_tensors\u001b[38;5;241m.\u001b[39mwrap(output, like\u001b[38;5;241m=\u001b[39minpt)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py:65\u001b[0m, in \u001b[0;36mnormalize_image\u001b[0;34m(image, mean, std, inplace)\u001b[0m\n\u001b[1;32m     63\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msub_(mean)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### ref torch\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "\n",
    "\n",
    "model = mobilenet_v3_small().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "n_epochs = 100\n",
    "earlyStopping = 10\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
