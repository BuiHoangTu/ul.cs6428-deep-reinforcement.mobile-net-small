{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/micromamba/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 16200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 5400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 5400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "dataName = \"blanchon/EuroSAT_RGB\"\n",
    "dataPath = Path(dataName)\n",
    "if dataPath.exists() is False:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds: DatasetDict = load_dataset(dataName)  # type: ignore\n",
    "    ds.save_to_disk(dataPath)  # type: ignore\n",
    "\n",
    "else:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    ds: DatasetDict = load_from_disk(dataPath)  # type: ignore\n",
    "\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCmA2DxVPUCfJAx/FWd58o/5aN+dPWVpOHYt9aOWxu5JqxC1IKsFBjpTNvOABn6UzHkY2nscAfSpoUT70i5UdccUyXYW3KMKQMCgq1iMGjtShRnFK+FUDFAkyLjHTmpIRwajNTwgbfxqmUtx3f2prDB4qXAFJgZqSiPbgFjyx4okwCFIPQVI33aay5cZ6AUImS0IwBk4okHAqZVRs561FcAKQB6UCsaer2sUNvA6RKjMzdO4GKy1cIMAD8q1dbbKWw9m4/GscmlD4dTSppLQlEuTyBUgG4cGqwqxGfl/GqaITJPK96ZcEKw4qYNkVBcDLYzSRT2I/NVT939abJKkhB5GKjYZPWmYqrEG3rvE0Kbs4U/zrHOa0tVnW5uVdeAFxis5jzUw+FF1NZMBU6cL+NQgZ71KDhRVMhE6c/U1BOf3hqWFsSLkVHMnzn1qepXQrN1pKlKHNN2VRJ//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][16199][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDy4dDUO4k8dPWpBwtNAANbnKGM96AeeuaVlIAzxmkGAKYBjuetB7lj2oJHHP4UrKQB6HvQAwU7j1H0puQKWgB5Hy0wdae33aaKQAe5oJwByORQcnvxSHg4pgHekOcjmjrSUALz7UtJSigBznJxTaXFJigBPw4oNKSd3tRkUDEpQKSl7UCExThxk02nqM0AIelJ2pzelMoGL3pPYUE4FA/WgAo60tOVM0gGgc1IOKUADimsewoEf//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"][900][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size = (64, 64)\n",
      "Mode = RGB\n",
      "Format = PNG\n"
     ]
    }
   ],
   "source": [
    "img = ds[\"train\"][0][\"image\"]\n",
    "\n",
    "print(f\"Size = {img.size}\")\n",
    "print(f\"Mode = {img.mode}\")\n",
    "print(f\"Format = {img.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Prepare data for training and evaluating.\n",
    "\n",
    "Also augment the train data by introduce random effects like cropting, rotating, flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as ImgF\n",
    "\n",
    "\n",
    "# ds[\"train\"][0][\"image\"] is a PIL.JpegImagePlugin.JpegImageFile\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset of images and its labels. Return image and label. If transform is provided, apply it to the image.\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    IMG_SIZE = 64, 64\n",
    "\n",
    "    def transformBuild(self):\n",
    "        transformations = []\n",
    "        transformations.append(v2.Resize(size=ImageDataset.IMG_SIZE))\n",
    "        if self.randCrop:\n",
    "            transformations.append(\n",
    "                v2.RandomResizedCrop(size=ImageDataset.IMG_SIZE, scale=(0.8, 1.0))\n",
    "            )\n",
    "        if self.randRot:\n",
    "            transformations.append(v2.RandomRotation(15))  # type: ignore\n",
    "        if self.randFlipH:\n",
    "            transformations.append(v2.RandomHorizontalFlip())\n",
    "        if self.randFlipV:\n",
    "            transformations.append(v2.RandomVerticalFlip())\n",
    "\n",
    "        transformations.append(v2.ToImage())\n",
    "        transformations.append(v2.ToDtype(torch.float32, True))\n",
    "\n",
    "        transformations.append(self.normaliser)\n",
    "\n",
    "        return v2.Compose(transformations)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        randCrop=False,\n",
    "        randRot=False,\n",
    "        randFlipH=False,\n",
    "        randFlipV=False,\n",
    "        normaliser=None,\n",
    "    ):\n",
    "        self.randCrop = randCrop\n",
    "        self.randRot = randRot\n",
    "        self.randFlipH = randFlipH\n",
    "        self.randFlipV = randFlipV\n",
    "\n",
    "        self.length = len(data)\n",
    "        self.imgSize = self.IMG_SIZE\n",
    "\n",
    "        self.images = [d[\"image\"] for d in data]\n",
    "        self.labels = [d[\"label\"] for d in data]\n",
    "\n",
    "        # calculate normaliser if not provided\n",
    "        if normaliser is None:\n",
    "            c = ImgF.to_tensor(self.images[0]).size(0)\n",
    "            mean = torch.zeros(c)\n",
    "            std = torch.zeros(c)\n",
    "\n",
    "            for img in self.images:\n",
    "                img = ImgF.to_tensor(img)\n",
    "                mean += img.mean(dim=(1, 2))\n",
    "                std += img.std(dim=(1, 2))\n",
    "\n",
    "            mean /= self.length\n",
    "            std /= self.length\n",
    "\n",
    "            normaliser = v2.Normalize(mean.tolist(), std.tolist())\n",
    "        self.normaliser = normaliser\n",
    "\n",
    "        # build the transformer\n",
    "        self.imgTransformer = self.transformBuild()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length * 3\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx // 3\n",
    "\n",
    "        return self.imgTransformer(self.images[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# img255Scaler = v2.Lambda(lambda x: x / 255.0)\n",
    "# trainSet = ImageDataset(ds[\"train\"], True, True, True, True, img255Scaler)\n",
    "# test if augmentations are overdoing it\n",
    "trainSet = ImageDataset(ds[\"train\"], True, True, True, True)\n",
    "valSet = ImageDataset(ds[\"validation\"], normaliser=trainSet.normaliser)\n",
    "testSet = ImageDataset(ds[\"test\"], normaliser=trainSet.normaliser)\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSize = trainSet[0][0].size()\n",
    "sampleSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate if the image is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.5575547..5.0735006].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = trainSet[0][0].permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = ds[\"train\"][0][\"image\"]\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, device, dataLoader):\n",
    "    \"\"\"Infer the predictions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for inference.\n",
    "        device (torch.device): The device to use.\n",
    "        dataLoader (DataLoader): The data to infer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The predictions and the actual labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    predProba = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            actual.append(labels)\n",
    "            predProba.append(outputs)\n",
    "\n",
    "    return torch.cat(predProba), torch.cat(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    trainLoader,\n",
    "    valLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=100,\n",
    "    earlyStopping=10,\n",
    "):\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        device (torch.device): The device to use.\n",
    "        trainLoader (DataLoader): The training data.\n",
    "        valLoader (DataLoader): The validation data.\n",
    "        criterion (_type_): The loss function.\n",
    "        optimizer (_type_): The optimizer.\n",
    "        n_epochs (int, optional): Number of epochs to train for. Defaults to 100.\n",
    "        earlyStopping (int, optional): Number of epochs to wait before stopping training if no improvement is made\n",
    "            on the validation loss. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    optimScheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.2)\n",
    "\n",
    "    # Training loop\n",
    "    earlyStopping = 10\n",
    "    bestValLoss = float(\"inf\")\n",
    "    bestModelState = None\n",
    "    patience = 0\n",
    "\n",
    "    trainLosses = []\n",
    "    valLosses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        trainLoss = 0\n",
    "        trainAcc = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainLoader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            trainLoss += loss.item()\n",
    "            trainAcc += acc.item()\n",
    "\n",
    "            # Print every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{n_epochs}], Step [{batch_idx}/{len(trainLoader)}], Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        trainLoss /= len(trainLoader)\n",
    "        trainAcc /= len(trainLoader)\n",
    "        trainLosses.append(trainLoss)\n",
    "\n",
    "        # Validation loss\n",
    "        pred, actual = infer(model, device, valLoader)\n",
    "        valLoss = criterion(pred, actual)\n",
    "        valAcc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "\n",
    "        valLosses.append(valLoss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}], Train Loss: {trainLoss:.4f}, Val Loss: {valLoss:.4f}, Train Acc: {trainAcc:.4f}, Val Acc: {valAcc:.4f}\"\n",
    "        )\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Early stopping\n",
    "        if valLoss < bestValLoss:\n",
    "            bestValLoss = valLoss\n",
    "            bestModelState = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > earlyStopping:\n",
    "                break\n",
    "\n",
    "        optimScheduler.step(valLoss)\n",
    "\n",
    "    # Save the model\n",
    "    # torch.save(bestModelState, \"model.pth\")\n",
    "    model.load_state_dict(bestModelState)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2d, BatchNorm2d and activation function.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, in_chn, out_chn, activation, kernel_size=3, alpha=1, stride=1, group=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2  # for same padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chn,\n",
    "            out_chn,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "            groups=group,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_chn)\n",
    "        self.activate = activation() if activation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activate is not None:\n",
    "            x = self.activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeAndExcite(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_chn, reduction=4):\n",
    "        super().__init__()\n",
    "        reduction_chn = in_chn // reduction\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(\n",
    "                1\n",
    "            ),  # 1,16,x,y->1,1,16,1 each channel is reduce to one value\n",
    "            nn.Flatten(),  #  1,16\n",
    "            nn.Linear(in_chn, reduction_chn, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduction_chn, in_chn, bias=False),\n",
    "            nn.Hardsigmoid(),\n",
    "            nn.Unflatten(1, (in_chn, 1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        out_chn,\n",
    "        expansion_channel,\n",
    "        activation,\n",
    "        se_reduction=4,\n",
    "        se_flag=False,\n",
    "        kernel=3,\n",
    "        stride=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #         expansion_channel=int(in_chn*expansiton_factor)\n",
    "        self.use_residual = (stride == 1) and (in_chn == out_chn)\n",
    "\n",
    "        self.residual_block_layers: list[nn.Module] = [\n",
    "            ConvBlock(\n",
    "                in_chn, expansion_channel, activation=activation, kernel_size=1\n",
    "            ),  # expansion_part\n",
    "            ConvBlock(\n",
    "                expansion_channel,\n",
    "                expansion_channel,\n",
    "                activation=activation,\n",
    "                group=expansion_channel,\n",
    "                stride=stride,\n",
    "                kernel_size=kernel,\n",
    "            ),  # depthwise conv\n",
    "        ]\n",
    "\n",
    "        if se_flag is True:\n",
    "            self.residual_block_layers.extend(\n",
    "                [SqueezeAndExcite(expansion_channel, reduction=se_reduction)]\n",
    "            )\n",
    "\n",
    "        self.residual_block_layers.extend(\n",
    "            [ConvBlock(expansion_channel, out_chn, activation=None, kernel_size=1)]\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*self.residual_block_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + x if self.use_residual else self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Hardswish, ReLU\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"large\":\n",
    "            layers_config = [\n",
    "                # kernel, exp, out, SE, NL, stride\n",
    "                [3, 16, 16, False, ReLU, 1],\n",
    "                [3, 64, 24, False, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 1],\n",
    "                [5, 72, 40, True, ReLU, 2],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [3, 240, 80, False, Hardswish, 2],\n",
    "                [3, 200, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 480, 112, True, Hardswish, 1],\n",
    "                [3, 672, 112, True, Hardswish, 1],\n",
    "                [5, 672, 160, True, Hardswish, 2],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 960, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, Hardswish, 2],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 120, 48, True, Hardswish, 1],\n",
    "                [5, 144, 48, True, Hardswish, 1],\n",
    "                [5, 288, 96, True, Hardswish, 2],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "        \n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Step [0/760], Loss: 2.2982, Accuracy: 0.0469\n",
      "Epoch [1/100], Step [100/760], Loss: 1.1734, Accuracy: 0.4688\n",
      "Epoch [1/100], Step [200/760], Loss: 1.0804, Accuracy: 0.6875\n",
      "Epoch [1/100], Step [300/760], Loss: 0.9230, Accuracy: 0.5938\n",
      "Epoch [1/100], Step [400/760], Loss: 0.9641, Accuracy: 0.6406\n",
      "Epoch [1/100], Step [500/760], Loss: 1.0403, Accuracy: 0.6875\n",
      "Epoch [1/100], Step [600/760], Loss: 0.7960, Accuracy: 0.7344\n",
      "Epoch [1/100], Step [700/760], Loss: 0.6627, Accuracy: 0.7344\n",
      "Epoch [1/100], Train Loss: 0.9029, Val Loss: 0.6425, Train Acc: 0.6721, Val Acc: 0.7635\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/760], Loss: 0.6013, Accuracy: 0.8125\n",
      "Epoch [2/100], Step [100/760], Loss: 0.7403, Accuracy: 0.7031\n",
      "Epoch [2/100], Step [200/760], Loss: 0.5268, Accuracy: 0.7812\n",
      "Epoch [2/100], Step [300/760], Loss: 0.3454, Accuracy: 0.8594\n",
      "Epoch [2/100], Step [400/760], Loss: 0.4920, Accuracy: 0.8125\n",
      "Epoch [2/100], Step [500/760], Loss: 0.6155, Accuracy: 0.7500\n",
      "Epoch [2/100], Step [600/760], Loss: 0.5460, Accuracy: 0.7812\n",
      "Epoch [2/100], Step [700/760], Loss: 0.6566, Accuracy: 0.7812\n",
      "Epoch [2/100], Train Loss: 0.6171, Val Loss: 0.4655, Train Acc: 0.7809, Val Acc: 0.8354\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/760], Loss: 0.5858, Accuracy: 0.7812\n",
      "Epoch [3/100], Step [100/760], Loss: 0.3809, Accuracy: 0.8750\n",
      "Epoch [3/100], Step [200/760], Loss: 0.4462, Accuracy: 0.8438\n",
      "Epoch [3/100], Step [300/760], Loss: 0.6130, Accuracy: 0.7969\n",
      "Epoch [3/100], Step [400/760], Loss: 0.6259, Accuracy: 0.8125\n",
      "Epoch [3/100], Step [500/760], Loss: 0.5190, Accuracy: 0.8281\n",
      "Epoch [3/100], Step [600/760], Loss: 0.5953, Accuracy: 0.7812\n",
      "Epoch [3/100], Step [700/760], Loss: 0.5792, Accuracy: 0.7969\n",
      "Epoch [3/100], Train Loss: 0.4881, Val Loss: 0.4050, Train Acc: 0.8311, Val Acc: 0.8571\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/760], Loss: 0.4436, Accuracy: 0.8281\n",
      "Epoch [4/100], Step [100/760], Loss: 0.3714, Accuracy: 0.8438\n",
      "Epoch [4/100], Step [200/760], Loss: 0.3548, Accuracy: 0.8438\n",
      "Epoch [4/100], Step [300/760], Loss: 0.6433, Accuracy: 0.8281\n",
      "Epoch [4/100], Step [400/760], Loss: 0.3294, Accuracy: 0.8906\n",
      "Epoch [4/100], Step [500/760], Loss: 0.3758, Accuracy: 0.8594\n",
      "Epoch [4/100], Step [600/760], Loss: 0.3381, Accuracy: 0.8594\n",
      "Epoch [4/100], Step [700/760], Loss: 0.2608, Accuracy: 0.8906\n",
      "Epoch [4/100], Train Loss: 0.4083, Val Loss: 0.3391, Train Acc: 0.8582, Val Acc: 0.8797\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/760], Loss: 0.3622, Accuracy: 0.8906\n",
      "Epoch [5/100], Step [100/760], Loss: 0.3301, Accuracy: 0.8750\n",
      "Epoch [5/100], Step [200/760], Loss: 0.3702, Accuracy: 0.8594\n",
      "Epoch [5/100], Step [300/760], Loss: 0.2976, Accuracy: 0.9062\n",
      "Epoch [5/100], Step [400/760], Loss: 0.4826, Accuracy: 0.8281\n",
      "Epoch [5/100], Step [500/760], Loss: 0.3401, Accuracy: 0.9219\n",
      "Epoch [5/100], Step [600/760], Loss: 0.1656, Accuracy: 0.9375\n",
      "Epoch [5/100], Step [700/760], Loss: 0.3594, Accuracy: 0.8438\n",
      "Epoch [5/100], Train Loss: 0.3592, Val Loss: 0.3151, Train Acc: 0.8771, Val Acc: 0.8897\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/760], Loss: 0.3638, Accuracy: 0.8750\n",
      "Epoch [6/100], Step [100/760], Loss: 0.2883, Accuracy: 0.8906\n",
      "Epoch [6/100], Step [200/760], Loss: 0.3377, Accuracy: 0.8281\n",
      "Epoch [6/100], Step [300/760], Loss: 0.2509, Accuracy: 0.9062\n",
      "Epoch [6/100], Step [400/760], Loss: 0.1456, Accuracy: 0.9375\n",
      "Epoch [6/100], Step [500/760], Loss: 0.3298, Accuracy: 0.8750\n",
      "Epoch [6/100], Step [600/760], Loss: 0.2396, Accuracy: 0.9219\n",
      "Epoch [6/100], Step [700/760], Loss: 0.3538, Accuracy: 0.8594\n",
      "Epoch [6/100], Train Loss: 0.3158, Val Loss: 0.2700, Train Acc: 0.8907, Val Acc: 0.9079\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/760], Loss: 0.1994, Accuracy: 0.9062\n",
      "Epoch [7/100], Step [100/760], Loss: 0.1066, Accuracy: 0.9688\n",
      "Epoch [7/100], Step [200/760], Loss: 0.3891, Accuracy: 0.8594\n",
      "Epoch [7/100], Step [300/760], Loss: 0.2898, Accuracy: 0.9062\n",
      "Epoch [7/100], Step [400/760], Loss: 0.2158, Accuracy: 0.9062\n",
      "Epoch [7/100], Step [500/760], Loss: 0.4171, Accuracy: 0.8438\n",
      "Epoch [7/100], Step [600/760], Loss: 0.1987, Accuracy: 0.9062\n",
      "Epoch [7/100], Step [700/760], Loss: 0.1950, Accuracy: 0.9375\n",
      "Epoch [7/100], Train Loss: 0.2891, Val Loss: 0.2352, Train Acc: 0.8997, Val Acc: 0.9205\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/760], Loss: 0.3380, Accuracy: 0.8906\n",
      "Epoch [8/100], Step [100/760], Loss: 0.2021, Accuracy: 0.9375\n",
      "Epoch [8/100], Step [200/760], Loss: 0.1745, Accuracy: 0.9062\n",
      "Epoch [8/100], Step [300/760], Loss: 0.2240, Accuracy: 0.9062\n",
      "Epoch [8/100], Step [400/760], Loss: 0.4046, Accuracy: 0.8750\n",
      "Epoch [8/100], Step [500/760], Loss: 0.2038, Accuracy: 0.9219\n",
      "Epoch [8/100], Step [600/760], Loss: 0.0724, Accuracy: 0.9844\n",
      "Epoch [8/100], Step [700/760], Loss: 0.3165, Accuracy: 0.8750\n",
      "Epoch [8/100], Train Loss: 0.2699, Val Loss: 0.2580, Train Acc: 0.9065, Val Acc: 0.9109\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/760], Loss: 0.3233, Accuracy: 0.9219\n",
      "Epoch [9/100], Step [100/760], Loss: 0.2908, Accuracy: 0.9219\n",
      "Epoch [9/100], Step [200/760], Loss: 0.1197, Accuracy: 0.9531\n",
      "Epoch [9/100], Step [300/760], Loss: 0.2679, Accuracy: 0.9062\n",
      "Epoch [9/100], Step [400/760], Loss: 0.2763, Accuracy: 0.9219\n",
      "Epoch [9/100], Step [500/760], Loss: 0.1251, Accuracy: 0.9531\n",
      "Epoch [9/100], Step [600/760], Loss: 0.2304, Accuracy: 0.9375\n",
      "Epoch [9/100], Step [700/760], Loss: 0.5822, Accuracy: 0.7812\n",
      "Epoch [9/100], Train Loss: 0.2507, Val Loss: 0.2068, Train Acc: 0.9134, Val Acc: 0.9299\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/760], Loss: 0.3054, Accuracy: 0.9219\n",
      "Epoch [10/100], Step [100/760], Loss: 0.2771, Accuracy: 0.8594\n",
      "Epoch [10/100], Step [200/760], Loss: 0.1818, Accuracy: 0.9375\n",
      "Epoch [10/100], Step [300/760], Loss: 0.2507, Accuracy: 0.9375\n",
      "Epoch [10/100], Step [400/760], Loss: 0.3128, Accuracy: 0.9062\n",
      "Epoch [10/100], Step [500/760], Loss: 0.3558, Accuracy: 0.8906\n",
      "Epoch [10/100], Step [600/760], Loss: 0.2819, Accuracy: 0.9688\n",
      "Epoch [10/100], Step [700/760], Loss: 0.1660, Accuracy: 0.9375\n",
      "Epoch [10/100], Train Loss: 0.2439, Val Loss: 0.1882, Train Acc: 0.9162, Val Acc: 0.9350\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/760], Loss: 0.1276, Accuracy: 0.9531\n",
      "Epoch [11/100], Step [100/760], Loss: 0.1765, Accuracy: 0.9531\n",
      "Epoch [11/100], Step [200/760], Loss: 0.1576, Accuracy: 0.9219\n",
      "Epoch [11/100], Step [300/760], Loss: 0.3015, Accuracy: 0.8750\n",
      "Epoch [11/100], Step [400/760], Loss: 0.0661, Accuracy: 0.9688\n",
      "Epoch [11/100], Step [500/760], Loss: 0.1097, Accuracy: 0.9531\n",
      "Epoch [11/100], Step [600/760], Loss: 0.1463, Accuracy: 0.9375\n",
      "Epoch [11/100], Step [700/760], Loss: 0.2645, Accuracy: 0.9062\n",
      "Epoch [11/100], Train Loss: 0.2269, Val Loss: 0.1855, Train Acc: 0.9218, Val Acc: 0.9376\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/760], Loss: 0.1580, Accuracy: 0.9531\n",
      "Epoch [12/100], Step [100/760], Loss: 0.2007, Accuracy: 0.9531\n",
      "Epoch [12/100], Step [200/760], Loss: 0.4325, Accuracy: 0.8438\n",
      "Epoch [12/100], Step [300/760], Loss: 0.2507, Accuracy: 0.9531\n",
      "Epoch [12/100], Step [400/760], Loss: 0.2123, Accuracy: 0.9531\n",
      "Epoch [12/100], Step [500/760], Loss: 0.1644, Accuracy: 0.9375\n",
      "Epoch [12/100], Step [600/760], Loss: 0.2500, Accuracy: 0.8906\n",
      "Epoch [12/100], Step [700/760], Loss: 0.1641, Accuracy: 0.9219\n",
      "Epoch [12/100], Train Loss: 0.2133, Val Loss: 0.1676, Train Acc: 0.9270, Val Acc: 0.9430\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/760], Loss: 0.4039, Accuracy: 0.8906\n",
      "Epoch [13/100], Step [100/760], Loss: 0.1313, Accuracy: 0.9375\n",
      "Epoch [13/100], Step [200/760], Loss: 0.2278, Accuracy: 0.9062\n",
      "Epoch [13/100], Step [300/760], Loss: 0.0591, Accuracy: 0.9844\n",
      "Epoch [13/100], Step [400/760], Loss: 0.1938, Accuracy: 0.9062\n",
      "Epoch [13/100], Step [500/760], Loss: 0.1046, Accuracy: 0.9531\n",
      "Epoch [13/100], Step [600/760], Loss: 0.1792, Accuracy: 0.9219\n",
      "Epoch [13/100], Step [700/760], Loss: 0.2735, Accuracy: 0.9375\n",
      "Epoch [13/100], Train Loss: 0.2084, Val Loss: 0.1708, Train Acc: 0.9285, Val Acc: 0.9408\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/760], Loss: 0.2151, Accuracy: 0.8906\n",
      "Epoch [14/100], Step [100/760], Loss: 0.0841, Accuracy: 0.9688\n",
      "Epoch [14/100], Step [200/760], Loss: 0.1763, Accuracy: 0.9062\n",
      "Epoch [14/100], Step [300/760], Loss: 0.1875, Accuracy: 0.9531\n",
      "Epoch [14/100], Step [400/760], Loss: 0.2066, Accuracy: 0.9062\n",
      "Epoch [14/100], Step [500/760], Loss: 0.3781, Accuracy: 0.8750\n",
      "Epoch [14/100], Step [600/760], Loss: 0.1359, Accuracy: 0.9531\n",
      "Epoch [14/100], Step [700/760], Loss: 0.3154, Accuracy: 0.9062\n",
      "Epoch [14/100], Train Loss: 0.1988, Val Loss: 0.1798, Train Acc: 0.9312, Val Acc: 0.9370\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/760], Loss: 0.3514, Accuracy: 0.8750\n",
      "Epoch [15/100], Step [100/760], Loss: 0.2450, Accuracy: 0.9219\n",
      "Epoch [15/100], Step [200/760], Loss: 0.1701, Accuracy: 0.9531\n",
      "Epoch [15/100], Step [300/760], Loss: 0.1184, Accuracy: 0.9531\n",
      "Epoch [15/100], Step [400/760], Loss: 0.2460, Accuracy: 0.9375\n",
      "Epoch [15/100], Step [500/760], Loss: 0.2414, Accuracy: 0.9219\n",
      "Epoch [15/100], Step [600/760], Loss: 0.1138, Accuracy: 0.9688\n",
      "Epoch [15/100], Step [700/760], Loss: 0.1276, Accuracy: 0.9688\n",
      "Epoch [15/100], Train Loss: 0.1913, Val Loss: 0.1587, Train Acc: 0.9339, Val Acc: 0.9458\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/760], Loss: 0.1074, Accuracy: 0.9688\n",
      "Epoch [16/100], Step [100/760], Loss: 0.1761, Accuracy: 0.9375\n",
      "Epoch [16/100], Step [200/760], Loss: 0.1750, Accuracy: 0.9531\n",
      "Epoch [16/100], Step [300/760], Loss: 0.1714, Accuracy: 0.9688\n",
      "Epoch [16/100], Step [400/760], Loss: 0.0993, Accuracy: 0.9688\n",
      "Epoch [16/100], Step [500/760], Loss: 0.1288, Accuracy: 0.9688\n",
      "Epoch [16/100], Step [600/760], Loss: 0.1303, Accuracy: 0.9688\n",
      "Epoch [16/100], Step [700/760], Loss: 0.2890, Accuracy: 0.9375\n",
      "Epoch [16/100], Train Loss: 0.1830, Val Loss: 0.1317, Train Acc: 0.9373, Val Acc: 0.9530\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/760], Loss: 0.1110, Accuracy: 0.9688\n",
      "Epoch [17/100], Step [100/760], Loss: 0.1185, Accuracy: 0.9531\n",
      "Epoch [17/100], Step [200/760], Loss: 0.1571, Accuracy: 0.9062\n",
      "Epoch [17/100], Step [300/760], Loss: 0.2119, Accuracy: 0.9219\n",
      "Epoch [17/100], Step [400/760], Loss: 0.1338, Accuracy: 0.9688\n",
      "Epoch [17/100], Step [500/760], Loss: 0.1195, Accuracy: 0.9531\n",
      "Epoch [17/100], Step [600/760], Loss: 0.3243, Accuracy: 0.8906\n",
      "Epoch [17/100], Step [700/760], Loss: 0.1181, Accuracy: 0.9531\n",
      "Epoch [17/100], Train Loss: 0.1759, Val Loss: 0.1399, Train Acc: 0.9394, Val Acc: 0.9511\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/760], Loss: 0.2779, Accuracy: 0.9219\n",
      "Epoch [18/100], Step [100/760], Loss: 0.1002, Accuracy: 0.9531\n",
      "Epoch [18/100], Step [200/760], Loss: 0.1708, Accuracy: 0.9375\n",
      "Epoch [18/100], Step [300/760], Loss: 0.1966, Accuracy: 0.9219\n",
      "Epoch [18/100], Step [400/760], Loss: 0.0724, Accuracy: 0.9688\n",
      "Epoch [18/100], Step [500/760], Loss: 0.0679, Accuracy: 0.9688\n",
      "Epoch [18/100], Step [600/760], Loss: 0.3091, Accuracy: 0.8906\n",
      "Epoch [18/100], Step [700/760], Loss: 0.2313, Accuracy: 0.9219\n",
      "Epoch [18/100], Train Loss: 0.1677, Val Loss: 0.1453, Train Acc: 0.9416, Val Acc: 0.9496\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/760], Loss: 0.0706, Accuracy: 0.9688\n",
      "Epoch [19/100], Step [100/760], Loss: 0.2137, Accuracy: 0.9375\n",
      "Epoch [19/100], Step [200/760], Loss: 0.1065, Accuracy: 0.9688\n",
      "Epoch [19/100], Step [300/760], Loss: 0.1877, Accuracy: 0.9531\n",
      "Epoch [19/100], Step [400/760], Loss: 0.1971, Accuracy: 0.9219\n",
      "Epoch [19/100], Step [500/760], Loss: 0.2124, Accuracy: 0.9375\n",
      "Epoch [19/100], Step [600/760], Loss: 0.1972, Accuracy: 0.9219\n",
      "Epoch [19/100], Step [700/760], Loss: 0.1187, Accuracy: 0.9531\n",
      "Epoch [19/100], Train Loss: 0.1640, Val Loss: 0.1208, Train Acc: 0.9431, Val Acc: 0.9592\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/760], Loss: 0.1539, Accuracy: 0.9688\n",
      "Epoch [20/100], Step [100/760], Loss: 0.1707, Accuracy: 0.9219\n",
      "Epoch [20/100], Step [200/760], Loss: 0.1871, Accuracy: 0.9219\n",
      "Epoch [20/100], Step [300/760], Loss: 0.1513, Accuracy: 0.9688\n",
      "Epoch [20/100], Step [400/760], Loss: 0.2313, Accuracy: 0.9062\n",
      "Epoch [20/100], Step [500/760], Loss: 0.0260, Accuracy: 1.0000\n",
      "Epoch [20/100], Step [600/760], Loss: 0.1687, Accuracy: 0.9219\n",
      "Epoch [20/100], Step [700/760], Loss: 0.0780, Accuracy: 0.9688\n",
      "Epoch [20/100], Train Loss: 0.1560, Val Loss: 0.1727, Train Acc: 0.9457, Val Acc: 0.9391\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/760], Loss: 0.2282, Accuracy: 0.9375\n",
      "Epoch [21/100], Step [100/760], Loss: 0.1566, Accuracy: 0.9531\n",
      "Epoch [21/100], Step [200/760], Loss: 0.3116, Accuracy: 0.8594\n",
      "Epoch [21/100], Step [300/760], Loss: 0.0481, Accuracy: 0.9844\n",
      "Epoch [21/100], Step [400/760], Loss: 0.2171, Accuracy: 0.9219\n",
      "Epoch [21/100], Step [500/760], Loss: 0.1558, Accuracy: 0.9688\n",
      "Epoch [21/100], Step [600/760], Loss: 0.1074, Accuracy: 0.9688\n",
      "Epoch [21/100], Step [700/760], Loss: 0.2200, Accuracy: 0.9531\n",
      "Epoch [21/100], Train Loss: 0.1617, Val Loss: 0.1244, Train Acc: 0.9427, Val Acc: 0.9572\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/760], Loss: 0.1291, Accuracy: 0.9375\n",
      "Epoch [22/100], Step [100/760], Loss: 0.0162, Accuracy: 1.0000\n",
      "Epoch [22/100], Step [200/760], Loss: 0.1787, Accuracy: 0.9531\n",
      "Epoch [22/100], Step [300/760], Loss: 0.2873, Accuracy: 0.9219\n",
      "Epoch [22/100], Step [400/760], Loss: 0.0665, Accuracy: 0.9688\n",
      "Epoch [22/100], Step [500/760], Loss: 0.1697, Accuracy: 0.9531\n",
      "Epoch [22/100], Step [600/760], Loss: 0.1497, Accuracy: 0.9531\n",
      "Epoch [22/100], Step [700/760], Loss: 0.3582, Accuracy: 0.8750\n",
      "Epoch [22/100], Train Loss: 0.1561, Val Loss: 0.1171, Train Acc: 0.9444, Val Acc: 0.9586\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/760], Loss: 0.1333, Accuracy: 0.9375\n",
      "Epoch [23/100], Step [100/760], Loss: 0.0960, Accuracy: 0.9844\n",
      "Epoch [23/100], Step [200/760], Loss: 0.1003, Accuracy: 0.9688\n",
      "Epoch [23/100], Step [300/760], Loss: 0.1454, Accuracy: 0.9375\n",
      "Epoch [23/100], Step [400/760], Loss: 0.2147, Accuracy: 0.9531\n",
      "Epoch [23/100], Step [500/760], Loss: 0.0610, Accuracy: 0.9844\n",
      "Epoch [23/100], Step [600/760], Loss: 0.2298, Accuracy: 0.9062\n",
      "Epoch [23/100], Step [700/760], Loss: 0.1946, Accuracy: 0.9531\n",
      "Epoch [23/100], Train Loss: 0.1436, Val Loss: 0.1376, Train Acc: 0.9498, Val Acc: 0.9514\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/760], Loss: 0.2286, Accuracy: 0.8594\n",
      "Epoch [24/100], Step [100/760], Loss: 0.2165, Accuracy: 0.9219\n",
      "Epoch [24/100], Step [200/760], Loss: 0.1029, Accuracy: 0.9844\n",
      "Epoch [24/100], Step [300/760], Loss: 0.1127, Accuracy: 0.9531\n",
      "Epoch [24/100], Step [400/760], Loss: 0.1038, Accuracy: 0.9844\n",
      "Epoch [24/100], Step [500/760], Loss: 0.1312, Accuracy: 0.9531\n",
      "Epoch [24/100], Step [600/760], Loss: 0.2045, Accuracy: 0.9219\n",
      "Epoch [24/100], Step [700/760], Loss: 0.1836, Accuracy: 0.9531\n",
      "Epoch [24/100], Train Loss: 0.1387, Val Loss: 0.1152, Train Acc: 0.9522, Val Acc: 0.9586\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/760], Loss: 0.1610, Accuracy: 0.9375\n",
      "Epoch [25/100], Step [100/760], Loss: 0.1026, Accuracy: 0.9375\n",
      "Epoch [25/100], Step [200/760], Loss: 0.1087, Accuracy: 0.9688\n",
      "Epoch [25/100], Step [300/760], Loss: 0.2901, Accuracy: 0.9219\n",
      "Epoch [25/100], Step [400/760], Loss: 0.1553, Accuracy: 0.9531\n",
      "Epoch [25/100], Step [500/760], Loss: 0.1502, Accuracy: 0.9688\n",
      "Epoch [25/100], Step [600/760], Loss: 0.0999, Accuracy: 0.9375\n",
      "Epoch [25/100], Step [700/760], Loss: 0.1301, Accuracy: 0.9688\n",
      "Epoch [25/100], Train Loss: 0.1374, Val Loss: 0.1189, Train Acc: 0.9520, Val Acc: 0.9587\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/760], Loss: 0.1452, Accuracy: 0.9688\n",
      "Epoch [26/100], Step [100/760], Loss: 0.4197, Accuracy: 0.8906\n",
      "Epoch [26/100], Step [200/760], Loss: 0.1283, Accuracy: 0.9531\n",
      "Epoch [26/100], Step [300/760], Loss: 0.0638, Accuracy: 0.9844\n",
      "Epoch [26/100], Step [400/760], Loss: 0.3205, Accuracy: 0.9375\n",
      "Epoch [26/100], Step [500/760], Loss: 0.1725, Accuracy: 0.9375\n",
      "Epoch [26/100], Step [600/760], Loss: 0.0800, Accuracy: 0.9688\n",
      "Epoch [26/100], Step [700/760], Loss: 0.0722, Accuracy: 0.9531\n",
      "Epoch [26/100], Train Loss: 0.1329, Val Loss: 0.1047, Train Acc: 0.9527, Val Acc: 0.9642\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/760], Loss: 0.1975, Accuracy: 0.9219\n",
      "Epoch [27/100], Step [100/760], Loss: 0.2759, Accuracy: 0.9062\n",
      "Epoch [27/100], Step [200/760], Loss: 0.1393, Accuracy: 0.9531\n",
      "Epoch [27/100], Step [300/760], Loss: 0.0689, Accuracy: 0.9688\n",
      "Epoch [27/100], Step [400/760], Loss: 0.1416, Accuracy: 0.9375\n",
      "Epoch [27/100], Step [500/760], Loss: 0.2005, Accuracy: 0.9531\n",
      "Epoch [27/100], Step [600/760], Loss: 0.2279, Accuracy: 0.8906\n",
      "Epoch [27/100], Step [700/760], Loss: 0.1383, Accuracy: 0.9531\n",
      "Epoch [27/100], Train Loss: 0.1298, Val Loss: 0.1025, Train Acc: 0.9545, Val Acc: 0.9649\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/760], Loss: 0.0216, Accuracy: 1.0000\n",
      "Epoch [28/100], Step [100/760], Loss: 0.1997, Accuracy: 0.9531\n",
      "Epoch [28/100], Step [200/760], Loss: 0.1315, Accuracy: 0.9219\n",
      "Epoch [28/100], Step [300/760], Loss: 0.2425, Accuracy: 0.9062\n",
      "Epoch [28/100], Step [400/760], Loss: 0.1199, Accuracy: 0.9531\n",
      "Epoch [28/100], Step [500/760], Loss: 0.0807, Accuracy: 0.9688\n",
      "Epoch [28/100], Step [600/760], Loss: 0.2056, Accuracy: 0.9375\n",
      "Epoch [28/100], Step [700/760], Loss: 0.2615, Accuracy: 0.9062\n",
      "Epoch [28/100], Train Loss: 0.1334, Val Loss: 0.1052, Train Acc: 0.9544, Val Acc: 0.9637\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/760], Loss: 0.0515, Accuracy: 0.9688\n",
      "Epoch [29/100], Step [100/760], Loss: 0.0314, Accuracy: 1.0000\n",
      "Epoch [29/100], Step [200/760], Loss: 0.1260, Accuracy: 0.9844\n",
      "Epoch [29/100], Step [300/760], Loss: 0.1062, Accuracy: 0.9531\n",
      "Epoch [29/100], Step [400/760], Loss: 0.1093, Accuracy: 0.9688\n",
      "Epoch [29/100], Step [500/760], Loss: 0.0477, Accuracy: 1.0000\n",
      "Epoch [29/100], Step [600/760], Loss: 0.0171, Accuracy: 1.0000\n",
      "Epoch [29/100], Step [700/760], Loss: 0.1796, Accuracy: 0.9375\n",
      "Epoch [29/100], Train Loss: 0.1272, Val Loss: 0.0961, Train Acc: 0.9552, Val Acc: 0.9666\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/760], Loss: 0.0812, Accuracy: 0.9531\n",
      "Epoch [30/100], Step [100/760], Loss: 0.0499, Accuracy: 0.9844\n",
      "Epoch [30/100], Step [200/760], Loss: 0.2139, Accuracy: 0.9062\n",
      "Epoch [30/100], Step [300/760], Loss: 0.1877, Accuracy: 0.9375\n",
      "Epoch [30/100], Step [400/760], Loss: 0.0612, Accuracy: 0.9844\n",
      "Epoch [30/100], Step [500/760], Loss: 0.0400, Accuracy: 0.9844\n",
      "Epoch [30/100], Step [600/760], Loss: 0.0928, Accuracy: 0.9375\n",
      "Epoch [30/100], Step [700/760], Loss: 0.1542, Accuracy: 0.9219\n",
      "Epoch [30/100], Train Loss: 0.1178, Val Loss: 0.1028, Train Acc: 0.9582, Val Acc: 0.9641\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/760], Loss: 0.0661, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [100/760], Loss: 0.0642, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [200/760], Loss: 0.1040, Accuracy: 0.9375\n",
      "Epoch [31/100], Step [300/760], Loss: 0.0614, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [400/760], Loss: 0.0407, Accuracy: 0.9844\n",
      "Epoch [31/100], Step [500/760], Loss: 0.0602, Accuracy: 0.9844\n",
      "Epoch [31/100], Step [600/760], Loss: 0.1989, Accuracy: 0.9531\n",
      "Epoch [31/100], Step [700/760], Loss: 0.1740, Accuracy: 0.9219\n",
      "Epoch [31/100], Train Loss: 0.1213, Val Loss: 0.0998, Train Acc: 0.9573, Val Acc: 0.9657\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/760], Loss: 0.0962, Accuracy: 0.9688\n",
      "Epoch [32/100], Step [100/760], Loss: 0.0655, Accuracy: 0.9688\n",
      "Epoch [32/100], Step [200/760], Loss: 0.1232, Accuracy: 0.9688\n",
      "Epoch [32/100], Step [300/760], Loss: 0.0608, Accuracy: 0.9844\n",
      "Epoch [32/100], Step [400/760], Loss: 0.0376, Accuracy: 0.9844\n",
      "Epoch [32/100], Step [500/760], Loss: 0.1054, Accuracy: 0.9688\n",
      "Epoch [32/100], Step [600/760], Loss: 0.1900, Accuracy: 0.9219\n",
      "Epoch [32/100], Step [700/760], Loss: 0.2935, Accuracy: 0.9219\n",
      "Epoch [32/100], Train Loss: 0.1183, Val Loss: 0.0892, Train Acc: 0.9592, Val Acc: 0.9686\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/760], Loss: 0.1049, Accuracy: 0.9688\n",
      "Epoch [33/100], Step [100/760], Loss: 0.1064, Accuracy: 0.9531\n",
      "Epoch [33/100], Step [200/760], Loss: 0.1069, Accuracy: 0.9844\n",
      "Epoch [33/100], Step [300/760], Loss: 0.1243, Accuracy: 0.9531\n",
      "Epoch [33/100], Step [400/760], Loss: 0.1120, Accuracy: 0.9688\n",
      "Epoch [33/100], Step [500/760], Loss: 0.1102, Accuracy: 0.9688\n",
      "Epoch [33/100], Step [600/760], Loss: 0.1787, Accuracy: 0.9219\n",
      "Epoch [33/100], Step [700/760], Loss: 0.0893, Accuracy: 0.9688\n",
      "Epoch [33/100], Train Loss: 0.1163, Val Loss: 0.0905, Train Acc: 0.9595, Val Acc: 0.9685\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/760], Loss: 0.0369, Accuracy: 0.9844\n",
      "Epoch [34/100], Step [100/760], Loss: 0.1672, Accuracy: 0.9375\n",
      "Epoch [34/100], Step [200/760], Loss: 0.0471, Accuracy: 0.9844\n",
      "Epoch [34/100], Step [300/760], Loss: 0.0539, Accuracy: 0.9688\n",
      "Epoch [34/100], Step [400/760], Loss: 0.1310, Accuracy: 0.9531\n",
      "Epoch [34/100], Step [500/760], Loss: 0.0868, Accuracy: 0.9688\n",
      "Epoch [34/100], Step [600/760], Loss: 0.0616, Accuracy: 0.9844\n",
      "Epoch [34/100], Step [700/760], Loss: 0.0922, Accuracy: 0.9531\n",
      "Epoch [34/100], Train Loss: 0.1100, Val Loss: 0.1068, Train Acc: 0.9610, Val Acc: 0.9613\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/760], Loss: 0.1265, Accuracy: 0.9531\n",
      "Epoch [35/100], Step [100/760], Loss: 0.0734, Accuracy: 0.9844\n",
      "Epoch [35/100], Step [200/760], Loss: 0.0467, Accuracy: 0.9844\n",
      "Epoch [35/100], Step [300/760], Loss: 0.1099, Accuracy: 0.9688\n",
      "Epoch [35/100], Step [400/760], Loss: 0.1478, Accuracy: 0.9375\n",
      "Epoch [35/100], Step [500/760], Loss: 0.2041, Accuracy: 0.9531\n",
      "Epoch [35/100], Step [600/760], Loss: 0.3089, Accuracy: 0.9219\n",
      "Epoch [35/100], Step [700/760], Loss: 0.1771, Accuracy: 0.9219\n",
      "Epoch [35/100], Train Loss: 0.1121, Val Loss: 0.0931, Train Acc: 0.9609, Val Acc: 0.9668\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/760], Loss: 0.1888, Accuracy: 0.9375\n",
      "Epoch [36/100], Step [100/760], Loss: 0.0679, Accuracy: 0.9688\n",
      "Epoch [36/100], Step [200/760], Loss: 0.1176, Accuracy: 0.9531\n",
      "Epoch [36/100], Step [300/760], Loss: 0.1733, Accuracy: 0.9375\n",
      "Epoch [36/100], Step [400/760], Loss: 0.1164, Accuracy: 0.9531\n",
      "Epoch [36/100], Step [500/760], Loss: 0.0814, Accuracy: 0.9844\n",
      "Epoch [36/100], Step [600/760], Loss: 0.0193, Accuracy: 1.0000\n",
      "Epoch [36/100], Step [700/760], Loss: 0.0820, Accuracy: 0.9688\n",
      "Epoch [36/100], Train Loss: 0.1079, Val Loss: 0.1083, Train Acc: 0.9619, Val Acc: 0.9612\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/760], Loss: 0.1662, Accuracy: 0.9531\n",
      "Epoch [37/100], Step [100/760], Loss: 0.0685, Accuracy: 0.9688\n",
      "Epoch [37/100], Step [200/760], Loss: 0.0387, Accuracy: 1.0000\n",
      "Epoch [37/100], Step [300/760], Loss: 0.0580, Accuracy: 1.0000\n",
      "Epoch [37/100], Step [400/760], Loss: 0.0217, Accuracy: 1.0000\n",
      "Epoch [37/100], Step [500/760], Loss: 0.1240, Accuracy: 0.9688\n",
      "Epoch [37/100], Step [600/760], Loss: 0.0166, Accuracy: 1.0000\n",
      "Epoch [37/100], Step [700/760], Loss: 0.1252, Accuracy: 0.9531\n",
      "Epoch [37/100], Train Loss: 0.0788, Val Loss: 0.0531, Train Acc: 0.9723, Val Acc: 0.9812\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/760], Loss: 0.0070, Accuracy: 1.0000\n",
      "Epoch [38/100], Step [100/760], Loss: 0.0810, Accuracy: 0.9844\n",
      "Epoch [38/100], Step [200/760], Loss: 0.0741, Accuracy: 0.9531\n",
      "Epoch [38/100], Step [300/760], Loss: 0.0528, Accuracy: 0.9688\n",
      "Epoch [38/100], Step [400/760], Loss: 0.1138, Accuracy: 0.9219\n",
      "Epoch [38/100], Step [500/760], Loss: 0.0523, Accuracy: 0.9688\n",
      "Epoch [38/100], Step [600/760], Loss: 0.0887, Accuracy: 0.9688\n",
      "Epoch [38/100], Step [700/760], Loss: 0.0388, Accuracy: 0.9844\n",
      "Epoch [38/100], Train Loss: 0.0681, Val Loss: 0.0500, Train Acc: 0.9756, Val Acc: 0.9821\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/760], Loss: 0.0263, Accuracy: 1.0000\n",
      "Epoch [39/100], Step [100/760], Loss: 0.0684, Accuracy: 0.9844\n",
      "Epoch [39/100], Step [200/760], Loss: 0.0334, Accuracy: 0.9688\n",
      "Epoch [39/100], Step [300/760], Loss: 0.0275, Accuracy: 0.9844\n",
      "Epoch [39/100], Step [400/760], Loss: 0.0256, Accuracy: 1.0000\n",
      "Epoch [39/100], Step [500/760], Loss: 0.0897, Accuracy: 0.9844\n",
      "Epoch [39/100], Step [600/760], Loss: 0.1176, Accuracy: 0.9688\n",
      "Epoch [39/100], Step [700/760], Loss: 0.0295, Accuracy: 0.9844\n",
      "Epoch [39/100], Train Loss: 0.0668, Val Loss: 0.0481, Train Acc: 0.9761, Val Acc: 0.9835\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/760], Loss: 0.1079, Accuracy: 0.9688\n",
      "Epoch [40/100], Step [100/760], Loss: 0.0180, Accuracy: 1.0000\n",
      "Epoch [40/100], Step [200/760], Loss: 0.0354, Accuracy: 0.9844\n",
      "Epoch [40/100], Step [300/760], Loss: 0.0797, Accuracy: 0.9531\n",
      "Epoch [40/100], Step [400/760], Loss: 0.0148, Accuracy: 1.0000\n",
      "Epoch [40/100], Step [500/760], Loss: 0.0957, Accuracy: 0.9844\n",
      "Epoch [40/100], Step [600/760], Loss: 0.1336, Accuracy: 0.9375\n",
      "Epoch [40/100], Step [700/760], Loss: 0.0432, Accuracy: 0.9844\n",
      "Epoch [40/100], Train Loss: 0.0606, Val Loss: 0.0435, Train Acc: 0.9781, Val Acc: 0.9852\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/760], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [41/100], Step [100/760], Loss: 0.0477, Accuracy: 0.9688\n",
      "Epoch [41/100], Step [200/760], Loss: 0.1016, Accuracy: 0.9688\n",
      "Epoch [41/100], Step [300/760], Loss: 0.0386, Accuracy: 1.0000\n",
      "Epoch [41/100], Step [400/760], Loss: 0.0786, Accuracy: 0.9844\n",
      "Epoch [41/100], Step [500/760], Loss: 0.0641, Accuracy: 0.9531\n",
      "Epoch [41/100], Step [600/760], Loss: 0.0901, Accuracy: 0.9531\n",
      "Epoch [41/100], Step [700/760], Loss: 0.0795, Accuracy: 0.9688\n",
      "Epoch [41/100], Train Loss: 0.0638, Val Loss: 0.0424, Train Acc: 0.9771, Val Acc: 0.9857\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/760], Loss: 0.0231, Accuracy: 1.0000\n",
      "Epoch [42/100], Step [100/760], Loss: 0.0413, Accuracy: 0.9844\n",
      "Epoch [42/100], Step [200/760], Loss: 0.0910, Accuracy: 0.9688\n",
      "Epoch [42/100], Step [300/760], Loss: 0.0533, Accuracy: 0.9844\n",
      "Epoch [42/100], Step [400/760], Loss: 0.0245, Accuracy: 0.9844\n",
      "Epoch [42/100], Step [500/760], Loss: 0.0398, Accuracy: 0.9844\n",
      "Epoch [42/100], Step [600/760], Loss: 0.0580, Accuracy: 0.9844\n",
      "Epoch [42/100], Step [700/760], Loss: 0.0137, Accuracy: 1.0000\n",
      "Epoch [42/100], Train Loss: 0.0570, Val Loss: 0.0441, Train Acc: 0.9794, Val Acc: 0.9840\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/760], Loss: 0.0815, Accuracy: 0.9531\n",
      "Epoch [43/100], Step [100/760], Loss: 0.0827, Accuracy: 0.9688\n",
      "Epoch [43/100], Step [200/760], Loss: 0.0705, Accuracy: 0.9688\n",
      "Epoch [43/100], Step [300/760], Loss: 0.0260, Accuracy: 1.0000\n",
      "Epoch [43/100], Step [400/760], Loss: 0.0164, Accuracy: 1.0000\n",
      "Epoch [43/100], Step [500/760], Loss: 0.1372, Accuracy: 0.9219\n",
      "Epoch [43/100], Step [600/760], Loss: 0.0239, Accuracy: 1.0000\n",
      "Epoch [43/100], Step [700/760], Loss: 0.0093, Accuracy: 1.0000\n",
      "Epoch [43/100], Train Loss: 0.0573, Val Loss: 0.0421, Train Acc: 0.9796, Val Acc: 0.9850\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/760], Loss: 0.0859, Accuracy: 0.9688\n",
      "Epoch [44/100], Step [100/760], Loss: 0.0225, Accuracy: 0.9844\n",
      "Epoch [44/100], Step [200/760], Loss: 0.0336, Accuracy: 0.9844\n",
      "Epoch [44/100], Step [300/760], Loss: 0.0561, Accuracy: 0.9688\n",
      "Epoch [44/100], Step [400/760], Loss: 0.0214, Accuracy: 1.0000\n",
      "Epoch [44/100], Step [500/760], Loss: 0.0520, Accuracy: 0.9688\n",
      "Epoch [44/100], Step [600/760], Loss: 0.0490, Accuracy: 0.9844\n",
      "Epoch [44/100], Step [700/760], Loss: 0.0842, Accuracy: 0.9688\n",
      "Epoch [44/100], Train Loss: 0.0548, Val Loss: 0.0454, Train Acc: 0.9801, Val Acc: 0.9838\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/760], Loss: 0.1235, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [100/760], Loss: 0.1378, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [200/760], Loss: 0.0577, Accuracy: 0.9844\n",
      "Epoch [45/100], Step [300/760], Loss: 0.0734, Accuracy: 0.9844\n",
      "Epoch [45/100], Step [400/760], Loss: 0.0528, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [500/760], Loss: 0.0471, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [600/760], Loss: 0.0487, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [700/760], Loss: 0.0567, Accuracy: 0.9844\n",
      "Epoch [45/100], Train Loss: 0.0597, Val Loss: 0.0411, Train Acc: 0.9782, Val Acc: 0.9854\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/760], Loss: 0.0385, Accuracy: 0.9844\n",
      "Epoch [46/100], Step [100/760], Loss: 0.0715, Accuracy: 0.9844\n",
      "Epoch [46/100], Step [200/760], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [46/100], Step [300/760], Loss: 0.1299, Accuracy: 0.9531\n",
      "Epoch [46/100], Step [400/760], Loss: 0.0448, Accuracy: 0.9844\n",
      "Epoch [46/100], Step [500/760], Loss: 0.0364, Accuracy: 0.9844\n",
      "Epoch [46/100], Step [600/760], Loss: 0.0094, Accuracy: 1.0000\n",
      "Epoch [46/100], Step [700/760], Loss: 0.0497, Accuracy: 0.9844\n",
      "Epoch [46/100], Train Loss: 0.0534, Val Loss: 0.0377, Train Acc: 0.9804, Val Acc: 0.9862\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/760], Loss: 0.0845, Accuracy: 0.9688\n",
      "Epoch [47/100], Step [100/760], Loss: 0.0354, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [200/760], Loss: 0.0325, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [300/760], Loss: 0.0777, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [400/760], Loss: 0.0380, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [500/760], Loss: 0.0247, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [600/760], Loss: 0.0669, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [700/760], Loss: 0.1102, Accuracy: 0.9531\n",
      "Epoch [47/100], Train Loss: 0.0532, Val Loss: 0.0361, Train Acc: 0.9814, Val Acc: 0.9874\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/760], Loss: 0.0048, Accuracy: 1.0000\n",
      "Epoch [48/100], Step [100/760], Loss: 0.0165, Accuracy: 1.0000\n",
      "Epoch [48/100], Step [200/760], Loss: 0.0486, Accuracy: 0.9844\n",
      "Epoch [48/100], Step [300/760], Loss: 0.0447, Accuracy: 0.9688\n",
      "Epoch [48/100], Step [400/760], Loss: 0.0814, Accuracy: 0.9688\n",
      "Epoch [48/100], Step [500/760], Loss: 0.0506, Accuracy: 0.9844\n",
      "Epoch [48/100], Step [600/760], Loss: 0.0246, Accuracy: 0.9844\n",
      "Epoch [48/100], Step [700/760], Loss: 0.0189, Accuracy: 1.0000\n",
      "Epoch [48/100], Train Loss: 0.0520, Val Loss: 0.0374, Train Acc: 0.9816, Val Acc: 0.9869\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/760], Loss: 0.0346, Accuracy: 0.9844\n",
      "Epoch [49/100], Step [100/760], Loss: 0.0082, Accuracy: 1.0000\n",
      "Epoch [49/100], Step [200/760], Loss: 0.0243, Accuracy: 0.9844\n",
      "Epoch [49/100], Step [300/760], Loss: 0.0187, Accuracy: 1.0000\n",
      "Epoch [49/100], Step [400/760], Loss: 0.0827, Accuracy: 0.9688\n",
      "Epoch [49/100], Step [500/760], Loss: 0.0348, Accuracy: 0.9844\n",
      "Epoch [49/100], Step [600/760], Loss: 0.0071, Accuracy: 1.0000\n",
      "Epoch [49/100], Step [700/760], Loss: 0.0185, Accuracy: 1.0000\n",
      "Epoch [49/100], Train Loss: 0.0523, Val Loss: 0.0351, Train Acc: 0.9807, Val Acc: 0.9876\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/760], Loss: 0.0864, Accuracy: 0.9844\n",
      "Epoch [50/100], Step [100/760], Loss: 0.0442, Accuracy: 0.9844\n",
      "Epoch [50/100], Step [200/760], Loss: 0.0217, Accuracy: 1.0000\n",
      "Epoch [50/100], Step [300/760], Loss: 0.0787, Accuracy: 0.9531\n",
      "Epoch [50/100], Step [400/760], Loss: 0.0075, Accuracy: 1.0000\n",
      "Epoch [50/100], Step [500/760], Loss: 0.0354, Accuracy: 0.9844\n",
      "Epoch [50/100], Step [600/760], Loss: 0.0226, Accuracy: 1.0000\n",
      "Epoch [50/100], Step [700/760], Loss: 0.0491, Accuracy: 0.9688\n",
      "Epoch [50/100], Train Loss: 0.0532, Val Loss: 0.0337, Train Acc: 0.9803, Val Acc: 0.9880\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/760], Loss: 0.0184, Accuracy: 1.0000\n",
      "Epoch [51/100], Step [100/760], Loss: 0.0238, Accuracy: 0.9844\n",
      "Epoch [51/100], Step [200/760], Loss: 0.0524, Accuracy: 0.9688\n",
      "Epoch [51/100], Step [300/760], Loss: 0.0057, Accuracy: 1.0000\n",
      "Epoch [51/100], Step [400/760], Loss: 0.0362, Accuracy: 0.9844\n",
      "Epoch [51/100], Step [500/760], Loss: 0.0340, Accuracy: 0.9844\n",
      "Epoch [51/100], Step [600/760], Loss: 0.0098, Accuracy: 1.0000\n",
      "Epoch [51/100], Step [700/760], Loss: 0.1314, Accuracy: 0.9688\n",
      "Epoch [51/100], Train Loss: 0.0496, Val Loss: 0.0353, Train Acc: 0.9822, Val Acc: 0.9872\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/760], Loss: 0.1661, Accuracy: 0.9688\n",
      "Epoch [52/100], Step [100/760], Loss: 0.0488, Accuracy: 0.9688\n",
      "Epoch [52/100], Step [200/760], Loss: 0.0363, Accuracy: 0.9844\n",
      "Epoch [52/100], Step [300/760], Loss: 0.0113, Accuracy: 1.0000\n",
      "Epoch [52/100], Step [400/760], Loss: 0.1015, Accuracy: 0.9688\n",
      "Epoch [52/100], Step [500/760], Loss: 0.0109, Accuracy: 1.0000\n",
      "Epoch [52/100], Step [600/760], Loss: 0.0214, Accuracy: 0.9844\n",
      "Epoch [52/100], Step [700/760], Loss: 0.0475, Accuracy: 0.9844\n",
      "Epoch [52/100], Train Loss: 0.0509, Val Loss: 0.0374, Train Acc: 0.9815, Val Acc: 0.9860\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/760], Loss: 0.0371, Accuracy: 0.9844\n",
      "Epoch [53/100], Step [100/760], Loss: 0.0150, Accuracy: 1.0000\n",
      "Epoch [53/100], Step [200/760], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [53/100], Step [300/760], Loss: 0.0050, Accuracy: 1.0000\n",
      "Epoch [53/100], Step [400/760], Loss: 0.0228, Accuracy: 1.0000\n",
      "Epoch [53/100], Step [500/760], Loss: 0.0900, Accuracy: 0.9844\n",
      "Epoch [53/100], Step [600/760], Loss: 0.0225, Accuracy: 1.0000\n",
      "Epoch [53/100], Step [700/760], Loss: 0.0066, Accuracy: 1.0000\n",
      "Epoch [53/100], Train Loss: 0.0511, Val Loss: 0.0323, Train Acc: 0.9811, Val Acc: 0.9892\n",
      "==================================================\n",
      "Epoch [54/100], Step [0/760], Loss: 0.0343, Accuracy: 1.0000\n",
      "Epoch [54/100], Step [100/760], Loss: 0.1075, Accuracy: 0.9688\n",
      "Epoch [54/100], Step [200/760], Loss: 0.0304, Accuracy: 0.9844\n",
      "Epoch [54/100], Step [300/760], Loss: 0.0260, Accuracy: 1.0000\n",
      "Epoch [54/100], Step [400/760], Loss: 0.0673, Accuracy: 0.9844\n",
      "Epoch [54/100], Step [500/760], Loss: 0.1105, Accuracy: 0.9531\n",
      "Epoch [54/100], Step [600/760], Loss: 0.0204, Accuracy: 1.0000\n",
      "Epoch [54/100], Step [700/760], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [54/100], Train Loss: 0.0460, Val Loss: 0.0313, Train Acc: 0.9837, Val Acc: 0.9891\n",
      "==================================================\n",
      "Epoch [55/100], Step [0/760], Loss: 0.0284, Accuracy: 0.9844\n",
      "Epoch [55/100], Step [100/760], Loss: 0.0880, Accuracy: 0.9531\n",
      "Epoch [55/100], Step [200/760], Loss: 0.0122, Accuracy: 1.0000\n",
      "Epoch [55/100], Step [300/760], Loss: 0.0069, Accuracy: 1.0000\n",
      "Epoch [55/100], Step [400/760], Loss: 0.0790, Accuracy: 0.9688\n",
      "Epoch [55/100], Step [500/760], Loss: 0.0240, Accuracy: 0.9844\n",
      "Epoch [55/100], Step [600/760], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [55/100], Step [700/760], Loss: 0.0952, Accuracy: 0.9531\n",
      "Epoch [55/100], Train Loss: 0.0485, Val Loss: 0.0346, Train Acc: 0.9822, Val Acc: 0.9881\n",
      "==================================================\n",
      "Epoch [56/100], Step [0/760], Loss: 0.0632, Accuracy: 0.9688\n",
      "Epoch [56/100], Step [100/760], Loss: 0.0320, Accuracy: 0.9844\n",
      "Epoch [56/100], Step [200/760], Loss: 0.0562, Accuracy: 0.9688\n",
      "Epoch [56/100], Step [300/760], Loss: 0.0100, Accuracy: 1.0000\n",
      "Epoch [56/100], Step [400/760], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [56/100], Step [500/760], Loss: 0.0463, Accuracy: 0.9844\n",
      "Epoch [56/100], Step [600/760], Loss: 0.0726, Accuracy: 0.9844\n",
      "Epoch [56/100], Step [700/760], Loss: 0.0368, Accuracy: 0.9844\n",
      "Epoch [56/100], Train Loss: 0.0458, Val Loss: 0.0323, Train Acc: 0.9836, Val Acc: 0.9881\n",
      "==================================================\n",
      "Epoch [57/100], Step [0/760], Loss: 0.1058, Accuracy: 0.9531\n",
      "Epoch [57/100], Step [100/760], Loss: 0.1304, Accuracy: 0.9375\n",
      "Epoch [57/100], Step [200/760], Loss: 0.0205, Accuracy: 1.0000\n",
      "Epoch [57/100], Step [300/760], Loss: 0.0187, Accuracy: 0.9844\n",
      "Epoch [57/100], Step [400/760], Loss: 0.0360, Accuracy: 0.9688\n",
      "Epoch [57/100], Step [500/760], Loss: 0.1008, Accuracy: 0.9531\n",
      "Epoch [57/100], Step [600/760], Loss: 0.0074, Accuracy: 1.0000\n",
      "Epoch [57/100], Step [700/760], Loss: 0.0693, Accuracy: 0.9844\n",
      "Epoch [57/100], Train Loss: 0.0481, Val Loss: 0.0326, Train Acc: 0.9823, Val Acc: 0.9885\n",
      "==================================================\n",
      "Epoch [58/100], Step [0/760], Loss: 0.0156, Accuracy: 1.0000\n",
      "Epoch [58/100], Step [100/760], Loss: 0.0313, Accuracy: 0.9844\n",
      "Epoch [58/100], Step [200/760], Loss: 0.1284, Accuracy: 0.9375\n",
      "Epoch [58/100], Step [300/760], Loss: 0.0439, Accuracy: 0.9688\n",
      "Epoch [58/100], Step [400/760], Loss: 0.0117, Accuracy: 1.0000\n",
      "Epoch [58/100], Step [500/760], Loss: 0.0287, Accuracy: 1.0000\n",
      "Epoch [58/100], Step [600/760], Loss: 0.0251, Accuracy: 1.0000\n",
      "Epoch [58/100], Step [700/760], Loss: 0.0232, Accuracy: 0.9844\n",
      "Epoch [58/100], Train Loss: 0.0455, Val Loss: 0.0299, Train Acc: 0.9834, Val Acc: 0.9891\n",
      "==================================================\n",
      "Epoch [59/100], Step [0/760], Loss: 0.0395, Accuracy: 0.9844\n",
      "Epoch [59/100], Step [100/760], Loss: 0.0572, Accuracy: 0.9688\n",
      "Epoch [59/100], Step [200/760], Loss: 0.0097, Accuracy: 1.0000\n",
      "Epoch [59/100], Step [300/760], Loss: 0.0643, Accuracy: 0.9844\n",
      "Epoch [59/100], Step [400/760], Loss: 0.0174, Accuracy: 1.0000\n",
      "Epoch [59/100], Step [500/760], Loss: 0.0269, Accuracy: 0.9844\n",
      "Epoch [59/100], Step [600/760], Loss: 0.0112, Accuracy: 1.0000\n",
      "Epoch [59/100], Step [700/760], Loss: 0.0713, Accuracy: 0.9688\n",
      "Epoch [59/100], Train Loss: 0.0454, Val Loss: 0.0324, Train Acc: 0.9832, Val Acc: 0.9883\n",
      "==================================================\n",
      "Epoch [60/100], Step [0/760], Loss: 0.0068, Accuracy: 1.0000\n",
      "Epoch [60/100], Step [100/760], Loss: 0.0571, Accuracy: 0.9688\n",
      "Epoch [60/100], Step [200/760], Loss: 0.0203, Accuracy: 0.9844\n",
      "Epoch [60/100], Step [300/760], Loss: 0.0581, Accuracy: 0.9688\n",
      "Epoch [60/100], Step [400/760], Loss: 0.0091, Accuracy: 1.0000\n",
      "Epoch [60/100], Step [500/760], Loss: 0.0726, Accuracy: 0.9688\n",
      "Epoch [60/100], Step [600/760], Loss: 0.0837, Accuracy: 0.9375\n",
      "Epoch [60/100], Step [700/760], Loss: 0.0310, Accuracy: 0.9844\n",
      "Epoch [60/100], Train Loss: 0.0444, Val Loss: 0.0289, Train Acc: 0.9836, Val Acc: 0.9899\n",
      "==================================================\n",
      "Epoch [61/100], Step [0/760], Loss: 0.0753, Accuracy: 0.9844\n",
      "Epoch [61/100], Step [100/760], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [61/100], Step [200/760], Loss: 0.0497, Accuracy: 0.9844\n",
      "Epoch [61/100], Step [300/760], Loss: 0.0481, Accuracy: 0.9844\n",
      "Epoch [61/100], Step [400/760], Loss: 0.0953, Accuracy: 0.9531\n",
      "Epoch [61/100], Step [500/760], Loss: 0.0736, Accuracy: 0.9688\n",
      "Epoch [61/100], Step [600/760], Loss: 0.0442, Accuracy: 0.9844\n",
      "Epoch [61/100], Step [700/760], Loss: 0.0433, Accuracy: 0.9844\n",
      "Epoch [61/100], Train Loss: 0.0473, Val Loss: 0.0293, Train Acc: 0.9825, Val Acc: 0.9897\n",
      "==================================================\n",
      "Epoch [62/100], Step [0/760], Loss: 0.0106, Accuracy: 1.0000\n",
      "Epoch [62/100], Step [100/760], Loss: 0.0027, Accuracy: 1.0000\n",
      "Epoch [62/100], Step [200/760], Loss: 0.0850, Accuracy: 0.9688\n",
      "Epoch [62/100], Step [300/760], Loss: 0.0324, Accuracy: 0.9844\n",
      "Epoch [62/100], Step [400/760], Loss: 0.0370, Accuracy: 1.0000\n",
      "Epoch [62/100], Step [500/760], Loss: 0.0150, Accuracy: 1.0000\n",
      "Epoch [62/100], Step [600/760], Loss: 0.0126, Accuracy: 1.0000\n",
      "Epoch [62/100], Step [700/760], Loss: 0.0155, Accuracy: 1.0000\n",
      "Epoch [62/100], Train Loss: 0.0454, Val Loss: 0.0271, Train Acc: 0.9836, Val Acc: 0.9904\n",
      "==================================================\n",
      "Epoch [63/100], Step [0/760], Loss: 0.0372, Accuracy: 0.9844\n",
      "Epoch [63/100], Step [100/760], Loss: 0.0854, Accuracy: 0.9531\n",
      "Epoch [63/100], Step [200/760], Loss: 0.1126, Accuracy: 0.9688\n",
      "Epoch [63/100], Step [300/760], Loss: 0.0473, Accuracy: 0.9844\n",
      "Epoch [63/100], Step [400/760], Loss: 0.0108, Accuracy: 1.0000\n",
      "Epoch [63/100], Step [500/760], Loss: 0.0806, Accuracy: 0.9531\n",
      "Epoch [63/100], Step [600/760], Loss: 0.0521, Accuracy: 0.9844\n",
      "Epoch [63/100], Step [700/760], Loss: 0.0654, Accuracy: 0.9688\n",
      "Epoch [63/100], Train Loss: 0.0435, Val Loss: 0.0270, Train Acc: 0.9837, Val Acc: 0.9903\n",
      "==================================================\n",
      "Epoch [64/100], Step [0/760], Loss: 0.0155, Accuracy: 1.0000\n",
      "Epoch [64/100], Step [100/760], Loss: 0.0203, Accuracy: 0.9844\n",
      "Epoch [64/100], Step [200/760], Loss: 0.0419, Accuracy: 0.9688\n",
      "Epoch [64/100], Step [300/760], Loss: 0.0245, Accuracy: 1.0000\n",
      "Epoch [64/100], Step [400/760], Loss: 0.0807, Accuracy: 0.9531\n",
      "Epoch [64/100], Step [500/760], Loss: 0.0311, Accuracy: 0.9844\n",
      "Epoch [64/100], Step [600/760], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [64/100], Step [700/760], Loss: 0.0037, Accuracy: 1.0000\n",
      "Epoch [64/100], Train Loss: 0.0434, Val Loss: 0.0268, Train Acc: 0.9844, Val Acc: 0.9907\n",
      "==================================================\n",
      "Epoch [65/100], Step [0/760], Loss: 0.0252, Accuracy: 1.0000\n",
      "Epoch [65/100], Step [100/760], Loss: 0.0091, Accuracy: 1.0000\n",
      "Epoch [65/100], Step [200/760], Loss: 0.0281, Accuracy: 0.9844\n",
      "Epoch [65/100], Step [300/760], Loss: 0.0385, Accuracy: 0.9844\n",
      "Epoch [65/100], Step [400/760], Loss: 0.0462, Accuracy: 0.9844\n",
      "Epoch [65/100], Step [500/760], Loss: 0.0394, Accuracy: 0.9844\n",
      "Epoch [65/100], Step [600/760], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [65/100], Step [700/760], Loss: 0.0163, Accuracy: 1.0000\n",
      "Epoch [65/100], Train Loss: 0.0430, Val Loss: 0.0271, Train Acc: 0.9851, Val Acc: 0.9908\n",
      "==================================================\n",
      "Epoch [66/100], Step [0/760], Loss: 0.0047, Accuracy: 1.0000\n",
      "Epoch [66/100], Step [100/760], Loss: 0.0711, Accuracy: 0.9688\n",
      "Epoch [66/100], Step [200/760], Loss: 0.0170, Accuracy: 1.0000\n",
      "Epoch [66/100], Step [300/760], Loss: 0.0432, Accuracy: 0.9688\n",
      "Epoch [66/100], Step [400/760], Loss: 0.0307, Accuracy: 0.9844\n",
      "Epoch [66/100], Step [500/760], Loss: 0.0212, Accuracy: 1.0000\n",
      "Epoch [66/100], Step [600/760], Loss: 0.0442, Accuracy: 0.9688\n",
      "Epoch [66/100], Step [700/760], Loss: 0.0618, Accuracy: 0.9844\n",
      "Epoch [66/100], Train Loss: 0.0415, Val Loss: 0.0268, Train Acc: 0.9851, Val Acc: 0.9904\n",
      "==================================================\n",
      "Epoch [67/100], Step [0/760], Loss: 0.0578, Accuracy: 0.9688\n",
      "Epoch [67/100], Step [100/760], Loss: 0.0060, Accuracy: 1.0000\n",
      "Epoch [67/100], Step [200/760], Loss: 0.0481, Accuracy: 0.9688\n",
      "Epoch [67/100], Step [300/760], Loss: 0.0366, Accuracy: 0.9844\n",
      "Epoch [67/100], Step [400/760], Loss: 0.0529, Accuracy: 0.9688\n",
      "Epoch [67/100], Step [500/760], Loss: 0.0210, Accuracy: 1.0000\n",
      "Epoch [67/100], Step [600/760], Loss: 0.0306, Accuracy: 0.9844\n",
      "Epoch [67/100], Step [700/760], Loss: 0.0807, Accuracy: 0.9844\n",
      "Epoch [67/100], Train Loss: 0.0428, Val Loss: 0.0280, Train Acc: 0.9853, Val Acc: 0.9900\n",
      "==================================================\n",
      "Epoch [68/100], Step [0/760], Loss: 0.0126, Accuracy: 1.0000\n",
      "Epoch [68/100], Step [100/760], Loss: 0.0631, Accuracy: 0.9844\n",
      "Epoch [68/100], Step [200/760], Loss: 0.0653, Accuracy: 0.9844\n",
      "Epoch [68/100], Step [300/760], Loss: 0.0181, Accuracy: 0.9844\n",
      "Epoch [68/100], Step [400/760], Loss: 0.0060, Accuracy: 1.0000\n",
      "Epoch [68/100], Step [500/760], Loss: 0.0620, Accuracy: 0.9688\n",
      "Epoch [68/100], Step [600/760], Loss: 0.0627, Accuracy: 0.9844\n",
      "Epoch [68/100], Step [700/760], Loss: 0.0626, Accuracy: 0.9688\n",
      "Epoch [68/100], Train Loss: 0.0412, Val Loss: 0.0249, Train Acc: 0.9854, Val Acc: 0.9915\n",
      "==================================================\n",
      "Epoch [69/100], Step [0/760], Loss: 0.0085, Accuracy: 1.0000\n",
      "Epoch [69/100], Step [100/760], Loss: 0.0045, Accuracy: 1.0000\n",
      "Epoch [69/100], Step [200/760], Loss: 0.1353, Accuracy: 0.9531\n",
      "Epoch [69/100], Step [300/760], Loss: 0.0143, Accuracy: 1.0000\n",
      "Epoch [69/100], Step [400/760], Loss: 0.0913, Accuracy: 0.9688\n",
      "Epoch [69/100], Step [500/760], Loss: 0.0206, Accuracy: 1.0000\n",
      "Epoch [69/100], Step [600/760], Loss: 0.0373, Accuracy: 0.9688\n",
      "Epoch [69/100], Step [700/760], Loss: 0.0478, Accuracy: 0.9844\n",
      "Epoch [69/100], Train Loss: 0.0429, Val Loss: 0.0258, Train Acc: 0.9846, Val Acc: 0.9912\n",
      "==================================================\n",
      "Epoch [70/100], Step [0/760], Loss: 0.0049, Accuracy: 1.0000\n",
      "Epoch [70/100], Step [100/760], Loss: 0.0106, Accuracy: 1.0000\n",
      "Epoch [70/100], Step [200/760], Loss: 0.1607, Accuracy: 0.9531\n",
      "Epoch [70/100], Step [300/760], Loss: 0.0993, Accuracy: 0.9688\n",
      "Epoch [70/100], Step [400/760], Loss: 0.0391, Accuracy: 0.9688\n",
      "Epoch [70/100], Step [500/760], Loss: 0.0055, Accuracy: 1.0000\n",
      "Epoch [70/100], Step [600/760], Loss: 0.0183, Accuracy: 1.0000\n",
      "Epoch [70/100], Step [700/760], Loss: 0.0343, Accuracy: 0.9688\n",
      "Epoch [70/100], Train Loss: 0.0393, Val Loss: 0.0251, Train Acc: 0.9862, Val Acc: 0.9912\n",
      "==================================================\n",
      "Epoch [71/100], Step [0/760], Loss: 0.0446, Accuracy: 0.9688\n",
      "Epoch [71/100], Step [100/760], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [71/100], Step [200/760], Loss: 0.0249, Accuracy: 0.9844\n",
      "Epoch [71/100], Step [300/760], Loss: 0.0108, Accuracy: 1.0000\n",
      "Epoch [71/100], Step [400/760], Loss: 0.0542, Accuracy: 0.9844\n",
      "Epoch [71/100], Step [500/760], Loss: 0.0155, Accuracy: 1.0000\n",
      "Epoch [71/100], Step [600/760], Loss: 0.0433, Accuracy: 0.9844\n",
      "Epoch [71/100], Step [700/760], Loss: 0.0366, Accuracy: 0.9844\n",
      "Epoch [71/100], Train Loss: 0.0399, Val Loss: 0.0249, Train Acc: 0.9853, Val Acc: 0.9910\n",
      "==================================================\n",
      "Epoch [72/100], Step [0/760], Loss: 0.0518, Accuracy: 0.9688\n",
      "Epoch [72/100], Step [100/760], Loss: 0.0230, Accuracy: 0.9844\n",
      "Epoch [72/100], Step [200/760], Loss: 0.0541, Accuracy: 0.9844\n",
      "Epoch [72/100], Step [300/760], Loss: 0.0388, Accuracy: 0.9688\n",
      "Epoch [72/100], Step [400/760], Loss: 0.0028, Accuracy: 1.0000\n",
      "Epoch [72/100], Step [500/760], Loss: 0.0241, Accuracy: 0.9844\n",
      "Epoch [72/100], Step [600/760], Loss: 0.0210, Accuracy: 1.0000\n",
      "Epoch [72/100], Step [700/760], Loss: 0.0062, Accuracy: 1.0000\n",
      "Epoch [72/100], Train Loss: 0.0419, Val Loss: 0.0240, Train Acc: 0.9848, Val Acc: 0.9917\n",
      "==================================================\n",
      "Epoch [73/100], Step [0/760], Loss: 0.0055, Accuracy: 1.0000\n",
      "Epoch [73/100], Step [100/760], Loss: 0.0336, Accuracy: 0.9844\n",
      "Epoch [73/100], Step [200/760], Loss: 0.0150, Accuracy: 1.0000\n",
      "Epoch [73/100], Step [300/760], Loss: 0.0156, Accuracy: 1.0000\n",
      "Epoch [73/100], Step [400/760], Loss: 0.0044, Accuracy: 1.0000\n",
      "Epoch [73/100], Step [500/760], Loss: 0.0485, Accuracy: 0.9688\n",
      "Epoch [73/100], Step [600/760], Loss: 0.0059, Accuracy: 1.0000\n",
      "Epoch [73/100], Step [700/760], Loss: 0.0192, Accuracy: 0.9844\n",
      "Epoch [73/100], Train Loss: 0.0382, Val Loss: 0.0241, Train Acc: 0.9862, Val Acc: 0.9914\n",
      "==================================================\n",
      "Epoch [74/100], Step [0/760], Loss: 0.0636, Accuracy: 0.9688\n",
      "Epoch [74/100], Step [100/760], Loss: 0.1288, Accuracy: 0.9688\n",
      "Epoch [74/100], Step [200/760], Loss: 0.0293, Accuracy: 0.9844\n",
      "Epoch [74/100], Step [300/760], Loss: 0.0571, Accuracy: 0.9688\n",
      "Epoch [74/100], Step [400/760], Loss: 0.1167, Accuracy: 0.9688\n",
      "Epoch [74/100], Step [500/760], Loss: 0.0091, Accuracy: 1.0000\n",
      "Epoch [74/100], Step [600/760], Loss: 0.0835, Accuracy: 0.9688\n",
      "Epoch [74/100], Step [700/760], Loss: 0.0038, Accuracy: 1.0000\n",
      "Epoch [74/100], Train Loss: 0.0395, Val Loss: 0.0243, Train Acc: 0.9856, Val Acc: 0.9916\n",
      "==================================================\n",
      "Epoch [75/100], Step [0/760], Loss: 0.0057, Accuracy: 1.0000\n",
      "Epoch [75/100], Step [100/760], Loss: 0.0275, Accuracy: 0.9844\n",
      "Epoch [75/100], Step [200/760], Loss: 0.0720, Accuracy: 0.9844\n",
      "Epoch [75/100], Step [300/760], Loss: 0.0185, Accuracy: 0.9844\n",
      "Epoch [75/100], Step [400/760], Loss: 0.0256, Accuracy: 1.0000\n",
      "Epoch [75/100], Step [500/760], Loss: 0.0502, Accuracy: 0.9531\n",
      "Epoch [75/100], Step [600/760], Loss: 0.0126, Accuracy: 1.0000\n",
      "Epoch [75/100], Step [700/760], Loss: 0.0449, Accuracy: 0.9844\n",
      "Epoch [75/100], Train Loss: 0.0378, Val Loss: 0.0230, Train Acc: 0.9861, Val Acc: 0.9919\n",
      "==================================================\n",
      "Epoch [76/100], Step [0/760], Loss: 0.0906, Accuracy: 0.9688\n",
      "Epoch [76/100], Step [100/760], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [76/100], Step [200/760], Loss: 0.0345, Accuracy: 0.9844\n",
      "Epoch [76/100], Step [300/760], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [76/100], Step [400/760], Loss: 0.0053, Accuracy: 1.0000\n",
      "Epoch [76/100], Step [500/760], Loss: 0.0179, Accuracy: 1.0000\n",
      "Epoch [76/100], Step [600/760], Loss: 0.0651, Accuracy: 0.9688\n",
      "Epoch [76/100], Step [700/760], Loss: 0.0054, Accuracy: 1.0000\n",
      "Epoch [76/100], Train Loss: 0.0365, Val Loss: 0.0274, Train Acc: 0.9863, Val Acc: 0.9906\n",
      "==================================================\n",
      "Epoch [77/100], Step [0/760], Loss: 0.0124, Accuracy: 1.0000\n",
      "Epoch [77/100], Step [100/760], Loss: 0.0229, Accuracy: 0.9844\n",
      "Epoch [77/100], Step [200/760], Loss: 0.0642, Accuracy: 0.9844\n",
      "Epoch [77/100], Step [300/760], Loss: 0.0358, Accuracy: 0.9844\n",
      "Epoch [77/100], Step [400/760], Loss: 0.0376, Accuracy: 0.9844\n",
      "Epoch [77/100], Step [500/760], Loss: 0.0201, Accuracy: 1.0000\n",
      "Epoch [77/100], Step [600/760], Loss: 0.0514, Accuracy: 0.9844\n",
      "Epoch [77/100], Step [700/760], Loss: 0.0306, Accuracy: 0.9844\n",
      "Epoch [77/100], Train Loss: 0.0371, Val Loss: 0.0209, Train Acc: 0.9866, Val Acc: 0.9924\n",
      "==================================================\n",
      "Epoch [78/100], Step [0/760], Loss: 0.0187, Accuracy: 0.9844\n",
      "Epoch [78/100], Step [100/760], Loss: 0.0425, Accuracy: 0.9688\n",
      "Epoch [78/100], Step [200/760], Loss: 0.0418, Accuracy: 0.9844\n",
      "Epoch [78/100], Step [300/760], Loss: 0.0179, Accuracy: 1.0000\n",
      "Epoch [78/100], Step [400/760], Loss: 0.0059, Accuracy: 1.0000\n",
      "Epoch [78/100], Step [500/760], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [78/100], Step [600/760], Loss: 0.0162, Accuracy: 1.0000\n",
      "Epoch [78/100], Step [700/760], Loss: 0.0582, Accuracy: 0.9688\n",
      "Epoch [78/100], Train Loss: 0.0383, Val Loss: 0.0248, Train Acc: 0.9866, Val Acc: 0.9909\n",
      "==================================================\n",
      "Epoch [79/100], Step [0/760], Loss: 0.0291, Accuracy: 0.9844\n",
      "Epoch [79/100], Step [100/760], Loss: 0.0070, Accuracy: 1.0000\n",
      "Epoch [79/100], Step [200/760], Loss: 0.0355, Accuracy: 0.9844\n",
      "Epoch [79/100], Step [300/760], Loss: 0.0281, Accuracy: 1.0000\n",
      "Epoch [79/100], Step [400/760], Loss: 0.0830, Accuracy: 0.9688\n",
      "Epoch [79/100], Step [500/760], Loss: 0.0141, Accuracy: 1.0000\n",
      "Epoch [79/100], Step [600/760], Loss: 0.0155, Accuracy: 0.9844\n",
      "Epoch [79/100], Step [700/760], Loss: 0.0377, Accuracy: 0.9844\n",
      "Epoch [79/100], Train Loss: 0.0354, Val Loss: 0.0233, Train Acc: 0.9868, Val Acc: 0.9918\n",
      "==================================================\n",
      "Epoch [80/100], Step [0/760], Loss: 0.0508, Accuracy: 0.9688\n",
      "Epoch [80/100], Step [100/760], Loss: 0.0109, Accuracy: 1.0000\n",
      "Epoch [80/100], Step [200/760], Loss: 0.0091, Accuracy: 1.0000\n",
      "Epoch [80/100], Step [300/760], Loss: 0.0066, Accuracy: 1.0000\n",
      "Epoch [80/100], Step [400/760], Loss: 0.0116, Accuracy: 1.0000\n",
      "Epoch [80/100], Step [500/760], Loss: 0.0054, Accuracy: 1.0000\n",
      "Epoch [80/100], Step [600/760], Loss: 0.0807, Accuracy: 0.9688\n",
      "Epoch [80/100], Step [700/760], Loss: 0.0164, Accuracy: 1.0000\n",
      "Epoch [80/100], Train Loss: 0.0354, Val Loss: 0.0284, Train Acc: 0.9878, Val Acc: 0.9903\n",
      "==================================================\n",
      "Epoch [81/100], Step [0/760], Loss: 0.0363, Accuracy: 0.9688\n",
      "Epoch [81/100], Step [100/760], Loss: 0.0146, Accuracy: 1.0000\n",
      "Epoch [81/100], Step [200/760], Loss: 0.0876, Accuracy: 0.9844\n",
      "Epoch [81/100], Step [300/760], Loss: 0.0767, Accuracy: 0.9688\n",
      "Epoch [81/100], Step [400/760], Loss: 0.0638, Accuracy: 0.9844\n",
      "Epoch [81/100], Step [500/760], Loss: 0.0370, Accuracy: 0.9844\n",
      "Epoch [81/100], Step [600/760], Loss: 0.0615, Accuracy: 0.9688\n",
      "Epoch [81/100], Step [700/760], Loss: 0.0589, Accuracy: 0.9844\n",
      "Epoch [81/100], Train Loss: 0.0378, Val Loss: 0.0234, Train Acc: 0.9860, Val Acc: 0.9921\n",
      "==================================================\n",
      "Epoch [82/100], Step [0/760], Loss: 0.0290, Accuracy: 1.0000\n",
      "Epoch [82/100], Step [100/760], Loss: 0.0176, Accuracy: 0.9844\n",
      "Epoch [82/100], Step [200/760], Loss: 0.0158, Accuracy: 0.9844\n",
      "Epoch [82/100], Step [300/760], Loss: 0.1051, Accuracy: 0.9688\n",
      "Epoch [82/100], Step [400/760], Loss: 0.0435, Accuracy: 0.9844\n",
      "Epoch [82/100], Step [500/760], Loss: 0.0235, Accuracy: 0.9844\n",
      "Epoch [82/100], Step [600/760], Loss: 0.0095, Accuracy: 1.0000\n",
      "Epoch [82/100], Step [700/760], Loss: 0.0090, Accuracy: 1.0000\n",
      "Epoch [82/100], Train Loss: 0.0304, Val Loss: 0.0170, Train Acc: 0.9889, Val Acc: 0.9945\n",
      "==================================================\n",
      "Epoch [83/100], Step [0/760], Loss: 0.0218, Accuracy: 0.9844\n",
      "Epoch [83/100], Step [100/760], Loss: 0.0387, Accuracy: 0.9844\n",
      "Epoch [83/100], Step [200/760], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [83/100], Step [300/760], Loss: 0.0095, Accuracy: 1.0000\n",
      "Epoch [83/100], Step [400/760], Loss: 0.0137, Accuracy: 1.0000\n",
      "Epoch [83/100], Step [500/760], Loss: 0.0080, Accuracy: 1.0000\n",
      "Epoch [83/100], Step [600/760], Loss: 0.0067, Accuracy: 1.0000\n",
      "Epoch [83/100], Step [700/760], Loss: 0.0198, Accuracy: 1.0000\n",
      "Epoch [83/100], Train Loss: 0.0294, Val Loss: 0.0184, Train Acc: 0.9898, Val Acc: 0.9937\n",
      "==================================================\n",
      "Epoch [84/100], Step [0/760], Loss: 0.0228, Accuracy: 0.9844\n",
      "Epoch [84/100], Step [100/760], Loss: 0.0040, Accuracy: 1.0000\n",
      "Epoch [84/100], Step [200/760], Loss: 0.0243, Accuracy: 1.0000\n",
      "Epoch [84/100], Step [300/760], Loss: 0.0391, Accuracy: 0.9844\n",
      "Epoch [84/100], Step [400/760], Loss: 0.0077, Accuracy: 1.0000\n",
      "Epoch [84/100], Step [500/760], Loss: 0.0233, Accuracy: 0.9844\n",
      "Epoch [84/100], Step [600/760], Loss: 0.0440, Accuracy: 0.9688\n",
      "Epoch [84/100], Step [700/760], Loss: 0.0689, Accuracy: 0.9844\n",
      "Epoch [84/100], Train Loss: 0.0263, Val Loss: 0.0162, Train Acc: 0.9907, Val Acc: 0.9944\n",
      "==================================================\n",
      "Epoch [85/100], Step [0/760], Loss: 0.0152, Accuracy: 1.0000\n",
      "Epoch [85/100], Step [100/760], Loss: 0.0296, Accuracy: 0.9844\n",
      "Epoch [85/100], Step [200/760], Loss: 0.1388, Accuracy: 0.9844\n",
      "Epoch [85/100], Step [300/760], Loss: 0.0180, Accuracy: 0.9844\n",
      "Epoch [85/100], Step [400/760], Loss: 0.0112, Accuracy: 1.0000\n",
      "Epoch [85/100], Step [500/760], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [85/100], Step [600/760], Loss: 0.0126, Accuracy: 1.0000\n",
      "Epoch [85/100], Step [700/760], Loss: 0.0168, Accuracy: 0.9844\n",
      "Epoch [85/100], Train Loss: 0.0296, Val Loss: 0.0175, Train Acc: 0.9896, Val Acc: 0.9944\n",
      "==================================================\n",
      "Epoch [86/100], Step [0/760], Loss: 0.0082, Accuracy: 1.0000\n",
      "Epoch [86/100], Step [100/760], Loss: 0.0143, Accuracy: 1.0000\n",
      "Epoch [86/100], Step [200/760], Loss: 0.0189, Accuracy: 1.0000\n",
      "Epoch [86/100], Step [300/760], Loss: 0.0076, Accuracy: 1.0000\n",
      "Epoch [86/100], Step [400/760], Loss: 0.0978, Accuracy: 0.9531\n",
      "Epoch [86/100], Step [500/760], Loss: 0.0401, Accuracy: 0.9844\n",
      "Epoch [86/100], Step [600/760], Loss: 0.0427, Accuracy: 0.9844\n",
      "Epoch [86/100], Step [700/760], Loss: 0.0085, Accuracy: 1.0000\n",
      "Epoch [86/100], Train Loss: 0.0270, Val Loss: 0.0166, Train Acc: 0.9906, Val Acc: 0.9942\n",
      "==================================================\n",
      "Epoch [87/100], Step [0/760], Loss: 0.0425, Accuracy: 0.9844\n",
      "Epoch [87/100], Step [100/760], Loss: 0.0053, Accuracy: 1.0000\n",
      "Epoch [87/100], Step [200/760], Loss: 0.0306, Accuracy: 0.9844\n",
      "Epoch [87/100], Step [300/760], Loss: 0.0666, Accuracy: 0.9531\n",
      "Epoch [87/100], Step [400/760], Loss: 0.0444, Accuracy: 0.9844\n",
      "Epoch [87/100], Step [500/760], Loss: 0.0059, Accuracy: 1.0000\n",
      "Epoch [87/100], Step [600/760], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [87/100], Step [700/760], Loss: 0.0281, Accuracy: 0.9844\n",
      "Epoch [87/100], Train Loss: 0.0275, Val Loss: 0.0165, Train Acc: 0.9902, Val Acc: 0.9944\n",
      "==================================================\n",
      "Epoch [88/100], Step [0/760], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [88/100], Step [100/760], Loss: 0.0074, Accuracy: 1.0000\n",
      "Epoch [88/100], Step [200/760], Loss: 0.0096, Accuracy: 1.0000\n",
      "Epoch [88/100], Step [300/760], Loss: 0.0294, Accuracy: 0.9844\n",
      "Epoch [88/100], Step [400/760], Loss: 0.0060, Accuracy: 1.0000\n",
      "Epoch [88/100], Step [500/760], Loss: 0.0659, Accuracy: 0.9844\n",
      "Epoch [88/100], Step [600/760], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [88/100], Step [700/760], Loss: 0.0430, Accuracy: 0.9844\n",
      "Epoch [88/100], Train Loss: 0.0267, Val Loss: 0.0150, Train Acc: 0.9902, Val Acc: 0.9949\n",
      "==================================================\n",
      "Epoch [89/100], Step [0/760], Loss: 0.1352, Accuracy: 0.9688\n",
      "Epoch [89/100], Step [100/760], Loss: 0.0884, Accuracy: 0.9375\n",
      "Epoch [89/100], Step [200/760], Loss: 0.0067, Accuracy: 1.0000\n",
      "Epoch [89/100], Step [300/760], Loss: 0.0133, Accuracy: 1.0000\n",
      "Epoch [89/100], Step [400/760], Loss: 0.0047, Accuracy: 1.0000\n",
      "Epoch [89/100], Step [500/760], Loss: 0.0220, Accuracy: 0.9844\n",
      "Epoch [89/100], Step [600/760], Loss: 0.0523, Accuracy: 0.9844\n",
      "Epoch [89/100], Step [700/760], Loss: 0.1214, Accuracy: 0.9688\n",
      "Epoch [89/100], Train Loss: 0.0275, Val Loss: 0.0155, Train Acc: 0.9905, Val Acc: 0.9945\n",
      "==================================================\n",
      "Epoch [90/100], Step [0/760], Loss: 0.0336, Accuracy: 0.9844\n",
      "Epoch [90/100], Step [100/760], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [90/100], Step [200/760], Loss: 0.0493, Accuracy: 0.9688\n",
      "Epoch [90/100], Step [300/760], Loss: 0.0032, Accuracy: 1.0000\n",
      "Epoch [90/100], Step [400/760], Loss: 0.0257, Accuracy: 0.9844\n",
      "Epoch [90/100], Step [500/760], Loss: 0.0268, Accuracy: 0.9844\n",
      "Epoch [90/100], Step [600/760], Loss: 0.0358, Accuracy: 0.9844\n",
      "Epoch [90/100], Step [700/760], Loss: 0.0390, Accuracy: 0.9844\n",
      "Epoch [90/100], Train Loss: 0.0269, Val Loss: 0.0146, Train Acc: 0.9898, Val Acc: 0.9948\n",
      "==================================================\n",
      "Epoch [91/100], Step [0/760], Loss: 0.0110, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [100/760], Loss: 0.0273, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [200/760], Loss: 0.0217, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [300/760], Loss: 0.0064, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [400/760], Loss: 0.0083, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [500/760], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [600/760], Loss: 0.0096, Accuracy: 1.0000\n",
      "Epoch [91/100], Step [700/760], Loss: 0.0570, Accuracy: 0.9844\n",
      "Epoch [91/100], Train Loss: 0.0259, Val Loss: 0.0166, Train Acc: 0.9906, Val Acc: 0.9942\n",
      "==================================================\n",
      "Epoch [92/100], Step [0/760], Loss: 0.0202, Accuracy: 1.0000\n",
      "Epoch [92/100], Step [100/760], Loss: 0.0830, Accuracy: 0.9844\n",
      "Epoch [92/100], Step [200/760], Loss: 0.0312, Accuracy: 0.9844\n",
      "Epoch [92/100], Step [300/760], Loss: 0.0339, Accuracy: 0.9844\n",
      "Epoch [92/100], Step [400/760], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [92/100], Step [500/760], Loss: 0.0240, Accuracy: 0.9844\n",
      "Epoch [92/100], Step [600/760], Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch [92/100], Step [700/760], Loss: 0.0197, Accuracy: 1.0000\n",
      "Epoch [92/100], Train Loss: 0.0257, Val Loss: 0.0149, Train Acc: 0.9909, Val Acc: 0.9949\n",
      "==================================================\n",
      "Epoch [93/100], Step [0/760], Loss: 0.0137, Accuracy: 0.9844\n",
      "Epoch [93/100], Step [100/760], Loss: 0.0222, Accuracy: 0.9844\n",
      "Epoch [93/100], Step [200/760], Loss: 0.0050, Accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:18\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)\u001b[0m\n\u001b[1;32m     42\u001b[0m trainAcc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainLoader):\n\u001b[1;32m     46\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     82\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:46\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_inputs(flat_inputs)\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[0;32m---> 46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_transform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_transform_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mneeds_transform\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_geometry.py:279\u001b[0m, in \u001b[0;36mRandomResizedCrop._get_params\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    277\u001b[0m     target_area \u001b[38;5;241m=\u001b[39m area \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    278\u001b[0m     aspect_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\n\u001b[0;32m--> 279\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_ratio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_ratio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    285\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(target_area \u001b[38;5;241m*\u001b[39m aspect_ratio)))\n\u001b[1;32m    286\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(target_area \u001b[38;5;241m/\u001b[39m aspect_ratio)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# model = SimpleCnn().to(device)\n",
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=10,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 4.16 s, total: 2min 9s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred, actual = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "print(f\"Test Accuracy: {acc.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
