{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/micromamba/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 16200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 5400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label', 'filename'],\n",
      "        num_rows: 5400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "dataName = \"blanchon/EuroSAT_RGB\"\n",
    "dataPath = Path(dataName)\n",
    "if dataPath.exists() is False:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds: DatasetDict = load_dataset(dataName)  # type: ignore\n",
    "    ds.save_to_disk(dataPath)  # type: ignore\n",
    "\n",
    "else:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    ds: DatasetDict = load_from_disk(dataPath)  # type: ignore\n",
    "\n",
    "num_classes = len(ds[\"train\"].unique(\"label\"))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCmA2DxVPUCfJAx/FWd58o/5aN+dPWVpOHYt9aOWxu5JqxC1IKsFBjpTNvOABn6UzHkY2nscAfSpoUT70i5UdccUyXYW3KMKQMCgq1iMGjtShRnFK+FUDFAkyLjHTmpIRwajNTwgbfxqmUtx3f2prDB4qXAFJgZqSiPbgFjyx4okwCFIPQVI33aay5cZ6AUImS0IwBk4okHAqZVRs561FcAKQB6UCsaer2sUNvA6RKjMzdO4GKy1cIMAD8q1dbbKWw9m4/GscmlD4dTSppLQlEuTyBUgG4cGqwqxGfl/GqaITJPK96ZcEKw4qYNkVBcDLYzSRT2I/NVT939abJKkhB5GKjYZPWmYqrEG3rvE0Kbs4U/zrHOa0tVnW5uVdeAFxis5jzUw+FF1NZMBU6cL+NQgZ71KDhRVMhE6c/U1BOf3hqWFsSLkVHMnzn1qepXQrN1pKlKHNN2VRJ//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][16199][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDy4dDUO4k8dPWpBwtNAANbnKGM96AeeuaVlIAzxmkGAKYBjuetB7lj2oJHHP4UrKQB6HvQAwU7j1H0puQKWgB5Hy0wdae33aaKQAe5oJwByORQcnvxSHg4pgHekOcjmjrSUALz7UtJSigBznJxTaXFJigBPw4oNKSd3tRkUDEpQKSl7UCExThxk02nqM0AIelJ2pzelMoGL3pPYUE4FA/WgAo60tOVM0gGgc1IOKUADimsewoEf//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"test\"][900][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size = (64, 64)\n",
      "Mode = RGB\n",
      "Format = PNG\n"
     ]
    }
   ],
   "source": [
    "img = ds[\"train\"][0][\"image\"]\n",
    "\n",
    "print(f\"Size = {img.size}\")\n",
    "print(f\"Mode = {img.mode}\")\n",
    "print(f\"Format = {img.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Prepare data for training and evaluating.\n",
    "\n",
    "Also augment the train data by introduce random effects like cropting, rotating, flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as ImgF\n",
    "\n",
    "\n",
    "# ds[\"train\"][0][\"image\"] is a PIL.JpegImagePlugin.JpegImageFile\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset of images and its labels. Return image and label. If transform is provided, apply it to the image.\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    IMG_SIZE = 64, 64\n",
    "\n",
    "    def _transformBuild(self):\n",
    "        transformations = []\n",
    "        transformations.append(v2.Resize(size=ImageDataset.IMG_SIZE))\n",
    "        if self.randRot:\n",
    "            transformations.append(v2.RandomRotation(15))  # type: ignore\n",
    "        if self.randCrop:\n",
    "            transformations.append(\n",
    "                v2.RandomResizedCrop(size=ImageDataset.IMG_SIZE, scale=(0.8, 1.0))\n",
    "            )\n",
    "        if self.randFlipH:\n",
    "            transformations.append(v2.RandomHorizontalFlip())\n",
    "        if self.randFlipV:\n",
    "            transformations.append(v2.RandomVerticalFlip())\n",
    "\n",
    "        transformations.append(v2.ToImage())\n",
    "        transformations.append(v2.ToDtype(torch.float32, True))\n",
    "\n",
    "        return v2.Compose(transformations)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        randCrop=False,\n",
    "        randRot=False,\n",
    "        randFlipH=False,\n",
    "        randFlipV=False,\n",
    "    ):\n",
    "        self.randCrop = randCrop\n",
    "        self.randRot = randRot\n",
    "        self.randFlipH = randFlipH\n",
    "        self.randFlipV = randFlipV\n",
    "\n",
    "        self.length = len(data)\n",
    "        self.imgSize = self.IMG_SIZE\n",
    "\n",
    "        self.images = [d[\"image\"] for d in data]\n",
    "        self.labels = [d[\"label\"] for d in data]\n",
    "\n",
    "        # build the transformer\n",
    "        self.imgTransformer = self._transformBuild()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx\n",
    "\n",
    "        return self.imgTransformer(self.images[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# test if augmentations are overdoing it\n",
    "trainSet = ImageDataset(ds[\"train\"], True, True, True, True)\n",
    "valSet = ImageDataset(ds[\"validation\"])\n",
    "testSet = ImageDataset(ds[\"test\"])\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSize = trainSet[0][0].size()\n",
    "sampleSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate if the image is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = trainSet[0][0].permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = ds[\"train\"][0][\"image\"]\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, device, dataLoader):\n",
    "    \"\"\"Infer the predictions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for inference.\n",
    "        device (torch.device): The device to use.\n",
    "        dataLoader (DataLoader): The data to infer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The predictions and the actual labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    predProba = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            actual.append(labels)\n",
    "            predProba.append(outputs)\n",
    "\n",
    "    return torch.cat(predProba), torch.cat(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    trainLoader,\n",
    "    valLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=100,\n",
    "    earlyStopping=10,\n",
    "):\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        device (torch.device): The device to use.\n",
    "        trainLoader (DataLoader): The training data.\n",
    "        valLoader (DataLoader): The validation data.\n",
    "        criterion (_type_): The loss function.\n",
    "        optimizer (_type_): The optimizer.\n",
    "        n_epochs (int, optional): Number of epochs to train for. Defaults to 100.\n",
    "        earlyStopping (int, optional): Number of epochs to wait before stopping training if no improvement is made\n",
    "            on the validation loss. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    optimScheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.2)\n",
    "\n",
    "    # Training loop\n",
    "    earlyStopping = 10\n",
    "    bestValLoss = float(\"inf\")\n",
    "    bestModelState = None\n",
    "    patience = 0\n",
    "\n",
    "    trainLosses = []\n",
    "    valLosses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        trainLoss = 0\n",
    "        trainAcc = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainLoader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            trainLoss += loss.item()\n",
    "            trainAcc += acc.item()\n",
    "\n",
    "            # Print every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{n_epochs}], Step [{batch_idx}/{len(trainLoader)}], Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        trainLoss /= len(trainLoader)\n",
    "        trainAcc /= len(trainLoader)\n",
    "        trainLosses.append(trainLoss)\n",
    "\n",
    "        # Validation loss\n",
    "        pred, actual = infer(model, device, valLoader)\n",
    "        valLoss = criterion(pred, actual)\n",
    "        valAcc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "\n",
    "        valLosses.append(valLoss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{n_epochs}], Train Loss: {trainLoss:.4f}, Val Loss: {valLoss:.4f}, Train Acc: {trainAcc:.4f}, Val Acc: {valAcc:.4f}\"\n",
    "        )\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Early stopping\n",
    "        if valLoss < bestValLoss:\n",
    "            bestValLoss = valLoss\n",
    "            bestModelState = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > earlyStopping:\n",
    "                break\n",
    "\n",
    "        optimScheduler.step(valLoss)\n",
    "\n",
    "    # Save the model\n",
    "    # torch.save(bestModelState, \"model.pth\")\n",
    "    model.load_state_dict(bestModelState)  # type: ignore\n",
    "    \n",
    "    return trainLosses, valLosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2d, BatchNorm2d and activation function.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, in_chn, out_chn, activation, kernel_size=3, alpha=1, stride=1, group=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2  # for same padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chn,\n",
    "            out_chn,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "            groups=group,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_chn)\n",
    "        self.activate = activation() if activation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activate is not None:\n",
    "            x = self.activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeAndExcite(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_chn, reduction=4):\n",
    "        super().__init__()\n",
    "        reduction_chn = in_chn // reduction\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(\n",
    "                1\n",
    "            ),  # 1,16,x,y->1,1,16,1 each channel is reduce to one value\n",
    "            nn.Flatten(),  #  1,16\n",
    "            nn.Linear(in_chn, reduction_chn, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduction_chn, in_chn, bias=False),\n",
    "            nn.Hardsigmoid(),\n",
    "            nn.Unflatten(1, (in_chn, 1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        out_chn,\n",
    "        expansion_channel,\n",
    "        activation,\n",
    "        se_reduction=4,\n",
    "        se_flag=False,\n",
    "        kernel=3,\n",
    "        stride=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #         expansion_channel=int(in_chn*expansiton_factor)\n",
    "        self.use_residual = (stride == 1) and (in_chn == out_chn)\n",
    "\n",
    "        self.residual_block_layers: list[nn.Module] = [\n",
    "            ConvBlock(\n",
    "                in_chn, expansion_channel, activation=activation, kernel_size=1\n",
    "            ),  # expansion_part\n",
    "            ConvBlock(\n",
    "                expansion_channel,\n",
    "                expansion_channel,\n",
    "                activation=activation,\n",
    "                group=expansion_channel,\n",
    "                stride=stride,\n",
    "                kernel_size=kernel,\n",
    "            ),  # depthwise conv\n",
    "        ]\n",
    "\n",
    "        if se_flag is True:\n",
    "            self.residual_block_layers.extend(\n",
    "                [SqueezeAndExcite(expansion_channel, reduction=se_reduction)]\n",
    "            )\n",
    "\n",
    "        self.residual_block_layers.extend(\n",
    "            [ConvBlock(expansion_channel, out_chn, activation=None, kernel_size=1)]\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*self.residual_block_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + x if self.use_residual else self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Hardswish, ReLU\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"large\":\n",
    "            layers_config = [\n",
    "                # kernel, exp, out, SE, NL, stride\n",
    "                [3, 16, 16, False, ReLU, 1],\n",
    "                [3, 64, 24, False, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 1],\n",
    "                [5, 72, 40, True, ReLU, 2],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [5, 120, 40, True, ReLU, 1],\n",
    "                [3, 240, 80, False, Hardswish, 2],\n",
    "                [3, 200, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 184, 80, False, Hardswish, 1],\n",
    "                [3, 480, 112, True, Hardswish, 1],\n",
    "                [3, 672, 112, True, Hardswish, 1],\n",
    "                [5, 672, 160, True, Hardswish, 2],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "                [5, 960, 160, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 960, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, Hardswish, 2],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 240, 40, True, Hardswish, 1],\n",
    "                [5, 120, 48, True, Hardswish, 1],\n",
    "                [5, 144, 48, True, Hardswish, 1],\n",
    "                [5, 288, 96, True, Hardswish, 2],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "                [5, 576, 96, True, Hardswish, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "        \n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device = \", device)\n",
    "n_epochs = 100\n",
    "earlyStopping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting \n",
    "\n",
    "Due to high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Step [0/254], Loss: 2.2964, Accuracy: 0.0625\n",
      "Epoch [1/100], Step [100/254], Loss: 2.2347, Accuracy: 0.2344\n",
      "Epoch [1/100], Step [200/254], Loss: 1.5120, Accuracy: 0.3594\n",
      "Epoch [1/100], Train Loss: 3.1375, Val Loss: 5.3135, Train Acc: 0.2870, Val Acc: 0.2299\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 1.9193, Accuracy: 0.2656\n",
      "Epoch [2/100], Step [100/254], Loss: 1.7059, Accuracy: 0.3750\n",
      "Epoch [2/100], Step [200/254], Loss: 1.6379, Accuracy: 0.4375\n",
      "Epoch [2/100], Train Loss: 1.5862, Val Loss: 6.3246, Train Acc: 0.4315, Val Acc: 0.2241\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 1.5092, Accuracy: 0.4062\n",
      "Epoch [3/100], Step [100/254], Loss: 2.0220, Accuracy: 0.2344\n",
      "Epoch [3/100], Step [200/254], Loss: 2.2077, Accuracy: 0.1875\n",
      "Epoch [3/100], Train Loss: 1.9332, Val Loss: 6.3059, Train Acc: 0.2825, Val Acc: 0.1114\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 2.2451, Accuracy: 0.2031\n",
      "Epoch [4/100], Step [100/254], Loss: 1.9650, Accuracy: 0.2188\n",
      "Epoch [4/100], Step [200/254], Loss: 2.0492, Accuracy: 0.2656\n",
      "Epoch [4/100], Train Loss: 1.9982, Val Loss: 502.1431, Train Acc: 0.2618, Val Acc: 0.0736\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 1.9130, Accuracy: 0.2500\n",
      "Epoch [5/100], Step [100/254], Loss: 2.0290, Accuracy: 0.2812\n",
      "Epoch [5/100], Step [200/254], Loss: 2.2276, Accuracy: 0.2500\n",
      "Epoch [5/100], Train Loss: 2.1081, Val Loss: 15537.9961, Train Acc: 0.2355, Val Acc: 0.1128\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/254], Loss: 2.0719, Accuracy: 0.1250\n",
      "Epoch [6/100], Step [100/254], Loss: 1.8519, Accuracy: 0.2344\n",
      "Epoch [6/100], Step [200/254], Loss: 1.7165, Accuracy: 0.3438\n",
      "Epoch [6/100], Train Loss: 1.8603, Val Loss: 1.9494, Train Acc: 0.2665, Val Acc: 0.2349\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/254], Loss: 1.8001, Accuracy: 0.2500\n",
      "Epoch [7/100], Step [100/254], Loss: 1.4698, Accuracy: 0.4688\n",
      "Epoch [7/100], Step [200/254], Loss: 1.8581, Accuracy: 0.2656\n",
      "Epoch [7/100], Train Loss: 1.7249, Val Loss: 1.6978, Train Acc: 0.3296, Val Acc: 0.3272\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/254], Loss: 1.6881, Accuracy: 0.3750\n",
      "Epoch [8/100], Step [100/254], Loss: 1.6347, Accuracy: 0.4375\n",
      "Epoch [8/100], Step [200/254], Loss: 1.6998, Accuracy: 0.2969\n",
      "Epoch [8/100], Train Loss: 1.6276, Val Loss: 1.7702, Train Acc: 0.3773, Val Acc: 0.3278\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/254], Loss: 1.7060, Accuracy: 0.3281\n",
      "Epoch [9/100], Step [100/254], Loss: 1.5427, Accuracy: 0.4375\n",
      "Epoch [9/100], Step [200/254], Loss: 1.4880, Accuracy: 0.4844\n",
      "Epoch [9/100], Train Loss: 1.5495, Val Loss: 1.5776, Train Acc: 0.4114, Val Acc: 0.4165\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/254], Loss: 1.4702, Accuracy: 0.3750\n",
      "Epoch [10/100], Step [100/254], Loss: 1.3763, Accuracy: 0.4688\n",
      "Epoch [10/100], Step [200/254], Loss: 1.6309, Accuracy: 0.4844\n",
      "Epoch [10/100], Train Loss: 1.4905, Val Loss: 1.4491, Train Acc: 0.4424, Val Acc: 0.4684\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/254], Loss: 1.2144, Accuracy: 0.5312\n",
      "Epoch [11/100], Step [100/254], Loss: 1.2816, Accuracy: 0.5312\n",
      "Epoch [11/100], Step [200/254], Loss: 1.4061, Accuracy: 0.4375\n",
      "Epoch [11/100], Train Loss: 1.3719, Val Loss: 1.4623, Train Acc: 0.5007, Val Acc: 0.4951\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/254], Loss: 1.2397, Accuracy: 0.5781\n",
      "Epoch [12/100], Step [100/254], Loss: 1.1928, Accuracy: 0.5625\n",
      "Epoch [12/100], Step [200/254], Loss: 1.3081, Accuracy: 0.5781\n",
      "Epoch [12/100], Train Loss: 1.2833, Val Loss: 2.0320, Train Acc: 0.5400, Val Acc: 0.4426\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/254], Loss: 1.1432, Accuracy: 0.6719\n",
      "Epoch [13/100], Step [100/254], Loss: 1.0868, Accuracy: 0.6562\n",
      "Epoch [13/100], Step [200/254], Loss: 1.1088, Accuracy: 0.6094\n",
      "Epoch [13/100], Train Loss: 1.2184, Val Loss: 1.1540, Train Acc: 0.5601, Val Acc: 0.5869\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/254], Loss: 1.3896, Accuracy: 0.4688\n",
      "Epoch [14/100], Step [100/254], Loss: 1.4014, Accuracy: 0.5156\n",
      "Epoch [14/100], Step [200/254], Loss: 1.3534, Accuracy: 0.5000\n",
      "Epoch [14/100], Train Loss: 1.3296, Val Loss: 1.2103, Train Acc: 0.5183, Val Acc: 0.5525\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/254], Loss: 1.4055, Accuracy: 0.4844\n",
      "Epoch [15/100], Step [100/254], Loss: 1.5608, Accuracy: 0.4688\n",
      "Epoch [15/100], Step [200/254], Loss: 1.0293, Accuracy: 0.5938\n",
      "Epoch [15/100], Train Loss: 1.2255, Val Loss: 1.3248, Train Acc: 0.5436, Val Acc: 0.4972\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/254], Loss: 0.9056, Accuracy: 0.7031\n",
      "Epoch [16/100], Step [100/254], Loss: 0.9688, Accuracy: 0.7031\n",
      "Epoch [16/100], Step [200/254], Loss: 0.8817, Accuracy: 0.6562\n",
      "Epoch [16/100], Train Loss: 1.1610, Val Loss: 1.1145, Train Acc: 0.5720, Val Acc: 0.5940\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/254], Loss: 1.3216, Accuracy: 0.5469\n",
      "Epoch [17/100], Step [100/254], Loss: 1.2254, Accuracy: 0.5469\n",
      "Epoch [17/100], Step [200/254], Loss: 1.0461, Accuracy: 0.5156\n",
      "Epoch [17/100], Train Loss: 1.1335, Val Loss: 1.0535, Train Acc: 0.5834, Val Acc: 0.6143\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/254], Loss: 1.1052, Accuracy: 0.5625\n",
      "Epoch [18/100], Step [100/254], Loss: 1.1130, Accuracy: 0.5781\n",
      "Epoch [18/100], Step [200/254], Loss: 1.1820, Accuracy: 0.6562\n",
      "Epoch [18/100], Train Loss: 1.0547, Val Loss: 1.2970, Train Acc: 0.6129, Val Acc: 0.5396\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/254], Loss: 1.0087, Accuracy: 0.5781\n",
      "Epoch [19/100], Step [100/254], Loss: 1.2005, Accuracy: 0.5781\n",
      "Epoch [19/100], Step [200/254], Loss: 1.0576, Accuracy: 0.5625\n",
      "Epoch [19/100], Train Loss: 1.0389, Val Loss: 0.9626, Train Acc: 0.6228, Val Acc: 0.6462\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/254], Loss: 1.0616, Accuracy: 0.6094\n",
      "Epoch [20/100], Step [100/254], Loss: 1.0359, Accuracy: 0.6875\n",
      "Epoch [20/100], Step [200/254], Loss: 1.4339, Accuracy: 0.5156\n",
      "Epoch [20/100], Train Loss: 1.2841, Val Loss: 2.8692, Train Acc: 0.5431, Val Acc: 0.3739\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/254], Loss: 1.2954, Accuracy: 0.5312\n",
      "Epoch [21/100], Step [100/254], Loss: 1.3177, Accuracy: 0.5469\n",
      "Epoch [21/100], Step [200/254], Loss: 1.0123, Accuracy: 0.6719\n",
      "Epoch [21/100], Train Loss: 1.1770, Val Loss: 1.1851, Train Acc: 0.5746, Val Acc: 0.5691\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/254], Loss: 1.3064, Accuracy: 0.5000\n",
      "Epoch [22/100], Step [100/254], Loss: 1.1828, Accuracy: 0.5625\n",
      "Epoch [22/100], Step [200/254], Loss: 1.0573, Accuracy: 0.7188\n",
      "Epoch [22/100], Train Loss: 1.0451, Val Loss: 1.7266, Train Acc: 0.6212, Val Acc: 0.4262\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/254], Loss: 1.5580, Accuracy: 0.3438\n",
      "Epoch [23/100], Step [100/254], Loss: 1.5720, Accuracy: 0.4375\n",
      "Epoch [23/100], Step [200/254], Loss: 1.5045, Accuracy: 0.4844\n",
      "Epoch [23/100], Train Loss: 1.5081, Val Loss: 1.3816, Train Acc: 0.4627, Val Acc: 0.4939\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/254], Loss: 1.1507, Accuracy: 0.5781\n",
      "Epoch [24/100], Step [100/254], Loss: 1.1354, Accuracy: 0.5469\n",
      "Epoch [24/100], Step [200/254], Loss: 1.1539, Accuracy: 0.5469\n",
      "Epoch [24/100], Train Loss: 1.1614, Val Loss: 1.0922, Train Acc: 0.5784, Val Acc: 0.5996\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/254], Loss: 0.8753, Accuracy: 0.6875\n",
      "Epoch [25/100], Step [100/254], Loss: 1.2717, Accuracy: 0.5781\n",
      "Epoch [25/100], Step [200/254], Loss: 1.0574, Accuracy: 0.5625\n",
      "Epoch [25/100], Train Loss: 1.0867, Val Loss: 1.0710, Train Acc: 0.6083, Val Acc: 0.6136\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/254], Loss: 1.1077, Accuracy: 0.5156\n",
      "Epoch [26/100], Step [100/254], Loss: 1.0211, Accuracy: 0.6562\n",
      "Epoch [26/100], Step [200/254], Loss: 0.9961, Accuracy: 0.6250\n",
      "Epoch [26/100], Train Loss: 1.0497, Val Loss: 0.9940, Train Acc: 0.6243, Val Acc: 0.6404\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/254], Loss: 0.9495, Accuracy: 0.6250\n",
      "Epoch [27/100], Step [100/254], Loss: 0.9530, Accuracy: 0.6250\n",
      "Epoch [27/100], Step [200/254], Loss: 1.2842, Accuracy: 0.6094\n",
      "Epoch [27/100], Train Loss: 1.0149, Val Loss: 0.9109, Train Acc: 0.6369, Val Acc: 0.6731\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/254], Loss: 0.8458, Accuracy: 0.6719\n",
      "Epoch [28/100], Step [100/254], Loss: 0.9705, Accuracy: 0.6406\n",
      "Epoch [28/100], Step [200/254], Loss: 0.8621, Accuracy: 0.6406\n",
      "Epoch [28/100], Train Loss: 0.9804, Val Loss: 0.9503, Train Acc: 0.6436, Val Acc: 0.6564\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/254], Loss: 0.9274, Accuracy: 0.6719\n",
      "Epoch [29/100], Step [100/254], Loss: 1.0946, Accuracy: 0.5938\n",
      "Epoch [29/100], Step [200/254], Loss: 0.8415, Accuracy: 0.7031\n",
      "Epoch [29/100], Train Loss: 0.9610, Val Loss: 0.8767, Train Acc: 0.6524, Val Acc: 0.6737\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/254], Loss: 1.0066, Accuracy: 0.5781\n",
      "Epoch [30/100], Step [100/254], Loss: 0.9154, Accuracy: 0.5469\n",
      "Epoch [30/100], Step [200/254], Loss: 0.7293, Accuracy: 0.7812\n",
      "Epoch [30/100], Train Loss: 0.9371, Val Loss: 0.8742, Train Acc: 0.6590, Val Acc: 0.6660\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/254], Loss: 0.8734, Accuracy: 0.6719\n",
      "Epoch [31/100], Step [100/254], Loss: 0.8422, Accuracy: 0.6875\n",
      "Epoch [31/100], Step [200/254], Loss: 0.8888, Accuracy: 0.6562\n",
      "Epoch [31/100], Train Loss: 0.9041, Val Loss: 0.8092, Train Acc: 0.6691, Val Acc: 0.7043\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/254], Loss: 0.8246, Accuracy: 0.6406\n",
      "Epoch [32/100], Step [100/254], Loss: 0.9373, Accuracy: 0.7500\n",
      "Epoch [32/100], Step [200/254], Loss: 1.1450, Accuracy: 0.6406\n",
      "Epoch [32/100], Train Loss: 0.8998, Val Loss: 0.8174, Train Acc: 0.6744, Val Acc: 0.6985\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/254], Loss: 0.7045, Accuracy: 0.7344\n",
      "Epoch [33/100], Step [100/254], Loss: 1.0931, Accuracy: 0.5781\n",
      "Epoch [33/100], Step [200/254], Loss: 0.8505, Accuracy: 0.7344\n",
      "Epoch [33/100], Train Loss: 0.8775, Val Loss: 0.8037, Train Acc: 0.6778, Val Acc: 0.7077\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/254], Loss: 0.8299, Accuracy: 0.6562\n",
      "Epoch [34/100], Step [100/254], Loss: 0.8996, Accuracy: 0.6719\n",
      "Epoch [34/100], Step [200/254], Loss: 0.6517, Accuracy: 0.7969\n",
      "Epoch [34/100], Train Loss: 0.8492, Val Loss: 0.7581, Train Acc: 0.6915, Val Acc: 0.7236\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/254], Loss: 0.6645, Accuracy: 0.7500\n",
      "Epoch [35/100], Step [100/254], Loss: 0.9349, Accuracy: 0.6719\n",
      "Epoch [35/100], Step [200/254], Loss: 0.8704, Accuracy: 0.6406\n",
      "Epoch [35/100], Train Loss: 0.8486, Val Loss: 0.9472, Train Acc: 0.6919, Val Acc: 0.6637\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/254], Loss: 0.6446, Accuracy: 0.7031\n",
      "Epoch [36/100], Step [100/254], Loss: 0.8999, Accuracy: 0.6719\n",
      "Epoch [36/100], Step [200/254], Loss: 0.7777, Accuracy: 0.6875\n",
      "Epoch [36/100], Train Loss: 0.8419, Val Loss: 0.7901, Train Acc: 0.6878, Val Acc: 0.7053\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/254], Loss: 0.7252, Accuracy: 0.7812\n",
      "Epoch [37/100], Step [100/254], Loss: 0.7870, Accuracy: 0.7188\n",
      "Epoch [37/100], Step [200/254], Loss: 0.8102, Accuracy: 0.6250\n",
      "Epoch [37/100], Train Loss: 0.8119, Val Loss: 0.8275, Train Acc: 0.7026, Val Acc: 0.6974\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/254], Loss: 0.8014, Accuracy: 0.6406\n",
      "Epoch [38/100], Step [100/254], Loss: 0.8238, Accuracy: 0.6719\n",
      "Epoch [38/100], Step [200/254], Loss: 0.8511, Accuracy: 0.5938\n",
      "Epoch [38/100], Train Loss: 0.8255, Val Loss: 0.7474, Train Acc: 0.6969, Val Acc: 0.7254\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/254], Loss: 0.8524, Accuracy: 0.7031\n",
      "Epoch [39/100], Step [100/254], Loss: 0.9542, Accuracy: 0.6562\n",
      "Epoch [39/100], Step [200/254], Loss: 0.8498, Accuracy: 0.6719\n",
      "Epoch [39/100], Train Loss: 0.8077, Val Loss: 0.7117, Train Acc: 0.7080, Val Acc: 0.7349\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/254], Loss: 0.8298, Accuracy: 0.7344\n",
      "Epoch [40/100], Step [100/254], Loss: 0.5935, Accuracy: 0.7969\n",
      "Epoch [40/100], Step [200/254], Loss: 0.8127, Accuracy: 0.7188\n",
      "Epoch [40/100], Train Loss: 0.7767, Val Loss: 0.7016, Train Acc: 0.7140, Val Acc: 0.7440\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/254], Loss: 0.6014, Accuracy: 0.7656\n",
      "Epoch [41/100], Step [100/254], Loss: 0.8665, Accuracy: 0.7031\n",
      "Epoch [41/100], Step [200/254], Loss: 0.8649, Accuracy: 0.5938\n",
      "Epoch [41/100], Train Loss: 0.7699, Val Loss: 0.7923, Train Acc: 0.7226, Val Acc: 0.7126\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/254], Loss: 0.9304, Accuracy: 0.6875\n",
      "Epoch [42/100], Step [100/254], Loss: 0.8818, Accuracy: 0.6875\n",
      "Epoch [42/100], Step [200/254], Loss: 0.6345, Accuracy: 0.7500\n",
      "Epoch [42/100], Train Loss: 0.7671, Val Loss: 0.6970, Train Acc: 0.7237, Val Acc: 0.7473\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/254], Loss: 0.6230, Accuracy: 0.7812\n",
      "Epoch [43/100], Step [100/254], Loss: 0.5898, Accuracy: 0.8438\n",
      "Epoch [43/100], Step [200/254], Loss: 0.7254, Accuracy: 0.7812\n",
      "Epoch [43/100], Train Loss: 0.7528, Val Loss: 0.6845, Train Acc: 0.7260, Val Acc: 0.7539\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/254], Loss: 0.7878, Accuracy: 0.7969\n",
      "Epoch [44/100], Step [100/254], Loss: 0.8111, Accuracy: 0.7031\n",
      "Epoch [44/100], Step [200/254], Loss: 0.6203, Accuracy: 0.6875\n",
      "Epoch [44/100], Train Loss: 0.7392, Val Loss: 0.6839, Train Acc: 0.7330, Val Acc: 0.7546\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/254], Loss: 0.6225, Accuracy: 0.7969\n",
      "Epoch [45/100], Step [100/254], Loss: 0.6524, Accuracy: 0.7031\n",
      "Epoch [45/100], Step [200/254], Loss: 0.6736, Accuracy: 0.7188\n",
      "Epoch [45/100], Train Loss: 0.7318, Val Loss: 0.7473, Train Acc: 0.7343, Val Acc: 0.7312\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/254], Loss: 1.1489, Accuracy: 0.6250\n",
      "Epoch [46/100], Step [100/254], Loss: 0.4570, Accuracy: 0.8281\n",
      "Epoch [46/100], Step [200/254], Loss: 0.7800, Accuracy: 0.6719\n",
      "Epoch [46/100], Train Loss: 0.7144, Val Loss: 0.6730, Train Acc: 0.7442, Val Acc: 0.7491\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/254], Loss: 0.6260, Accuracy: 0.8594\n",
      "Epoch [47/100], Step [100/254], Loss: 0.8904, Accuracy: 0.7344\n",
      "Epoch [47/100], Step [200/254], Loss: 0.5980, Accuracy: 0.8125\n",
      "Epoch [47/100], Train Loss: 0.6903, Val Loss: 0.6601, Train Acc: 0.7523, Val Acc: 0.7652\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/254], Loss: 0.7471, Accuracy: 0.7031\n",
      "Epoch [48/100], Step [100/254], Loss: 0.5863, Accuracy: 0.7812\n",
      "Epoch [48/100], Step [200/254], Loss: 0.7539, Accuracy: 0.7656\n",
      "Epoch [48/100], Train Loss: 0.6870, Val Loss: 1.2344, Train Acc: 0.7494, Val Acc: 0.5832\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/254], Loss: 0.6135, Accuracy: 0.8125\n",
      "Epoch [49/100], Step [100/254], Loss: 0.5869, Accuracy: 0.8281\n",
      "Epoch [49/100], Step [200/254], Loss: 0.7278, Accuracy: 0.6875\n",
      "Epoch [49/100], Train Loss: 0.6874, Val Loss: 0.6532, Train Acc: 0.7516, Val Acc: 0.7668\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/254], Loss: 0.6382, Accuracy: 0.7656\n",
      "Epoch [50/100], Step [100/254], Loss: 0.4515, Accuracy: 0.8906\n",
      "Epoch [50/100], Step [200/254], Loss: 0.5886, Accuracy: 0.7812\n",
      "Epoch [50/100], Train Loss: 0.6547, Val Loss: 0.5797, Train Acc: 0.7622, Val Acc: 0.7930\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/254], Loss: 0.6373, Accuracy: 0.8125\n",
      "Epoch [51/100], Step [100/254], Loss: 0.8587, Accuracy: 0.7188\n",
      "Epoch [51/100], Step [200/254], Loss: 0.4586, Accuracy: 0.8438\n",
      "Epoch [51/100], Train Loss: 0.6512, Val Loss: 0.6164, Train Acc: 0.7676, Val Acc: 0.7736\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/254], Loss: 0.5867, Accuracy: 0.7656\n",
      "Epoch [52/100], Step [100/254], Loss: 0.8487, Accuracy: 0.7656\n",
      "Epoch [52/100], Step [200/254], Loss: 0.6234, Accuracy: 0.8438\n",
      "Epoch [52/100], Train Loss: 0.7213, Val Loss: 0.5927, Train Acc: 0.7452, Val Acc: 0.7867\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/254], Loss: 0.8648, Accuracy: 0.6875\n",
      "Epoch [53/100], Step [100/254], Loss: 0.8019, Accuracy: 0.6875\n",
      "Epoch [53/100], Step [200/254], Loss: 0.9335, Accuracy: 0.6250\n",
      "Epoch [53/100], Train Loss: 0.7963, Val Loss: 0.6458, Train Acc: 0.7183, Val Acc: 0.7694\n",
      "==================================================\n",
      "Epoch [54/100], Step [0/254], Loss: 0.7532, Accuracy: 0.7656\n",
      "Epoch [54/100], Step [100/254], Loss: 0.7864, Accuracy: 0.7031\n",
      "Epoch [54/100], Step [200/254], Loss: 0.7794, Accuracy: 0.6875\n",
      "Epoch [54/100], Train Loss: 0.6807, Val Loss: 0.6377, Train Acc: 0.7549, Val Acc: 0.7628\n",
      "==================================================\n",
      "Epoch [55/100], Step [0/254], Loss: 0.4850, Accuracy: 0.8281\n",
      "Epoch [55/100], Step [100/254], Loss: 0.5945, Accuracy: 0.7969\n",
      "Epoch [55/100], Step [200/254], Loss: 0.6520, Accuracy: 0.7656\n",
      "Epoch [55/100], Train Loss: 0.6552, Val Loss: 0.5622, Train Acc: 0.7677, Val Acc: 0.7987\n",
      "==================================================\n",
      "Epoch [56/100], Step [0/254], Loss: 0.6257, Accuracy: 0.7812\n",
      "Epoch [56/100], Step [100/254], Loss: 0.5915, Accuracy: 0.7344\n",
      "Epoch [56/100], Step [200/254], Loss: 0.6582, Accuracy: 0.7344\n",
      "Epoch [56/100], Train Loss: 0.6436, Val Loss: 0.5635, Train Acc: 0.7713, Val Acc: 0.8002\n",
      "==================================================\n",
      "Epoch [57/100], Step [0/254], Loss: 0.6282, Accuracy: 0.7188\n",
      "Epoch [57/100], Step [100/254], Loss: 0.5063, Accuracy: 0.8281\n",
      "Epoch [57/100], Step [200/254], Loss: 0.6861, Accuracy: 0.7031\n",
      "Epoch [57/100], Train Loss: 0.6282, Val Loss: 0.5450, Train Acc: 0.7756, Val Acc: 0.8094\n",
      "==================================================\n",
      "Epoch [58/100], Step [0/254], Loss: 0.5382, Accuracy: 0.8281\n",
      "Epoch [58/100], Step [100/254], Loss: 0.5481, Accuracy: 0.8281\n",
      "Epoch [58/100], Step [200/254], Loss: 0.4681, Accuracy: 0.8438\n",
      "Epoch [58/100], Train Loss: 0.6250, Val Loss: 0.5483, Train Acc: 0.7790, Val Acc: 0.8027\n",
      "==================================================\n",
      "Epoch [59/100], Step [0/254], Loss: 0.7380, Accuracy: 0.7812\n",
      "Epoch [59/100], Step [100/254], Loss: 0.6100, Accuracy: 0.8281\n",
      "Epoch [59/100], Step [200/254], Loss: 0.6794, Accuracy: 0.7500\n",
      "Epoch [59/100], Train Loss: 0.6303, Val Loss: 0.5594, Train Acc: 0.7774, Val Acc: 0.7987\n",
      "==================================================\n",
      "Epoch [60/100], Step [0/254], Loss: 0.4857, Accuracy: 0.8281\n",
      "Epoch [60/100], Step [100/254], Loss: 0.8127, Accuracy: 0.6719\n",
      "Epoch [60/100], Step [200/254], Loss: 0.6083, Accuracy: 0.7812\n",
      "Epoch [60/100], Train Loss: 0.6129, Val Loss: 0.5536, Train Acc: 0.7813, Val Acc: 0.8093\n",
      "==================================================\n",
      "Epoch [61/100], Step [0/254], Loss: 0.7948, Accuracy: 0.7500\n",
      "Epoch [61/100], Step [100/254], Loss: 0.5469, Accuracy: 0.8125\n",
      "Epoch [61/100], Step [200/254], Loss: 0.7207, Accuracy: 0.7188\n",
      "Epoch [61/100], Train Loss: 0.6072, Val Loss: 0.5935, Train Acc: 0.7838, Val Acc: 0.8097\n",
      "==================================================\n",
      "Epoch [62/100], Step [0/254], Loss: 0.6940, Accuracy: 0.7500\n",
      "Epoch [62/100], Step [100/254], Loss: 0.6836, Accuracy: 0.6875\n",
      "Epoch [62/100], Step [200/254], Loss: 0.4755, Accuracy: 0.8125\n",
      "Epoch [62/100], Train Loss: 0.5947, Val Loss: 0.5312, Train Acc: 0.7864, Val Acc: 0.8132\n",
      "==================================================\n",
      "Epoch [63/100], Step [0/254], Loss: 0.6441, Accuracy: 0.7812\n",
      "Epoch [63/100], Step [100/254], Loss: 0.5851, Accuracy: 0.6875\n",
      "Epoch [63/100], Step [200/254], Loss: 0.4706, Accuracy: 0.8438\n",
      "Epoch [63/100], Train Loss: 0.6141, Val Loss: 0.5281, Train Acc: 0.7815, Val Acc: 0.8131\n",
      "==================================================\n",
      "Epoch [64/100], Step [0/254], Loss: 1.0329, Accuracy: 0.6875\n",
      "Epoch [64/100], Step [100/254], Loss: 0.5379, Accuracy: 0.7656\n",
      "Epoch [64/100], Step [200/254], Loss: 0.5730, Accuracy: 0.8438\n",
      "Epoch [64/100], Train Loss: 0.5968, Val Loss: 0.5804, Train Acc: 0.7896, Val Acc: 0.8069\n",
      "==================================================\n",
      "Epoch [65/100], Step [0/254], Loss: 0.5707, Accuracy: 0.8281\n",
      "Epoch [65/100], Step [100/254], Loss: 0.4844, Accuracy: 0.8125\n",
      "Epoch [65/100], Step [200/254], Loss: 0.7128, Accuracy: 0.7344\n",
      "Epoch [65/100], Train Loss: 0.6023, Val Loss: 0.5296, Train Acc: 0.7851, Val Acc: 0.8149\n",
      "==================================================\n",
      "Epoch [66/100], Step [0/254], Loss: 0.5926, Accuracy: 0.7969\n",
      "Epoch [66/100], Step [100/254], Loss: 0.3949, Accuracy: 0.8594\n",
      "Epoch [66/100], Step [200/254], Loss: 0.6596, Accuracy: 0.7969\n",
      "Epoch [66/100], Train Loss: 0.6069, Val Loss: 0.5244, Train Acc: 0.7831, Val Acc: 0.8128\n",
      "==================================================\n",
      "Epoch [67/100], Step [0/254], Loss: 0.7606, Accuracy: 0.7188\n",
      "Epoch [67/100], Step [100/254], Loss: 0.3743, Accuracy: 0.8906\n",
      "Epoch [67/100], Step [200/254], Loss: 0.9109, Accuracy: 0.6875\n",
      "Epoch [67/100], Train Loss: 0.5932, Val Loss: 0.5263, Train Acc: 0.7879, Val Acc: 0.8165\n",
      "==================================================\n",
      "Epoch [68/100], Step [0/254], Loss: 0.6920, Accuracy: 0.7344\n",
      "Epoch [68/100], Step [100/254], Loss: 0.7731, Accuracy: 0.7344\n",
      "Epoch [68/100], Step [200/254], Loss: 0.6844, Accuracy: 0.7500\n",
      "Epoch [68/100], Train Loss: 0.5999, Val Loss: 0.5325, Train Acc: 0.7860, Val Acc: 0.8145\n",
      "==================================================\n",
      "Epoch [69/100], Step [0/254], Loss: 0.5088, Accuracy: 0.8125\n",
      "Epoch [69/100], Step [100/254], Loss: 0.7078, Accuracy: 0.7500\n",
      "Epoch [69/100], Step [200/254], Loss: 0.5870, Accuracy: 0.7656\n",
      "Epoch [69/100], Train Loss: 0.6154, Val Loss: 0.6018, Train Acc: 0.7857, Val Acc: 0.8118\n",
      "==================================================\n",
      "Epoch [70/100], Step [0/254], Loss: 0.6383, Accuracy: 0.7812\n",
      "Epoch [70/100], Step [100/254], Loss: 0.6313, Accuracy: 0.6719\n",
      "Epoch [70/100], Step [200/254], Loss: 0.7984, Accuracy: 0.6875\n",
      "Epoch [70/100], Train Loss: 0.5940, Val Loss: 0.5208, Train Acc: 0.7894, Val Acc: 0.8168\n",
      "==================================================\n",
      "Epoch [71/100], Step [0/254], Loss: 0.5678, Accuracy: 0.7656\n",
      "Epoch [71/100], Step [100/254], Loss: 0.9245, Accuracy: 0.6406\n",
      "Epoch [71/100], Step [200/254], Loss: 0.5567, Accuracy: 0.8281\n",
      "Epoch [71/100], Train Loss: 0.5907, Val Loss: 0.5245, Train Acc: 0.7913, Val Acc: 0.8114\n",
      "==================================================\n",
      "Epoch [72/100], Step [0/254], Loss: 0.6938, Accuracy: 0.7812\n",
      "Epoch [72/100], Step [100/254], Loss: 0.4561, Accuracy: 0.8438\n",
      "Epoch [72/100], Step [200/254], Loss: 0.6597, Accuracy: 0.7344\n",
      "Epoch [72/100], Train Loss: 0.5953, Val Loss: 0.5201, Train Acc: 0.7893, Val Acc: 0.8164\n",
      "==================================================\n",
      "Epoch [73/100], Step [0/254], Loss: 0.4530, Accuracy: 0.8281\n",
      "Epoch [73/100], Step [100/254], Loss: 0.7022, Accuracy: 0.7500\n",
      "Epoch [73/100], Step [200/254], Loss: 0.4780, Accuracy: 0.8281\n",
      "Epoch [73/100], Train Loss: 0.5937, Val Loss: 0.5237, Train Acc: 0.7913, Val Acc: 0.8156\n",
      "==================================================\n",
      "Epoch [74/100], Step [0/254], Loss: 0.5066, Accuracy: 0.7812\n",
      "Epoch [74/100], Step [100/254], Loss: 0.5206, Accuracy: 0.7969\n",
      "Epoch [74/100], Step [200/254], Loss: 0.5068, Accuracy: 0.8438\n",
      "Epoch [74/100], Train Loss: 0.5985, Val Loss: 0.6057, Train Acc: 0.7897, Val Acc: 0.8144\n",
      "==================================================\n",
      "Epoch [75/100], Step [0/254], Loss: 0.7118, Accuracy: 0.7812\n",
      "Epoch [75/100], Step [100/254], Loss: 0.4959, Accuracy: 0.8125\n",
      "Epoch [75/100], Step [200/254], Loss: 0.5459, Accuracy: 0.7969\n",
      "Epoch [75/100], Train Loss: 0.5895, Val Loss: 0.5170, Train Acc: 0.7844, Val Acc: 0.8182\n",
      "==================================================\n",
      "Epoch [76/100], Step [0/254], Loss: 0.5217, Accuracy: 0.7812\n",
      "Epoch [76/100], Step [100/254], Loss: 0.7824, Accuracy: 0.7500\n",
      "Epoch [76/100], Step [200/254], Loss: 0.5357, Accuracy: 0.8281\n",
      "Epoch [76/100], Train Loss: 0.6034, Val Loss: 0.5319, Train Acc: 0.7881, Val Acc: 0.8152\n",
      "==================================================\n",
      "Epoch [77/100], Step [0/254], Loss: 0.5056, Accuracy: 0.8125\n",
      "Epoch [77/100], Step [100/254], Loss: 0.4629, Accuracy: 0.8125\n",
      "Epoch [77/100], Step [200/254], Loss: 0.5228, Accuracy: 0.8438\n",
      "Epoch [77/100], Train Loss: 0.5948, Val Loss: 0.5165, Train Acc: 0.7892, Val Acc: 0.8146\n",
      "==================================================\n",
      "Epoch [78/100], Step [0/254], Loss: 0.5521, Accuracy: 0.7969\n",
      "Epoch [78/100], Step [100/254], Loss: 0.5657, Accuracy: 0.8281\n",
      "Epoch [78/100], Step [200/254], Loss: 0.5909, Accuracy: 0.8125\n",
      "Epoch [78/100], Train Loss: 0.5888, Val Loss: 0.5141, Train Acc: 0.7940, Val Acc: 0.8163\n",
      "==================================================\n",
      "Epoch [79/100], Step [0/254], Loss: 0.4781, Accuracy: 0.8125\n",
      "Epoch [79/100], Step [100/254], Loss: 0.6011, Accuracy: 0.7656\n",
      "Epoch [79/100], Step [200/254], Loss: 0.4663, Accuracy: 0.8125\n",
      "Epoch [79/100], Train Loss: 0.5926, Val Loss: 0.5409, Train Acc: 0.7878, Val Acc: 0.8157\n",
      "==================================================\n",
      "Epoch [80/100], Step [0/254], Loss: 0.6000, Accuracy: 0.8281\n",
      "Epoch [80/100], Step [100/254], Loss: 0.6696, Accuracy: 0.8125\n",
      "Epoch [80/100], Step [200/254], Loss: 0.3967, Accuracy: 0.8438\n",
      "Epoch [80/100], Train Loss: 0.5948, Val Loss: 0.5075, Train Acc: 0.7881, Val Acc: 0.8202\n",
      "==================================================\n",
      "Epoch [81/100], Step [0/254], Loss: 0.6079, Accuracy: 0.7812\n",
      "Epoch [81/100], Step [100/254], Loss: 0.5556, Accuracy: 0.7812\n",
      "Epoch [81/100], Step [200/254], Loss: 0.5059, Accuracy: 0.8438\n",
      "Epoch [81/100], Train Loss: 0.5912, Val Loss: 0.5123, Train Acc: 0.7893, Val Acc: 0.8191\n",
      "==================================================\n",
      "Epoch [82/100], Step [0/254], Loss: 0.7305, Accuracy: 0.7344\n",
      "Epoch [82/100], Step [100/254], Loss: 0.3909, Accuracy: 0.8906\n",
      "Epoch [82/100], Step [200/254], Loss: 0.7471, Accuracy: 0.6719\n",
      "Epoch [82/100], Train Loss: 0.5952, Val Loss: 0.5415, Train Acc: 0.7841, Val Acc: 0.8160\n",
      "==================================================\n",
      "Epoch [83/100], Step [0/254], Loss: 0.7647, Accuracy: 0.7500\n",
      "Epoch [83/100], Step [100/254], Loss: 0.9137, Accuracy: 0.6562\n",
      "Epoch [83/100], Step [200/254], Loss: 0.6020, Accuracy: 0.7812\n",
      "Epoch [83/100], Train Loss: 0.5846, Val Loss: 0.5183, Train Acc: 0.7905, Val Acc: 0.8186\n",
      "==================================================\n",
      "Epoch [84/100], Step [0/254], Loss: 0.6887, Accuracy: 0.8125\n",
      "Epoch [84/100], Step [100/254], Loss: 0.8253, Accuracy: 0.7812\n",
      "Epoch [84/100], Step [200/254], Loss: 0.6374, Accuracy: 0.7344\n",
      "Epoch [84/100], Train Loss: 0.5930, Val Loss: 0.5672, Train Acc: 0.7889, Val Acc: 0.8169\n",
      "==================================================\n",
      "Epoch [85/100], Step [0/254], Loss: 0.5341, Accuracy: 0.7969\n",
      "Epoch [85/100], Step [100/254], Loss: 0.4417, Accuracy: 0.8594\n",
      "Epoch [85/100], Step [200/254], Loss: 0.6796, Accuracy: 0.7500\n",
      "Epoch [85/100], Train Loss: 0.5900, Val Loss: 0.5117, Train Acc: 0.7903, Val Acc: 0.8191\n",
      "==================================================\n",
      "Epoch [86/100], Step [0/254], Loss: 0.6780, Accuracy: 0.7344\n",
      "Epoch [86/100], Step [100/254], Loss: 0.4033, Accuracy: 0.8438\n",
      "Epoch [86/100], Step [200/254], Loss: 0.4959, Accuracy: 0.8594\n",
      "Epoch [86/100], Train Loss: 0.5809, Val Loss: 0.6582, Train Acc: 0.7935, Val Acc: 0.8220\n",
      "==================================================\n",
      "Epoch [87/100], Step [0/254], Loss: 0.7417, Accuracy: 0.7500\n",
      "Epoch [87/100], Step [100/254], Loss: 0.5045, Accuracy: 0.7500\n",
      "Epoch [87/100], Step [200/254], Loss: 0.6440, Accuracy: 0.7812\n",
      "Epoch [87/100], Train Loss: 0.5814, Val Loss: 0.5236, Train Acc: 0.7913, Val Acc: 0.8192\n",
      "==================================================\n",
      "Epoch [88/100], Step [0/254], Loss: 0.4294, Accuracy: 0.8594\n",
      "Epoch [88/100], Step [100/254], Loss: 0.6170, Accuracy: 0.7188\n",
      "Epoch [88/100], Step [200/254], Loss: 0.5957, Accuracy: 0.8438\n",
      "Epoch [88/100], Train Loss: 0.5890, Val Loss: 0.5058, Train Acc: 0.7915, Val Acc: 0.8211\n",
      "==================================================\n",
      "Epoch [89/100], Step [0/254], Loss: 0.5545, Accuracy: 0.7812\n",
      "Epoch [89/100], Step [100/254], Loss: 0.5773, Accuracy: 0.7656\n",
      "Epoch [89/100], Step [200/254], Loss: 0.6881, Accuracy: 0.7500\n",
      "Epoch [89/100], Train Loss: 0.5888, Val Loss: 0.5079, Train Acc: 0.7886, Val Acc: 0.8198\n",
      "==================================================\n",
      "Epoch [90/100], Step [0/254], Loss: 0.5973, Accuracy: 0.7344\n",
      "Epoch [90/100], Step [100/254], Loss: 0.5764, Accuracy: 0.7500\n",
      "Epoch [90/100], Step [200/254], Loss: 0.4095, Accuracy: 0.8438\n",
      "Epoch [90/100], Train Loss: 0.5848, Val Loss: 0.5253, Train Acc: 0.7913, Val Acc: 0.8177\n",
      "==================================================\n",
      "Epoch [91/100], Step [0/254], Loss: 0.6601, Accuracy: 0.7656\n",
      "Epoch [91/100], Step [100/254], Loss: 0.5887, Accuracy: 0.7500\n",
      "Epoch [91/100], Step [200/254], Loss: 0.7512, Accuracy: 0.7344\n",
      "Epoch [91/100], Train Loss: 0.5764, Val Loss: 0.5996, Train Acc: 0.7921, Val Acc: 0.8209\n",
      "==================================================\n",
      "Epoch [92/100], Step [0/254], Loss: 0.6852, Accuracy: 0.6875\n",
      "Epoch [92/100], Step [100/254], Loss: 0.5362, Accuracy: 0.8281\n",
      "Epoch [92/100], Step [200/254], Loss: 0.5507, Accuracy: 0.8125\n",
      "Epoch [92/100], Train Loss: 0.5812, Val Loss: 0.5055, Train Acc: 0.7926, Val Acc: 0.8186\n",
      "==================================================\n",
      "Epoch [93/100], Step [0/254], Loss: 0.8690, Accuracy: 0.7344\n",
      "Epoch [93/100], Step [100/254], Loss: 0.5665, Accuracy: 0.8125\n",
      "Epoch [93/100], Step [200/254], Loss: 0.6446, Accuracy: 0.7812\n",
      "Epoch [93/100], Train Loss: 0.5767, Val Loss: 0.5518, Train Acc: 0.7927, Val Acc: 0.8206\n",
      "==================================================\n",
      "Epoch [94/100], Step [0/254], Loss: 0.5655, Accuracy: 0.8125\n",
      "Epoch [94/100], Step [100/254], Loss: 0.5291, Accuracy: 0.8281\n",
      "Epoch [94/100], Step [200/254], Loss: 0.6042, Accuracy: 0.7812\n",
      "Epoch [94/100], Train Loss: 0.5866, Val Loss: 0.5075, Train Acc: 0.7943, Val Acc: 0.8188\n",
      "==================================================\n",
      "Epoch [95/100], Step [0/254], Loss: 0.7489, Accuracy: 0.7188\n",
      "Epoch [95/100], Step [100/254], Loss: 0.4176, Accuracy: 0.8750\n",
      "Epoch [95/100], Step [200/254], Loss: 0.7231, Accuracy: 0.7344\n",
      "Epoch [95/100], Train Loss: 0.5811, Val Loss: 0.5081, Train Acc: 0.7916, Val Acc: 0.8216\n",
      "==================================================\n",
      "Epoch [96/100], Step [0/254], Loss: 0.6064, Accuracy: 0.7812\n",
      "Epoch [96/100], Step [100/254], Loss: 1.0233, Accuracy: 0.6094\n",
      "Epoch [96/100], Step [200/254], Loss: 0.6448, Accuracy: 0.7812\n",
      "Epoch [96/100], Train Loss: 0.5749, Val Loss: 0.6929, Train Acc: 0.7919, Val Acc: 0.8207\n",
      "==================================================\n",
      "Epoch [97/100], Step [0/254], Loss: 0.6649, Accuracy: 0.7969\n",
      "Epoch [97/100], Step [100/254], Loss: 0.3988, Accuracy: 0.8906\n",
      "Epoch [97/100], Step [200/254], Loss: 0.6282, Accuracy: 0.7812\n",
      "Epoch [97/100], Train Loss: 0.5847, Val Loss: 0.5313, Train Acc: 0.7934, Val Acc: 0.8200\n",
      "==================================================\n",
      "Epoch [98/100], Step [0/254], Loss: 0.6521, Accuracy: 0.8125\n",
      "Epoch [98/100], Step [100/254], Loss: 0.4421, Accuracy: 0.8594\n",
      "Epoch [98/100], Step [200/254], Loss: 0.6615, Accuracy: 0.7031\n",
      "Epoch [98/100], Train Loss: 0.5867, Val Loss: 0.5624, Train Acc: 0.7938, Val Acc: 0.8192\n",
      "==================================================\n",
      "Epoch [99/100], Step [0/254], Loss: 0.5662, Accuracy: 0.7969\n",
      "Epoch [99/100], Step [100/254], Loss: 0.5605, Accuracy: 0.7344\n",
      "Epoch [99/100], Step [200/254], Loss: 0.5845, Accuracy: 0.7969\n",
      "Epoch [99/100], Train Loss: 0.5809, Val Loss: 0.5084, Train Acc: 0.7932, Val Acc: 0.8210\n",
      "==================================================\n",
      "Epoch [100/100], Step [0/254], Loss: 0.5593, Accuracy: 0.7500\n",
      "Epoch [100/100], Step [100/254], Loss: 0.6577, Accuracy: 0.7656\n",
      "Epoch [100/100], Step [200/254], Loss: 0.4851, Accuracy: 0.8125\n",
      "Epoch [100/100], Train Loss: 0.5815, Val Loss: 0.6370, Train Acc: 0.7925, Val Acc: 0.8236\n",
      "==================================================\n",
      "CPU times: user 13h 36min 56s, sys: 27min 19s, total: 14h 4min 16s\n",
      "Wall time: 1h 46min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suitable learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Step [0/254], Loss: 2.3072, Accuracy: 0.0938\n",
      "Epoch [1/100], Step [100/254], Loss: 1.0484, Accuracy: 0.5625\n",
      "Epoch [1/100], Step [200/254], Loss: 1.0228, Accuracy: 0.5938\n",
      "Epoch [1/100], Train Loss: 1.2454, Val Loss: 2.6897, Train Acc: 0.5452, Val Acc: 0.0738\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 0.8372, Accuracy: 0.7344\n",
      "Epoch [2/100], Step [100/254], Loss: 0.8029, Accuracy: 0.7031\n",
      "Epoch [2/100], Step [200/254], Loss: 0.8599, Accuracy: 0.7031\n",
      "Epoch [2/100], Train Loss: 0.8802, Val Loss: 1.8130, Train Acc: 0.6821, Val Acc: 0.4188\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 0.6136, Accuracy: 0.7656\n",
      "Epoch [3/100], Step [100/254], Loss: 1.1179, Accuracy: 0.5625\n",
      "Epoch [3/100], Step [200/254], Loss: 1.0394, Accuracy: 0.5938\n",
      "Epoch [3/100], Train Loss: 0.7825, Val Loss: 0.9701, Train Acc: 0.7217, Val Acc: 0.6691\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 0.6884, Accuracy: 0.7656\n",
      "Epoch [4/100], Step [100/254], Loss: 0.5601, Accuracy: 0.8438\n",
      "Epoch [4/100], Step [200/254], Loss: 0.7354, Accuracy: 0.7188\n",
      "Epoch [4/100], Train Loss: 0.7177, Val Loss: 0.9285, Train Acc: 0.7442, Val Acc: 0.6704\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 0.6905, Accuracy: 0.6875\n",
      "Epoch [5/100], Step [100/254], Loss: 0.5127, Accuracy: 0.8438\n",
      "Epoch [5/100], Step [200/254], Loss: 0.7640, Accuracy: 0.6875\n",
      "Epoch [5/100], Train Loss: 0.6601, Val Loss: 0.7512, Train Acc: 0.7720, Val Acc: 0.7344\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/254], Loss: 0.7690, Accuracy: 0.7344\n",
      "Epoch [6/100], Step [100/254], Loss: 0.6458, Accuracy: 0.8125\n",
      "Epoch [6/100], Step [200/254], Loss: 0.5371, Accuracy: 0.7344\n",
      "Epoch [6/100], Train Loss: 0.6055, Val Loss: 0.8701, Train Acc: 0.7894, Val Acc: 0.7030\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/254], Loss: 0.5375, Accuracy: 0.8438\n",
      "Epoch [7/100], Step [100/254], Loss: 0.6435, Accuracy: 0.7969\n",
      "Epoch [7/100], Step [200/254], Loss: 0.5378, Accuracy: 0.8750\n",
      "Epoch [7/100], Train Loss: 0.5400, Val Loss: 0.6500, Train Acc: 0.8118, Val Acc: 0.7887\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/254], Loss: 0.4778, Accuracy: 0.8125\n",
      "Epoch [8/100], Step [100/254], Loss: 0.3636, Accuracy: 0.8906\n",
      "Epoch [8/100], Step [200/254], Loss: 0.5292, Accuracy: 0.7656\n",
      "Epoch [8/100], Train Loss: 0.5026, Val Loss: 0.6610, Train Acc: 0.8239, Val Acc: 0.7752\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/254], Loss: 0.4748, Accuracy: 0.8438\n",
      "Epoch [9/100], Step [100/254], Loss: 0.5434, Accuracy: 0.8594\n",
      "Epoch [9/100], Step [200/254], Loss: 0.3227, Accuracy: 0.8594\n",
      "Epoch [9/100], Train Loss: 0.4870, Val Loss: 1.1424, Train Acc: 0.8292, Val Acc: 0.6677\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/254], Loss: 0.6286, Accuracy: 0.8125\n",
      "Epoch [10/100], Step [100/254], Loss: 0.4730, Accuracy: 0.8594\n",
      "Epoch [10/100], Step [200/254], Loss: 0.4289, Accuracy: 0.8281\n",
      "Epoch [10/100], Train Loss: 0.4783, Val Loss: 0.5818, Train Acc: 0.8370, Val Acc: 0.8032\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/254], Loss: 0.3883, Accuracy: 0.9219\n",
      "Epoch [11/100], Step [100/254], Loss: 0.4641, Accuracy: 0.8594\n",
      "Epoch [11/100], Step [200/254], Loss: 0.4735, Accuracy: 0.8594\n",
      "Epoch [11/100], Train Loss: 0.4435, Val Loss: 0.8402, Train Acc: 0.8473, Val Acc: 0.7328\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/254], Loss: 0.3551, Accuracy: 0.8906\n",
      "Epoch [12/100], Step [100/254], Loss: 0.4965, Accuracy: 0.8281\n",
      "Epoch [12/100], Step [200/254], Loss: 0.5826, Accuracy: 0.7969\n",
      "Epoch [12/100], Train Loss: 0.4224, Val Loss: 0.8357, Train Acc: 0.8519, Val Acc: 0.7341\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/254], Loss: 0.4588, Accuracy: 0.8438\n",
      "Epoch [13/100], Step [100/254], Loss: 0.3655, Accuracy: 0.8906\n",
      "Epoch [13/100], Step [200/254], Loss: 0.3618, Accuracy: 0.8594\n",
      "Epoch [13/100], Train Loss: 0.4173, Val Loss: 1.1962, Train Acc: 0.8557, Val Acc: 0.6791\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/254], Loss: 0.3439, Accuracy: 0.9219\n",
      "Epoch [14/100], Step [100/254], Loss: 0.5031, Accuracy: 0.8594\n",
      "Epoch [14/100], Step [200/254], Loss: 0.2615, Accuracy: 0.9219\n",
      "Epoch [14/100], Train Loss: 0.3983, Val Loss: 0.9087, Train Acc: 0.8623, Val Acc: 0.7277\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/254], Loss: 0.1705, Accuracy: 0.9531\n",
      "Epoch [15/100], Step [100/254], Loss: 0.3730, Accuracy: 0.8125\n",
      "Epoch [15/100], Step [200/254], Loss: 0.3357, Accuracy: 0.9062\n",
      "Epoch [15/100], Train Loss: 0.3214, Val Loss: 0.2785, Train Acc: 0.8909, Val Acc: 0.9027\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/254], Loss: 0.2071, Accuracy: 0.9062\n",
      "Epoch [16/100], Step [100/254], Loss: 0.3274, Accuracy: 0.8125\n",
      "Epoch [16/100], Step [200/254], Loss: 0.3276, Accuracy: 0.9062\n",
      "Epoch [16/100], Train Loss: 0.3180, Val Loss: 0.2514, Train Acc: 0.8891, Val Acc: 0.9126\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/254], Loss: 0.3958, Accuracy: 0.8594\n",
      "Epoch [17/100], Step [100/254], Loss: 0.2836, Accuracy: 0.8750\n",
      "Epoch [17/100], Step [200/254], Loss: 0.2850, Accuracy: 0.9219\n",
      "Epoch [17/100], Train Loss: 0.2904, Val Loss: 0.3043, Train Acc: 0.8997, Val Acc: 0.8946\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/254], Loss: 0.3141, Accuracy: 0.8906\n",
      "Epoch [18/100], Step [100/254], Loss: 0.3740, Accuracy: 0.8594\n",
      "Epoch [18/100], Step [200/254], Loss: 0.2675, Accuracy: 0.9062\n",
      "Epoch [18/100], Train Loss: 0.2949, Val Loss: 0.2396, Train Acc: 0.8980, Val Acc: 0.9158\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/254], Loss: 0.2294, Accuracy: 0.9375\n",
      "Epoch [19/100], Step [100/254], Loss: 0.2178, Accuracy: 0.9062\n",
      "Epoch [19/100], Step [200/254], Loss: 0.3623, Accuracy: 0.8906\n",
      "Epoch [19/100], Train Loss: 0.2931, Val Loss: 0.2366, Train Acc: 0.8973, Val Acc: 0.9173\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/254], Loss: 0.2337, Accuracy: 0.8906\n",
      "Epoch [20/100], Step [100/254], Loss: 0.2776, Accuracy: 0.8906\n",
      "Epoch [20/100], Step [200/254], Loss: 0.2996, Accuracy: 0.9062\n",
      "Epoch [20/100], Train Loss: 0.2748, Val Loss: 0.2584, Train Acc: 0.9003, Val Acc: 0.9072\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/254], Loss: 0.2387, Accuracy: 0.9062\n",
      "Epoch [21/100], Step [100/254], Loss: 0.3131, Accuracy: 0.8906\n",
      "Epoch [21/100], Step [200/254], Loss: 0.1921, Accuracy: 0.9375\n",
      "Epoch [21/100], Train Loss: 0.2794, Val Loss: 0.2701, Train Acc: 0.9003, Val Acc: 0.9019\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/254], Loss: 0.2271, Accuracy: 0.9375\n",
      "Epoch [22/100], Step [100/254], Loss: 0.2992, Accuracy: 0.9219\n",
      "Epoch [22/100], Step [200/254], Loss: 0.2738, Accuracy: 0.8438\n",
      "Epoch [22/100], Train Loss: 0.2803, Val Loss: 0.2978, Train Acc: 0.9029, Val Acc: 0.8959\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/254], Loss: 0.3045, Accuracy: 0.9219\n",
      "Epoch [23/100], Step [100/254], Loss: 0.1317, Accuracy: 0.9844\n",
      "Epoch [23/100], Step [200/254], Loss: 0.4783, Accuracy: 0.8438\n",
      "Epoch [23/100], Train Loss: 0.2767, Val Loss: 0.2840, Train Acc: 0.9034, Val Acc: 0.8987\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/254], Loss: 0.3763, Accuracy: 0.8594\n",
      "Epoch [24/100], Step [100/254], Loss: 0.2645, Accuracy: 0.9062\n",
      "Epoch [24/100], Step [200/254], Loss: 0.3573, Accuracy: 0.8906\n",
      "Epoch [24/100], Train Loss: 0.2544, Val Loss: 0.2016, Train Acc: 0.9126, Val Acc: 0.9310\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/254], Loss: 0.2131, Accuracy: 0.9219\n",
      "Epoch [25/100], Step [100/254], Loss: 0.3422, Accuracy: 0.8750\n",
      "Epoch [25/100], Step [200/254], Loss: 0.2214, Accuracy: 0.9219\n",
      "Epoch [25/100], Train Loss: 0.2535, Val Loss: 0.2027, Train Acc: 0.9117, Val Acc: 0.9288\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/254], Loss: 0.2572, Accuracy: 0.9375\n",
      "Epoch [26/100], Step [100/254], Loss: 0.2010, Accuracy: 0.9375\n",
      "Epoch [26/100], Step [200/254], Loss: 0.1930, Accuracy: 0.9062\n",
      "Epoch [26/100], Train Loss: 0.2552, Val Loss: 0.1982, Train Acc: 0.9115, Val Acc: 0.9314\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/254], Loss: 0.2434, Accuracy: 0.9062\n",
      "Epoch [27/100], Step [100/254], Loss: 0.3207, Accuracy: 0.8906\n",
      "Epoch [27/100], Step [200/254], Loss: 0.2551, Accuracy: 0.9219\n",
      "Epoch [27/100], Train Loss: 0.2550, Val Loss: 0.2002, Train Acc: 0.9099, Val Acc: 0.9307\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/254], Loss: 0.1346, Accuracy: 0.9688\n",
      "Epoch [28/100], Step [100/254], Loss: 0.3005, Accuracy: 0.9062\n",
      "Epoch [28/100], Step [200/254], Loss: 0.2363, Accuracy: 0.9531\n",
      "Epoch [28/100], Train Loss: 0.2543, Val Loss: 0.2017, Train Acc: 0.9115, Val Acc: 0.9283\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/254], Loss: 0.3009, Accuracy: 0.8906\n",
      "Epoch [29/100], Step [100/254], Loss: 0.1321, Accuracy: 0.9531\n",
      "Epoch [29/100], Step [200/254], Loss: 0.2629, Accuracy: 0.8750\n",
      "Epoch [29/100], Train Loss: 0.2451, Val Loss: 0.1994, Train Acc: 0.9161, Val Acc: 0.9313\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/254], Loss: 0.2534, Accuracy: 0.8906\n",
      "Epoch [30/100], Step [100/254], Loss: 0.1515, Accuracy: 0.9531\n",
      "Epoch [30/100], Step [200/254], Loss: 0.1481, Accuracy: 0.9531\n",
      "Epoch [30/100], Train Loss: 0.2387, Val Loss: 0.2011, Train Acc: 0.9144, Val Acc: 0.9292\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/254], Loss: 0.3147, Accuracy: 0.9062\n",
      "Epoch [31/100], Step [100/254], Loss: 0.2660, Accuracy: 0.9062\n",
      "Epoch [31/100], Step [200/254], Loss: 0.2833, Accuracy: 0.8594\n",
      "Epoch [31/100], Train Loss: 0.2390, Val Loss: 0.1931, Train Acc: 0.9144, Val Acc: 0.9310\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/254], Loss: 0.1753, Accuracy: 0.9219\n",
      "Epoch [32/100], Step [100/254], Loss: 0.1655, Accuracy: 0.9531\n",
      "Epoch [32/100], Step [200/254], Loss: 0.3050, Accuracy: 0.8750\n",
      "Epoch [32/100], Train Loss: 0.2350, Val Loss: 0.1876, Train Acc: 0.9155, Val Acc: 0.9342\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/254], Loss: 0.2917, Accuracy: 0.8906\n",
      "Epoch [33/100], Step [100/254], Loss: 0.2869, Accuracy: 0.8906\n",
      "Epoch [33/100], Step [200/254], Loss: 0.4849, Accuracy: 0.8438\n",
      "Epoch [33/100], Train Loss: 0.2413, Val Loss: 0.1958, Train Acc: 0.9160, Val Acc: 0.9320\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/254], Loss: 0.3046, Accuracy: 0.9062\n",
      "Epoch [34/100], Step [100/254], Loss: 0.2740, Accuracy: 0.8906\n",
      "Epoch [34/100], Step [200/254], Loss: 0.1667, Accuracy: 0.9375\n",
      "Epoch [34/100], Train Loss: 0.2459, Val Loss: 0.1920, Train Acc: 0.9166, Val Acc: 0.9320\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/254], Loss: 0.3366, Accuracy: 0.8906\n",
      "Epoch [35/100], Step [100/254], Loss: 0.1567, Accuracy: 0.9375\n",
      "Epoch [35/100], Step [200/254], Loss: 0.1280, Accuracy: 0.9688\n",
      "Epoch [35/100], Train Loss: 0.2405, Val Loss: 0.1922, Train Acc: 0.9154, Val Acc: 0.9320\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/254], Loss: 0.2132, Accuracy: 0.9531\n",
      "Epoch [36/100], Step [100/254], Loss: 0.2200, Accuracy: 0.9375\n",
      "Epoch [36/100], Step [200/254], Loss: 0.1648, Accuracy: 0.9531\n",
      "Epoch [36/100], Train Loss: 0.2374, Val Loss: 0.1903, Train Acc: 0.9148, Val Acc: 0.9333\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/254], Loss: 0.3446, Accuracy: 0.8750\n",
      "Epoch [37/100], Step [100/254], Loss: 0.2920, Accuracy: 0.9062\n",
      "Epoch [37/100], Step [200/254], Loss: 0.2941, Accuracy: 0.9219\n",
      "Epoch [37/100], Train Loss: 0.2327, Val Loss: 0.1898, Train Acc: 0.9205, Val Acc: 0.9317\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/254], Loss: 0.2396, Accuracy: 0.9219\n",
      "Epoch [38/100], Step [100/254], Loss: 0.1761, Accuracy: 0.9531\n",
      "Epoch [38/100], Step [200/254], Loss: 0.2282, Accuracy: 0.8906\n",
      "Epoch [38/100], Train Loss: 0.2425, Val Loss: 0.1881, Train Acc: 0.9164, Val Acc: 0.9337\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/254], Loss: 0.1383, Accuracy: 0.9531\n",
      "Epoch [39/100], Step [100/254], Loss: 0.1937, Accuracy: 0.9531\n",
      "Epoch [39/100], Step [200/254], Loss: 0.1854, Accuracy: 0.9219\n",
      "Epoch [39/100], Train Loss: 0.2338, Val Loss: 0.1933, Train Acc: 0.9193, Val Acc: 0.9334\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/254], Loss: 0.1407, Accuracy: 0.9375\n",
      "Epoch [40/100], Step [100/254], Loss: 0.1830, Accuracy: 0.9375\n",
      "Epoch [40/100], Step [200/254], Loss: 0.0544, Accuracy: 1.0000\n",
      "Epoch [40/100], Train Loss: 0.2339, Val Loss: 0.1866, Train Acc: 0.9158, Val Acc: 0.9343\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/254], Loss: 0.3219, Accuracy: 0.8438\n",
      "Epoch [41/100], Step [100/254], Loss: 0.1101, Accuracy: 0.9688\n",
      "Epoch [41/100], Step [200/254], Loss: 0.3443, Accuracy: 0.7969\n",
      "Epoch [41/100], Train Loss: 0.2366, Val Loss: 0.1900, Train Acc: 0.9157, Val Acc: 0.9322\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/254], Loss: 0.3968, Accuracy: 0.8438\n",
      "Epoch [42/100], Step [100/254], Loss: 0.1383, Accuracy: 0.9531\n",
      "Epoch [42/100], Step [200/254], Loss: 0.2318, Accuracy: 0.9062\n",
      "Epoch [42/100], Train Loss: 0.2327, Val Loss: 0.1859, Train Acc: 0.9178, Val Acc: 0.9341\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/254], Loss: 0.2036, Accuracy: 0.9219\n",
      "Epoch [43/100], Step [100/254], Loss: 0.1260, Accuracy: 0.9531\n",
      "Epoch [43/100], Step [200/254], Loss: 0.1569, Accuracy: 0.9219\n",
      "Epoch [43/100], Train Loss: 0.2293, Val Loss: 0.1893, Train Acc: 0.9198, Val Acc: 0.9320\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/254], Loss: 0.2128, Accuracy: 0.9375\n",
      "Epoch [44/100], Step [100/254], Loss: 0.1967, Accuracy: 0.9375\n",
      "Epoch [44/100], Step [200/254], Loss: 0.2945, Accuracy: 0.8906\n",
      "Epoch [44/100], Train Loss: 0.2402, Val Loss: 0.1888, Train Acc: 0.9153, Val Acc: 0.9341\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/254], Loss: 0.2727, Accuracy: 0.9375\n",
      "Epoch [45/100], Step [100/254], Loss: 0.2621, Accuracy: 0.9062\n",
      "Epoch [45/100], Step [200/254], Loss: 0.2536, Accuracy: 0.8750\n",
      "Epoch [45/100], Train Loss: 0.2388, Val Loss: 0.1865, Train Acc: 0.9174, Val Acc: 0.9339\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/254], Loss: 0.1203, Accuracy: 0.9531\n",
      "Epoch [46/100], Step [100/254], Loss: 0.3206, Accuracy: 0.8906\n",
      "Epoch [46/100], Step [200/254], Loss: 0.1534, Accuracy: 0.9375\n",
      "Epoch [46/100], Train Loss: 0.2335, Val Loss: 0.1906, Train Acc: 0.9187, Val Acc: 0.9340\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/254], Loss: 0.1638, Accuracy: 0.9219\n",
      "Epoch [47/100], Step [100/254], Loss: 0.2005, Accuracy: 0.9375\n",
      "Epoch [47/100], Step [200/254], Loss: 0.2517, Accuracy: 0.9375\n",
      "Epoch [47/100], Train Loss: 0.2378, Val Loss: 0.1906, Train Acc: 0.9151, Val Acc: 0.9328\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/254], Loss: 0.2845, Accuracy: 0.9219\n",
      "Epoch [48/100], Step [100/254], Loss: 0.2500, Accuracy: 0.8906\n",
      "Epoch [48/100], Step [200/254], Loss: 0.2160, Accuracy: 0.9375\n",
      "Epoch [48/100], Train Loss: 0.2292, Val Loss: 0.1882, Train Acc: 0.9193, Val Acc: 0.9337\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/254], Loss: 0.1194, Accuracy: 0.9688\n",
      "Epoch [49/100], Step [100/254], Loss: 0.2957, Accuracy: 0.8750\n",
      "Epoch [49/100], Step [200/254], Loss: 0.1118, Accuracy: 0.9844\n",
      "Epoch [49/100], Train Loss: 0.2313, Val Loss: 0.1892, Train Acc: 0.9192, Val Acc: 0.9338\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/254], Loss: 0.2657, Accuracy: 0.9062\n",
      "Epoch [50/100], Step [100/254], Loss: 0.3312, Accuracy: 0.8594\n",
      "Epoch [50/100], Step [200/254], Loss: 0.2511, Accuracy: 0.9375\n",
      "Epoch [50/100], Train Loss: 0.2339, Val Loss: 0.1896, Train Acc: 0.9179, Val Acc: 0.9352\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/254], Loss: 0.1479, Accuracy: 0.9844\n",
      "Epoch [51/100], Step [100/254], Loss: 0.1713, Accuracy: 0.9375\n",
      "Epoch [51/100], Step [200/254], Loss: 0.2883, Accuracy: 0.9062\n",
      "Epoch [51/100], Train Loss: 0.2348, Val Loss: 0.1870, Train Acc: 0.9197, Val Acc: 0.9346\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/254], Loss: 0.1791, Accuracy: 0.9219\n",
      "Epoch [52/100], Step [100/254], Loss: 0.2353, Accuracy: 0.9375\n",
      "Epoch [52/100], Step [200/254], Loss: 0.3412, Accuracy: 0.8750\n",
      "Epoch [52/100], Train Loss: 0.2392, Val Loss: 0.1895, Train Acc: 0.9157, Val Acc: 0.9331\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/254], Loss: 0.4188, Accuracy: 0.8438\n",
      "Epoch [53/100], Step [100/254], Loss: 0.2291, Accuracy: 0.9219\n",
      "Epoch [53/100], Step [200/254], Loss: 0.3609, Accuracy: 0.8281\n",
      "Epoch [53/100], Train Loss: 0.2262, Val Loss: 0.1862, Train Acc: 0.9189, Val Acc: 0.9363\n",
      "==================================================\n",
      "CPU times: user 7h 10min 17s, sys: 14min 17s, total: 7h 24min 35s\n",
      "Wall time: 56min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 911 ms, total: 28.4 s\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred, actual = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9209\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = (torch.argmax(pred, dim=1) == actual).float().mean()\n",
    "print(f\"Test Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using GELU instead of hard swish in bottleneck block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import GELU\n",
    "\n",
    "\n",
    "class MobileNetV3Gelu(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"small\":\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, GELU, 2],\n",
    "                [5, 240, 40, True, GELU, 1],\n",
    "                [5, 240, 40, True, GELU, 1],\n",
    "                [5, 120, 48, True, GELU, 1],\n",
    "                [5, 144, 48, True, GELU, 1],\n",
    "                [5, 288, 96, True, GELU, 2],\n",
    "                [5, 576, 96, True, GELU, 1],\n",
    "                [5, 576, 96, True, GELU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "\n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Step [0/254], Loss: 2.3029, Accuracy: 0.0469\n",
      "Epoch [1/100], Step [100/254], Loss: 1.0777, Accuracy: 0.5938\n",
      "Epoch [1/100], Step [200/254], Loss: 0.7113, Accuracy: 0.7344\n",
      "Epoch [1/100], Train Loss: 1.1523, Val Loss: 2.8583, Train Acc: 0.5842, Val Acc: 0.0738\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 1.1049, Accuracy: 0.5469\n",
      "Epoch [2/100], Step [100/254], Loss: 0.7643, Accuracy: 0.7031\n",
      "Epoch [2/100], Step [200/254], Loss: 0.8792, Accuracy: 0.6719\n",
      "Epoch [2/100], Train Loss: 0.8528, Val Loss: 1.6611, Train Acc: 0.6964, Val Acc: 0.4459\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 0.6187, Accuracy: 0.7500\n",
      "Epoch [3/100], Step [100/254], Loss: 0.5444, Accuracy: 0.8125\n",
      "Epoch [3/100], Step [200/254], Loss: 0.5530, Accuracy: 0.7812\n",
      "Epoch [3/100], Train Loss: 0.7244, Val Loss: 0.7937, Train Acc: 0.7435, Val Acc: 0.7225\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 0.7477, Accuracy: 0.7188\n",
      "Epoch [4/100], Step [100/254], Loss: 0.6297, Accuracy: 0.7188\n",
      "Epoch [4/100], Step [200/254], Loss: 0.4061, Accuracy: 0.8438\n",
      "Epoch [4/100], Train Loss: 0.6624, Val Loss: 0.6784, Train Acc: 0.7697, Val Acc: 0.7521\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 0.5155, Accuracy: 0.8438\n",
      "Epoch [5/100], Step [100/254], Loss: 0.5359, Accuracy: 0.8594\n",
      "Epoch [5/100], Step [200/254], Loss: 0.6121, Accuracy: 0.7344\n",
      "Epoch [5/100], Train Loss: 0.6065, Val Loss: 0.7859, Train Acc: 0.7872, Val Acc: 0.7403\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/254], Loss: 0.6138, Accuracy: 0.8438\n",
      "Epoch [6/100], Step [100/254], Loss: 0.2704, Accuracy: 0.9375\n",
      "Epoch [6/100], Step [200/254], Loss: 0.3434, Accuracy: 0.9219\n",
      "Epoch [6/100], Train Loss: 0.5869, Val Loss: 0.7892, Train Acc: 0.7987, Val Acc: 0.7281\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/254], Loss: 0.3339, Accuracy: 0.8906\n",
      "Epoch [7/100], Step [100/254], Loss: 0.4072, Accuracy: 0.8594\n",
      "Epoch [7/100], Step [200/254], Loss: 0.5419, Accuracy: 0.7969\n",
      "Epoch [7/100], Train Loss: 0.5196, Val Loss: 0.6009, Train Acc: 0.8223, Val Acc: 0.8056\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/254], Loss: 0.5640, Accuracy: 0.7969\n",
      "Epoch [8/100], Step [100/254], Loss: 0.6876, Accuracy: 0.7812\n",
      "Epoch [8/100], Step [200/254], Loss: 0.3652, Accuracy: 0.9062\n",
      "Epoch [8/100], Train Loss: 0.4882, Val Loss: 0.5917, Train Acc: 0.8335, Val Acc: 0.7991\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/254], Loss: 0.3786, Accuracy: 0.8594\n",
      "Epoch [9/100], Step [100/254], Loss: 0.2979, Accuracy: 0.9062\n",
      "Epoch [9/100], Step [200/254], Loss: 0.4052, Accuracy: 0.8438\n",
      "Epoch [9/100], Train Loss: 0.4663, Val Loss: 0.6509, Train Acc: 0.8439, Val Acc: 0.7885\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/254], Loss: 0.4434, Accuracy: 0.7969\n",
      "Epoch [10/100], Step [100/254], Loss: 0.3778, Accuracy: 0.8750\n",
      "Epoch [10/100], Step [200/254], Loss: 0.5915, Accuracy: 0.7812\n",
      "Epoch [10/100], Train Loss: 0.4598, Val Loss: 0.4792, Train Acc: 0.8441, Val Acc: 0.8398\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/254], Loss: 0.2887, Accuracy: 0.9219\n",
      "Epoch [11/100], Step [100/254], Loss: 0.4176, Accuracy: 0.8906\n",
      "Epoch [11/100], Step [200/254], Loss: 0.6411, Accuracy: 0.8125\n",
      "Epoch [11/100], Train Loss: 0.4227, Val Loss: 0.6088, Train Acc: 0.8578, Val Acc: 0.7902\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/254], Loss: 0.5545, Accuracy: 0.8281\n",
      "Epoch [12/100], Step [100/254], Loss: 0.3312, Accuracy: 0.9219\n",
      "Epoch [12/100], Step [200/254], Loss: 0.3727, Accuracy: 0.8906\n",
      "Epoch [12/100], Train Loss: 0.4068, Val Loss: 0.4437, Train Acc: 0.8617, Val Acc: 0.8481\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/254], Loss: 0.5003, Accuracy: 0.8438\n",
      "Epoch [13/100], Step [100/254], Loss: 0.3936, Accuracy: 0.8594\n",
      "Epoch [13/100], Step [200/254], Loss: 0.4285, Accuracy: 0.8281\n",
      "Epoch [13/100], Train Loss: 0.4073, Val Loss: 0.5371, Train Acc: 0.8604, Val Acc: 0.8208\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/254], Loss: 0.4651, Accuracy: 0.8438\n",
      "Epoch [14/100], Step [100/254], Loss: 0.3400, Accuracy: 0.8594\n",
      "Epoch [14/100], Step [200/254], Loss: 0.5133, Accuracy: 0.7969\n",
      "Epoch [14/100], Train Loss: 0.3812, Val Loss: 0.7344, Train Acc: 0.8706, Val Acc: 0.7723\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/254], Loss: 0.3786, Accuracy: 0.8438\n",
      "Epoch [15/100], Step [100/254], Loss: 0.5360, Accuracy: 0.7969\n",
      "Epoch [15/100], Step [200/254], Loss: 0.2500, Accuracy: 0.9375\n",
      "Epoch [15/100], Train Loss: 0.3611, Val Loss: 0.4067, Train Acc: 0.8786, Val Acc: 0.8516\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/254], Loss: 0.4632, Accuracy: 0.8438\n",
      "Epoch [16/100], Step [100/254], Loss: 0.3416, Accuracy: 0.8438\n",
      "Epoch [16/100], Step [200/254], Loss: 0.2626, Accuracy: 0.9062\n",
      "Epoch [16/100], Train Loss: 0.3513, Val Loss: 0.3327, Train Acc: 0.8795, Val Acc: 0.8841\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/254], Loss: 0.2812, Accuracy: 0.9219\n",
      "Epoch [17/100], Step [100/254], Loss: 0.4158, Accuracy: 0.8594\n",
      "Epoch [17/100], Step [200/254], Loss: 0.3264, Accuracy: 0.8594\n",
      "Epoch [17/100], Train Loss: 0.3449, Val Loss: 0.3201, Train Acc: 0.8828, Val Acc: 0.8912\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/254], Loss: 0.3084, Accuracy: 0.8594\n",
      "Epoch [18/100], Step [100/254], Loss: 0.2843, Accuracy: 0.8750\n",
      "Epoch [18/100], Step [200/254], Loss: 0.2894, Accuracy: 0.8906\n",
      "Epoch [18/100], Train Loss: 0.3267, Val Loss: 0.4741, Train Acc: 0.8883, Val Acc: 0.8425\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/254], Loss: 0.3338, Accuracy: 0.8750\n",
      "Epoch [19/100], Step [100/254], Loss: 0.7500, Accuracy: 0.7188\n",
      "Epoch [19/100], Step [200/254], Loss: 0.3403, Accuracy: 0.8438\n",
      "Epoch [19/100], Train Loss: 0.3305, Val Loss: 0.4552, Train Acc: 0.8853, Val Acc: 0.8454\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/254], Loss: 0.4283, Accuracy: 0.8594\n",
      "Epoch [20/100], Step [100/254], Loss: 0.3563, Accuracy: 0.9062\n",
      "Epoch [20/100], Step [200/254], Loss: 0.1729, Accuracy: 0.9531\n",
      "Epoch [20/100], Train Loss: 0.3091, Val Loss: 0.3103, Train Acc: 0.8928, Val Acc: 0.8911\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/254], Loss: 0.2130, Accuracy: 0.9219\n",
      "Epoch [21/100], Step [100/254], Loss: 0.3381, Accuracy: 0.8750\n",
      "Epoch [21/100], Step [200/254], Loss: 0.2610, Accuracy: 0.9219\n",
      "Epoch [21/100], Train Loss: 0.3059, Val Loss: 0.3612, Train Acc: 0.8956, Val Acc: 0.8756\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/254], Loss: 0.4616, Accuracy: 0.8281\n",
      "Epoch [22/100], Step [100/254], Loss: 0.4125, Accuracy: 0.8594\n",
      "Epoch [22/100], Step [200/254], Loss: 0.3128, Accuracy: 0.8906\n",
      "Epoch [22/100], Train Loss: 0.3057, Val Loss: 0.3903, Train Acc: 0.8951, Val Acc: 0.8680\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/254], Loss: 0.5946, Accuracy: 0.7969\n",
      "Epoch [23/100], Step [100/254], Loss: 0.5710, Accuracy: 0.8281\n",
      "Epoch [23/100], Step [200/254], Loss: 0.3389, Accuracy: 0.8906\n",
      "Epoch [23/100], Train Loss: 0.2906, Val Loss: 0.6601, Train Acc: 0.9020, Val Acc: 0.7861\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/254], Loss: 0.2111, Accuracy: 0.9375\n",
      "Epoch [24/100], Step [100/254], Loss: 0.2265, Accuracy: 0.9219\n",
      "Epoch [24/100], Step [200/254], Loss: 0.3731, Accuracy: 0.8750\n",
      "Epoch [24/100], Train Loss: 0.2832, Val Loss: 0.3603, Train Acc: 0.9051, Val Acc: 0.8722\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/254], Loss: 0.2624, Accuracy: 0.9375\n",
      "Epoch [25/100], Step [100/254], Loss: 0.3253, Accuracy: 0.8750\n",
      "Epoch [25/100], Step [200/254], Loss: 0.1124, Accuracy: 0.9531\n",
      "Epoch [25/100], Train Loss: 0.2245, Val Loss: 0.2460, Train Acc: 0.9221, Val Acc: 0.9133\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/254], Loss: 0.1411, Accuracy: 0.9531\n",
      "Epoch [26/100], Step [100/254], Loss: 0.1754, Accuracy: 0.9531\n",
      "Epoch [26/100], Step [200/254], Loss: 0.1932, Accuracy: 0.9531\n",
      "Epoch [26/100], Train Loss: 0.2135, Val Loss: 0.1791, Train Acc: 0.9283, Val Acc: 0.9367\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/254], Loss: 0.0721, Accuracy: 0.9844\n",
      "Epoch [27/100], Step [100/254], Loss: 0.0937, Accuracy: 0.9531\n",
      "Epoch [27/100], Step [200/254], Loss: 0.4421, Accuracy: 0.8281\n",
      "Epoch [27/100], Train Loss: 0.2149, Val Loss: 0.1933, Train Acc: 0.9294, Val Acc: 0.9303\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/254], Loss: 0.0868, Accuracy: 0.9844\n",
      "Epoch [28/100], Step [100/254], Loss: 0.3482, Accuracy: 0.9062\n",
      "Epoch [28/100], Step [200/254], Loss: 0.1365, Accuracy: 0.9531\n",
      "Epoch [28/100], Train Loss: 0.2048, Val Loss: 0.1683, Train Acc: 0.9277, Val Acc: 0.9419\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/254], Loss: 0.1788, Accuracy: 0.9531\n",
      "Epoch [29/100], Step [100/254], Loss: 0.2443, Accuracy: 0.8906\n",
      "Epoch [29/100], Step [200/254], Loss: 0.0929, Accuracy: 0.9688\n",
      "Epoch [29/100], Train Loss: 0.1973, Val Loss: 0.1564, Train Acc: 0.9296, Val Acc: 0.9447\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/254], Loss: 0.3044, Accuracy: 0.9062\n",
      "Epoch [30/100], Step [100/254], Loss: 0.1167, Accuracy: 0.9531\n",
      "Epoch [30/100], Step [200/254], Loss: 0.1619, Accuracy: 0.9531\n",
      "Epoch [30/100], Train Loss: 0.1967, Val Loss: 0.1684, Train Acc: 0.9298, Val Acc: 0.9405\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/254], Loss: 0.1699, Accuracy: 0.9531\n",
      "Epoch [31/100], Step [100/254], Loss: 0.1232, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [200/254], Loss: 0.2519, Accuracy: 0.8594\n",
      "Epoch [31/100], Train Loss: 0.1923, Val Loss: 0.1591, Train Acc: 0.9316, Val Acc: 0.9446\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/254], Loss: 0.1773, Accuracy: 0.9375\n",
      "Epoch [32/100], Step [100/254], Loss: 0.1628, Accuracy: 0.9375\n",
      "Epoch [32/100], Step [200/254], Loss: 0.1632, Accuracy: 0.9688\n",
      "Epoch [32/100], Train Loss: 0.1882, Val Loss: 0.1519, Train Acc: 0.9347, Val Acc: 0.9462\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/254], Loss: 0.0880, Accuracy: 0.9844\n",
      "Epoch [33/100], Step [100/254], Loss: 0.2338, Accuracy: 0.9375\n",
      "Epoch [33/100], Step [200/254], Loss: 0.2354, Accuracy: 0.9062\n",
      "Epoch [33/100], Train Loss: 0.1919, Val Loss: 0.1620, Train Acc: 0.9338, Val Acc: 0.9442\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/254], Loss: 0.2360, Accuracy: 0.9062\n",
      "Epoch [34/100], Step [100/254], Loss: 0.2574, Accuracy: 0.9219\n",
      "Epoch [34/100], Step [200/254], Loss: 0.1561, Accuracy: 0.9531\n",
      "Epoch [34/100], Train Loss: 0.1881, Val Loss: 0.1756, Train Acc: 0.9326, Val Acc: 0.9360\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/254], Loss: 0.1493, Accuracy: 0.9375\n",
      "Epoch [35/100], Step [100/254], Loss: 0.1420, Accuracy: 0.9688\n",
      "Epoch [35/100], Step [200/254], Loss: 0.2533, Accuracy: 0.9219\n",
      "Epoch [35/100], Train Loss: 0.1826, Val Loss: 0.1497, Train Acc: 0.9377, Val Acc: 0.9468\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/254], Loss: 0.0845, Accuracy: 0.9688\n",
      "Epoch [36/100], Step [100/254], Loss: 0.1192, Accuracy: 0.9531\n",
      "Epoch [36/100], Step [200/254], Loss: 0.2059, Accuracy: 0.9375\n",
      "Epoch [36/100], Train Loss: 0.1865, Val Loss: 0.1402, Train Acc: 0.9343, Val Acc: 0.9505\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/254], Loss: 0.1787, Accuracy: 0.9531\n",
      "Epoch [37/100], Step [100/254], Loss: 0.2443, Accuracy: 0.9219\n",
      "Epoch [37/100], Step [200/254], Loss: 0.1810, Accuracy: 0.9375\n",
      "Epoch [37/100], Train Loss: 0.1867, Val Loss: 0.1494, Train Acc: 0.9342, Val Acc: 0.9473\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/254], Loss: 0.1371, Accuracy: 0.9531\n",
      "Epoch [38/100], Step [100/254], Loss: 0.0943, Accuracy: 0.9531\n",
      "Epoch [38/100], Step [200/254], Loss: 0.2759, Accuracy: 0.9062\n",
      "Epoch [38/100], Train Loss: 0.1889, Val Loss: 0.1587, Train Acc: 0.9352, Val Acc: 0.9431\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/254], Loss: 0.1290, Accuracy: 0.9531\n",
      "Epoch [39/100], Step [100/254], Loss: 0.1253, Accuracy: 0.9219\n",
      "Epoch [39/100], Step [200/254], Loss: 0.1803, Accuracy: 0.9375\n",
      "Epoch [39/100], Train Loss: 0.1846, Val Loss: 0.1485, Train Acc: 0.9368, Val Acc: 0.9461\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/254], Loss: 0.1352, Accuracy: 0.9375\n",
      "Epoch [40/100], Step [100/254], Loss: 0.2105, Accuracy: 0.9531\n",
      "Epoch [40/100], Step [200/254], Loss: 0.1447, Accuracy: 0.9688\n",
      "Epoch [40/100], Train Loss: 0.1781, Val Loss: 0.1684, Train Acc: 0.9366, Val Acc: 0.9409\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/254], Loss: 0.2859, Accuracy: 0.8750\n",
      "Epoch [41/100], Step [100/254], Loss: 0.1058, Accuracy: 0.9844\n",
      "Epoch [41/100], Step [200/254], Loss: 0.1561, Accuracy: 0.9219\n",
      "Epoch [41/100], Train Loss: 0.1728, Val Loss: 0.1278, Train Acc: 0.9409, Val Acc: 0.9541\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/254], Loss: 0.1856, Accuracy: 0.9219\n",
      "Epoch [42/100], Step [100/254], Loss: 0.1944, Accuracy: 0.9531\n",
      "Epoch [42/100], Step [200/254], Loss: 0.0448, Accuracy: 0.9844\n",
      "Epoch [42/100], Train Loss: 0.1563, Val Loss: 0.1239, Train Acc: 0.9424, Val Acc: 0.9571\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/254], Loss: 0.0889, Accuracy: 0.9688\n",
      "Epoch [43/100], Step [100/254], Loss: 0.1099, Accuracy: 0.9688\n",
      "Epoch [43/100], Step [200/254], Loss: 0.2229, Accuracy: 0.9219\n",
      "Epoch [43/100], Train Loss: 0.1504, Val Loss: 0.1250, Train Acc: 0.9466, Val Acc: 0.9560\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/254], Loss: 0.0761, Accuracy: 0.9531\n",
      "Epoch [44/100], Step [100/254], Loss: 0.0979, Accuracy: 0.9531\n",
      "Epoch [44/100], Step [200/254], Loss: 0.0823, Accuracy: 0.9688\n",
      "Epoch [44/100], Train Loss: 0.1553, Val Loss: 0.1242, Train Acc: 0.9446, Val Acc: 0.9565\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/254], Loss: 0.1996, Accuracy: 0.9062\n",
      "Epoch [45/100], Step [100/254], Loss: 0.1933, Accuracy: 0.9219\n",
      "Epoch [45/100], Step [200/254], Loss: 0.0892, Accuracy: 0.9688\n",
      "Epoch [45/100], Train Loss: 0.1610, Val Loss: 0.1191, Train Acc: 0.9422, Val Acc: 0.9579\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/254], Loss: 0.2189, Accuracy: 0.8906\n",
      "Epoch [46/100], Step [100/254], Loss: 0.1678, Accuracy: 0.9375\n",
      "Epoch [46/100], Step [200/254], Loss: 0.3426, Accuracy: 0.9062\n",
      "Epoch [46/100], Train Loss: 0.1584, Val Loss: 0.1233, Train Acc: 0.9446, Val Acc: 0.9547\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/254], Loss: 0.1433, Accuracy: 0.9219\n",
      "Epoch [47/100], Step [100/254], Loss: 0.2061, Accuracy: 0.9219\n",
      "Epoch [47/100], Step [200/254], Loss: 0.1390, Accuracy: 0.9375\n",
      "Epoch [47/100], Train Loss: 0.1561, Val Loss: 0.1244, Train Acc: 0.9443, Val Acc: 0.9580\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/254], Loss: 0.2332, Accuracy: 0.9375\n",
      "Epoch [48/100], Step [100/254], Loss: 0.1005, Accuracy: 0.9531\n",
      "Epoch [48/100], Step [200/254], Loss: 0.1303, Accuracy: 0.9219\n",
      "Epoch [48/100], Train Loss: 0.1511, Val Loss: 0.1220, Train Acc: 0.9463, Val Acc: 0.9578\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/254], Loss: 0.0550, Accuracy: 0.9844\n",
      "Epoch [49/100], Step [100/254], Loss: 0.1345, Accuracy: 0.9375\n",
      "Epoch [49/100], Step [200/254], Loss: 0.0751, Accuracy: 0.9688\n",
      "Epoch [49/100], Train Loss: 0.1479, Val Loss: 0.1217, Train Acc: 0.9469, Val Acc: 0.9580\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/254], Loss: 0.2137, Accuracy: 0.9062\n",
      "Epoch [50/100], Step [100/254], Loss: 0.1067, Accuracy: 0.9375\n",
      "Epoch [50/100], Step [200/254], Loss: 0.0299, Accuracy: 0.9844\n",
      "Epoch [50/100], Train Loss: 0.1518, Val Loss: 0.1129, Train Acc: 0.9438, Val Acc: 0.9606\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/254], Loss: 0.1432, Accuracy: 0.9531\n",
      "Epoch [51/100], Step [100/254], Loss: 0.2848, Accuracy: 0.8906\n",
      "Epoch [51/100], Step [200/254], Loss: 0.0530, Accuracy: 0.9844\n",
      "Epoch [51/100], Train Loss: 0.1482, Val Loss: 0.1159, Train Acc: 0.9469, Val Acc: 0.9602\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/254], Loss: 0.2129, Accuracy: 0.8750\n",
      "Epoch [52/100], Step [100/254], Loss: 0.2478, Accuracy: 0.9062\n",
      "Epoch [52/100], Step [200/254], Loss: 0.1739, Accuracy: 0.9375\n",
      "Epoch [52/100], Train Loss: 0.1487, Val Loss: 0.1187, Train Acc: 0.9467, Val Acc: 0.9593\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/254], Loss: 0.2239, Accuracy: 0.8906\n",
      "Epoch [53/100], Step [100/254], Loss: 0.1473, Accuracy: 0.9688\n",
      "Epoch [53/100], Step [200/254], Loss: 0.1871, Accuracy: 0.9219\n",
      "Epoch [53/100], Train Loss: 0.1475, Val Loss: 0.1175, Train Acc: 0.9468, Val Acc: 0.9582\n",
      "==================================================\n",
      "Epoch [54/100], Step [0/254], Loss: 0.1154, Accuracy: 0.9531\n",
      "Epoch [54/100], Step [100/254], Loss: 0.1435, Accuracy: 0.9375\n",
      "Epoch [54/100], Step [200/254], Loss: 0.0424, Accuracy: 0.9844\n",
      "Epoch [54/100], Train Loss: 0.1547, Val Loss: 0.1185, Train Acc: 0.9459, Val Acc: 0.9574\n",
      "==================================================\n",
      "Epoch [55/100], Step [0/254], Loss: 0.0862, Accuracy: 0.9688\n",
      "Epoch [55/100], Step [100/254], Loss: 0.0724, Accuracy: 0.9688\n",
      "Epoch [55/100], Step [200/254], Loss: 0.0964, Accuracy: 0.9531\n",
      "Epoch [55/100], Train Loss: 0.1553, Val Loss: 0.1201, Train Acc: 0.9440, Val Acc: 0.9570\n",
      "==================================================\n",
      "Epoch [56/100], Step [0/254], Loss: 0.1451, Accuracy: 0.9688\n",
      "Epoch [56/100], Step [100/254], Loss: 0.2111, Accuracy: 0.9531\n",
      "Epoch [56/100], Step [200/254], Loss: 0.1764, Accuracy: 0.9688\n",
      "Epoch [56/100], Train Loss: 0.1468, Val Loss: 0.1155, Train Acc: 0.9472, Val Acc: 0.9581\n",
      "==================================================\n",
      "Epoch [57/100], Step [0/254], Loss: 0.2566, Accuracy: 0.8906\n",
      "Epoch [57/100], Step [100/254], Loss: 0.0826, Accuracy: 0.9688\n",
      "Epoch [57/100], Step [200/254], Loss: 0.2190, Accuracy: 0.9062\n",
      "Epoch [57/100], Train Loss: 0.1497, Val Loss: 0.1181, Train Acc: 0.9454, Val Acc: 0.9581\n",
      "==================================================\n",
      "Epoch [58/100], Step [0/254], Loss: 0.2472, Accuracy: 0.9375\n",
      "Epoch [58/100], Step [100/254], Loss: 0.1110, Accuracy: 0.9688\n",
      "Epoch [58/100], Step [200/254], Loss: 0.1024, Accuracy: 0.9688\n",
      "Epoch [58/100], Train Loss: 0.1516, Val Loss: 0.1182, Train Acc: 0.9479, Val Acc: 0.9568\n",
      "==================================================\n",
      "Epoch [59/100], Step [0/254], Loss: 0.1116, Accuracy: 0.9531\n",
      "Epoch [59/100], Step [100/254], Loss: 0.0973, Accuracy: 0.9688\n",
      "Epoch [59/100], Step [200/254], Loss: 0.0976, Accuracy: 0.9844\n",
      "Epoch [59/100], Train Loss: 0.1509, Val Loss: 0.1171, Train Acc: 0.9468, Val Acc: 0.9590\n",
      "==================================================\n",
      "Epoch [60/100], Step [0/254], Loss: 0.1028, Accuracy: 0.9688\n",
      "Epoch [60/100], Step [100/254], Loss: 0.2272, Accuracy: 0.9375\n",
      "Epoch [60/100], Step [200/254], Loss: 0.1147, Accuracy: 0.9531\n",
      "Epoch [60/100], Train Loss: 0.1523, Val Loss: 0.1167, Train Acc: 0.9460, Val Acc: 0.9587\n",
      "==================================================\n",
      "Epoch [61/100], Step [0/254], Loss: 0.2726, Accuracy: 0.9219\n",
      "Epoch [61/100], Step [100/254], Loss: 0.0930, Accuracy: 0.9688\n",
      "Epoch [61/100], Step [200/254], Loss: 0.2301, Accuracy: 0.9062\n",
      "Epoch [61/100], Train Loss: 0.1547, Val Loss: 0.1173, Train Acc: 0.9462, Val Acc: 0.9585\n",
      "==================================================\n",
      "CPU times: user 8h 36min 45s, sys: 17min 35s, total: 8h 54min 20s\n",
      "Wall time: 1h 8min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.1523405620901603,\n",
       "  0.8528436603039269,\n",
       "  0.7243547123952174,\n",
       "  0.6624058939105882,\n",
       "  0.6065422158072314,\n",
       "  0.5869444091254332,\n",
       "  0.5196047922757667,\n",
       "  0.4882235464265966,\n",
       "  0.46634577637112984,\n",
       "  0.4597809772322497,\n",
       "  0.4226906250311634,\n",
       "  0.40679400208897476,\n",
       "  0.40730571799625565,\n",
       "  0.38122993769256147,\n",
       "  0.3610886536597267,\n",
       "  0.3513236552418217,\n",
       "  0.3449224378532312,\n",
       "  0.32667241016710835,\n",
       "  0.3304662373298266,\n",
       "  0.3091287999523906,\n",
       "  0.3059096384764187,\n",
       "  0.30573234463653226,\n",
       "  0.29063769799517836,\n",
       "  0.28319622640769315,\n",
       "  0.2244520227564132,\n",
       "  0.2135209942925868,\n",
       "  0.21493461653296872,\n",
       "  0.20475120440594793,\n",
       "  0.19725297689144536,\n",
       "  0.1966512026485261,\n",
       "  0.19234007711368284,\n",
       "  0.18817698360076102,\n",
       "  0.19185450356306993,\n",
       "  0.188143566720129,\n",
       "  0.18263006295392833,\n",
       "  0.18654716529656112,\n",
       "  0.18669561047370978,\n",
       "  0.18891476594850304,\n",
       "  0.18456312226439556,\n",
       "  0.1781245426634165,\n",
       "  0.17278508139466206,\n",
       "  0.1562827003562427,\n",
       "  0.15040491052030577,\n",
       "  0.15533151549441138,\n",
       "  0.1610124783121108,\n",
       "  0.15838789070157086,\n",
       "  0.15614464194110528,\n",
       "  0.15108333668840213,\n",
       "  0.14790296289101829,\n",
       "  0.1518420387704776,\n",
       "  0.14823830968106355,\n",
       "  0.14865659626831454,\n",
       "  0.147497669280338,\n",
       "  0.1547110711965035,\n",
       "  0.15529739044196023,\n",
       "  0.14679182171234934,\n",
       "  0.14967961074036407,\n",
       "  0.15159030920114575,\n",
       "  0.15085371870063538,\n",
       "  0.15227809022584066,\n",
       "  0.1546817727489617],\n",
       " [tensor(2.8583, device='cuda:0'),\n",
       "  tensor(1.6611, device='cuda:0'),\n",
       "  tensor(0.7937, device='cuda:0'),\n",
       "  tensor(0.6784, device='cuda:0'),\n",
       "  tensor(0.7859, device='cuda:0'),\n",
       "  tensor(0.7892, device='cuda:0'),\n",
       "  tensor(0.6009, device='cuda:0'),\n",
       "  tensor(0.5917, device='cuda:0'),\n",
       "  tensor(0.6509, device='cuda:0'),\n",
       "  tensor(0.4792, device='cuda:0'),\n",
       "  tensor(0.6088, device='cuda:0'),\n",
       "  tensor(0.4437, device='cuda:0'),\n",
       "  tensor(0.5371, device='cuda:0'),\n",
       "  tensor(0.7344, device='cuda:0'),\n",
       "  tensor(0.4067, device='cuda:0'),\n",
       "  tensor(0.3327, device='cuda:0'),\n",
       "  tensor(0.3201, device='cuda:0'),\n",
       "  tensor(0.4741, device='cuda:0'),\n",
       "  tensor(0.4552, device='cuda:0'),\n",
       "  tensor(0.3103, device='cuda:0'),\n",
       "  tensor(0.3612, device='cuda:0'),\n",
       "  tensor(0.3903, device='cuda:0'),\n",
       "  tensor(0.6601, device='cuda:0'),\n",
       "  tensor(0.3603, device='cuda:0'),\n",
       "  tensor(0.2460, device='cuda:0'),\n",
       "  tensor(0.1791, device='cuda:0'),\n",
       "  tensor(0.1933, device='cuda:0'),\n",
       "  tensor(0.1683, device='cuda:0'),\n",
       "  tensor(0.1564, device='cuda:0'),\n",
       "  tensor(0.1684, device='cuda:0'),\n",
       "  tensor(0.1591, device='cuda:0'),\n",
       "  tensor(0.1519, device='cuda:0'),\n",
       "  tensor(0.1620, device='cuda:0'),\n",
       "  tensor(0.1756, device='cuda:0'),\n",
       "  tensor(0.1497, device='cuda:0'),\n",
       "  tensor(0.1402, device='cuda:0'),\n",
       "  tensor(0.1494, device='cuda:0'),\n",
       "  tensor(0.1587, device='cuda:0'),\n",
       "  tensor(0.1485, device='cuda:0'),\n",
       "  tensor(0.1684, device='cuda:0'),\n",
       "  tensor(0.1278, device='cuda:0'),\n",
       "  tensor(0.1239, device='cuda:0'),\n",
       "  tensor(0.1250, device='cuda:0'),\n",
       "  tensor(0.1242, device='cuda:0'),\n",
       "  tensor(0.1191, device='cuda:0'),\n",
       "  tensor(0.1233, device='cuda:0'),\n",
       "  tensor(0.1244, device='cuda:0'),\n",
       "  tensor(0.1220, device='cuda:0'),\n",
       "  tensor(0.1217, device='cuda:0'),\n",
       "  tensor(0.1129, device='cuda:0'),\n",
       "  tensor(0.1159, device='cuda:0'),\n",
       "  tensor(0.1187, device='cuda:0'),\n",
       "  tensor(0.1175, device='cuda:0'),\n",
       "  tensor(0.1185, device='cuda:0'),\n",
       "  tensor(0.1201, device='cuda:0'),\n",
       "  tensor(0.1155, device='cuda:0'),\n",
       "  tensor(0.1181, device='cuda:0'),\n",
       "  tensor(0.1182, device='cuda:0'),\n",
       "  tensor(0.1171, device='cuda:0'),\n",
       "  tensor(0.1167, device='cuda:0'),\n",
       "  tensor(0.1173, device='cuda:0')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3Gelu(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 966 ms, total: 27.6 s\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predGelu, actualGelu = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9439\n"
     ]
    }
   ],
   "source": [
    "geluAcc = (torch.argmax(predGelu, dim=1) == actualGelu).float().mean()\n",
    "print(f\"Test Accuracy: {geluAcc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using ELU instead of hardswish in bottleneck block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ELU\n",
    "\n",
    "\n",
    "class MobileNetV3Elu(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"small\":\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, ELU, 2],\n",
    "                [5, 240, 40, True, ELU, 1],\n",
    "                [5, 240, 40, True, ELU, 1],\n",
    "                [5, 120, 48, True, ELU, 1],\n",
    "                [5, 144, 48, True, ELU, 1],\n",
    "                [5, 288, 96, True, ELU, 2],\n",
    "                [5, 576, 96, True, ELU, 1],\n",
    "                [5, 576, 96, True, ELU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "\n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/254], Loss: 2.3026, Accuracy: 0.1562\n",
      "Epoch [1/100], Step [100/254], Loss: 1.0046, Accuracy: 0.5938\n",
      "Epoch [1/100], Step [200/254], Loss: 0.8198, Accuracy: 0.6875\n",
      "Epoch [1/100], Train Loss: 1.1276, Val Loss: 3.1147, Train Acc: 0.5912, Val Acc: 0.1106\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 0.9759, Accuracy: 0.6250\n",
      "Epoch [2/100], Step [100/254], Loss: 1.4649, Accuracy: 0.5469\n",
      "Epoch [2/100], Step [200/254], Loss: 1.0314, Accuracy: 0.6406\n",
      "Epoch [2/100], Train Loss: 0.8356, Val Loss: 1.7087, Train Acc: 0.6992, Val Acc: 0.4824\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 0.7938, Accuracy: 0.7344\n",
      "Epoch [3/100], Step [100/254], Loss: 0.6103, Accuracy: 0.7500\n",
      "Epoch [3/100], Step [200/254], Loss: 0.4519, Accuracy: 0.8281\n",
      "Epoch [3/100], Train Loss: 0.7434, Val Loss: 1.0834, Train Acc: 0.7379, Val Acc: 0.6596\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 0.5767, Accuracy: 0.8125\n",
      "Epoch [4/100], Step [100/254], Loss: 0.9586, Accuracy: 0.7188\n",
      "Epoch [4/100], Step [200/254], Loss: 0.7311, Accuracy: 0.7656\n",
      "Epoch [4/100], Train Loss: 0.6693, Val Loss: 1.0531, Train Acc: 0.7653, Val Acc: 0.6556\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 0.8296, Accuracy: 0.7188\n",
      "Epoch [5/100], Step [100/254], Loss: 0.7361, Accuracy: 0.7344\n",
      "Epoch [5/100], Step [200/254], Loss: 0.4659, Accuracy: 0.8438\n",
      "Epoch [5/100], Train Loss: 0.6028, Val Loss: 0.7604, Train Acc: 0.7834, Val Acc: 0.7393\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/254], Loss: 0.5217, Accuracy: 0.8125\n",
      "Epoch [6/100], Step [100/254], Loss: 0.5592, Accuracy: 0.8125\n",
      "Epoch [6/100], Step [200/254], Loss: 0.4287, Accuracy: 0.8906\n",
      "Epoch [6/100], Train Loss: 0.5647, Val Loss: 0.5888, Train Acc: 0.8001, Val Acc: 0.7962\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/254], Loss: 0.6953, Accuracy: 0.7969\n",
      "Epoch [7/100], Step [100/254], Loss: 0.4617, Accuracy: 0.8281\n",
      "Epoch [7/100], Step [200/254], Loss: 0.5094, Accuracy: 0.7969\n",
      "Epoch [7/100], Train Loss: 0.5212, Val Loss: 0.6278, Train Acc: 0.8157, Val Acc: 0.7937\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/254], Loss: 0.6283, Accuracy: 0.8438\n",
      "Epoch [8/100], Step [100/254], Loss: 0.5770, Accuracy: 0.8281\n",
      "Epoch [8/100], Step [200/254], Loss: 0.5153, Accuracy: 0.8594\n",
      "Epoch [8/100], Train Loss: 0.4743, Val Loss: 0.5605, Train Acc: 0.8380, Val Acc: 0.8068\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/254], Loss: 0.5060, Accuracy: 0.7969\n",
      "Epoch [9/100], Step [100/254], Loss: 0.2789, Accuracy: 0.8594\n",
      "Epoch [9/100], Step [200/254], Loss: 0.4437, Accuracy: 0.8438\n",
      "Epoch [9/100], Train Loss: 0.4510, Val Loss: 0.4394, Train Acc: 0.8434, Val Acc: 0.8446\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/254], Loss: 0.4094, Accuracy: 0.8125\n",
      "Epoch [10/100], Step [100/254], Loss: 0.3120, Accuracy: 0.9375\n",
      "Epoch [10/100], Step [200/254], Loss: 0.2822, Accuracy: 0.9062\n",
      "Epoch [10/100], Train Loss: 0.4179, Val Loss: 0.5336, Train Acc: 0.8596, Val Acc: 0.8288\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/254], Loss: 0.2802, Accuracy: 0.8906\n",
      "Epoch [11/100], Step [100/254], Loss: 0.3373, Accuracy: 0.8906\n",
      "Epoch [11/100], Step [200/254], Loss: 0.3963, Accuracy: 0.8438\n",
      "Epoch [11/100], Train Loss: 0.4081, Val Loss: 0.5619, Train Acc: 0.8615, Val Acc: 0.7985\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/254], Loss: 0.3815, Accuracy: 0.8438\n",
      "Epoch [12/100], Step [100/254], Loss: 0.3258, Accuracy: 0.8906\n",
      "Epoch [12/100], Step [200/254], Loss: 0.3608, Accuracy: 0.8594\n",
      "Epoch [12/100], Train Loss: 0.3945, Val Loss: 0.5381, Train Acc: 0.8637, Val Acc: 0.8190\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/254], Loss: 0.3663, Accuracy: 0.8750\n",
      "Epoch [13/100], Step [100/254], Loss: 0.3817, Accuracy: 0.8906\n",
      "Epoch [13/100], Step [200/254], Loss: 0.3322, Accuracy: 0.8906\n",
      "Epoch [13/100], Train Loss: 0.3777, Val Loss: 0.4580, Train Acc: 0.8730, Val Acc: 0.8478\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/254], Loss: 0.2521, Accuracy: 0.9375\n",
      "Epoch [14/100], Step [100/254], Loss: 0.2805, Accuracy: 0.9062\n",
      "Epoch [14/100], Step [200/254], Loss: 0.2280, Accuracy: 0.9219\n",
      "Epoch [14/100], Train Loss: 0.3112, Val Loss: 0.2500, Train Acc: 0.8943, Val Acc: 0.9135\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/254], Loss: 0.4481, Accuracy: 0.8906\n",
      "Epoch [15/100], Step [100/254], Loss: 0.2330, Accuracy: 0.9219\n",
      "Epoch [15/100], Step [200/254], Loss: 0.2712, Accuracy: 0.9219\n",
      "Epoch [15/100], Train Loss: 0.3001, Val Loss: 0.2638, Train Acc: 0.8958, Val Acc: 0.9069\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/254], Loss: 0.2440, Accuracy: 0.9375\n",
      "Epoch [16/100], Step [100/254], Loss: 0.3498, Accuracy: 0.8906\n",
      "Epoch [16/100], Step [200/254], Loss: 0.2988, Accuracy: 0.9062\n",
      "Epoch [16/100], Train Loss: 0.2807, Val Loss: 0.2351, Train Acc: 0.9021, Val Acc: 0.9195\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/254], Loss: 0.3059, Accuracy: 0.9219\n",
      "Epoch [17/100], Step [100/254], Loss: 0.2282, Accuracy: 0.8906\n",
      "Epoch [17/100], Step [200/254], Loss: 0.2471, Accuracy: 0.8906\n",
      "Epoch [17/100], Train Loss: 0.2751, Val Loss: 0.2478, Train Acc: 0.9035, Val Acc: 0.9123\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/254], Loss: 0.3484, Accuracy: 0.9219\n",
      "Epoch [18/100], Step [100/254], Loss: 0.2035, Accuracy: 0.9219\n",
      "Epoch [18/100], Step [200/254], Loss: 0.2176, Accuracy: 0.9375\n",
      "Epoch [18/100], Train Loss: 0.2804, Val Loss: 0.2319, Train Acc: 0.9013, Val Acc: 0.9183\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/254], Loss: 0.2558, Accuracy: 0.9219\n",
      "Epoch [19/100], Step [100/254], Loss: 0.3664, Accuracy: 0.8906\n",
      "Epoch [19/100], Step [200/254], Loss: 0.2160, Accuracy: 0.9062\n",
      "Epoch [19/100], Train Loss: 0.2740, Val Loss: 0.2225, Train Acc: 0.9025, Val Acc: 0.9215\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/254], Loss: 0.3165, Accuracy: 0.8750\n",
      "Epoch [20/100], Step [100/254], Loss: 0.2791, Accuracy: 0.8906\n",
      "Epoch [20/100], Step [200/254], Loss: 0.1200, Accuracy: 0.9688\n",
      "Epoch [20/100], Train Loss: 0.2654, Val Loss: 0.2288, Train Acc: 0.9073, Val Acc: 0.9185\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/254], Loss: 0.2881, Accuracy: 0.8906\n",
      "Epoch [21/100], Step [100/254], Loss: 0.3025, Accuracy: 0.9062\n",
      "Epoch [21/100], Step [200/254], Loss: 0.3202, Accuracy: 0.9219\n",
      "Epoch [21/100], Train Loss: 0.2607, Val Loss: 0.2159, Train Acc: 0.9067, Val Acc: 0.9236\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/254], Loss: 0.2772, Accuracy: 0.9219\n",
      "Epoch [22/100], Step [100/254], Loss: 0.2627, Accuracy: 0.9062\n",
      "Epoch [22/100], Step [200/254], Loss: 0.1807, Accuracy: 0.9062\n",
      "Epoch [22/100], Train Loss: 0.2556, Val Loss: 0.2313, Train Acc: 0.9099, Val Acc: 0.9178\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/254], Loss: 0.5445, Accuracy: 0.7969\n",
      "Epoch [23/100], Step [100/254], Loss: 0.3100, Accuracy: 0.8906\n",
      "Epoch [23/100], Step [200/254], Loss: 0.3543, Accuracy: 0.9062\n",
      "Epoch [23/100], Train Loss: 0.2612, Val Loss: 0.2304, Train Acc: 0.9070, Val Acc: 0.9196\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/254], Loss: 0.3858, Accuracy: 0.8281\n",
      "Epoch [24/100], Step [100/254], Loss: 0.2014, Accuracy: 0.9531\n",
      "Epoch [24/100], Step [200/254], Loss: 0.2538, Accuracy: 0.9219\n",
      "Epoch [24/100], Train Loss: 0.2577, Val Loss: 0.2372, Train Acc: 0.9083, Val Acc: 0.9176\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/254], Loss: 0.2448, Accuracy: 0.9375\n",
      "Epoch [25/100], Step [100/254], Loss: 0.3436, Accuracy: 0.9062\n",
      "Epoch [25/100], Step [200/254], Loss: 0.2473, Accuracy: 0.9062\n",
      "Epoch [25/100], Train Loss: 0.2528, Val Loss: 0.2162, Train Acc: 0.9115, Val Acc: 0.9231\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/254], Loss: 0.2430, Accuracy: 0.9219\n",
      "Epoch [26/100], Step [100/254], Loss: 0.0581, Accuracy: 1.0000\n",
      "Epoch [26/100], Step [200/254], Loss: 0.2171, Accuracy: 0.9531\n",
      "Epoch [26/100], Train Loss: 0.2399, Val Loss: 0.1912, Train Acc: 0.9144, Val Acc: 0.9336\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/254], Loss: 0.2231, Accuracy: 0.9219\n",
      "Epoch [27/100], Step [100/254], Loss: 0.4454, Accuracy: 0.8906\n",
      "Epoch [27/100], Step [200/254], Loss: 0.3172, Accuracy: 0.9062\n",
      "Epoch [27/100], Train Loss: 0.2353, Val Loss: 0.1873, Train Acc: 0.9174, Val Acc: 0.9349\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/254], Loss: 0.3073, Accuracy: 0.8750\n",
      "Epoch [28/100], Step [100/254], Loss: 0.3295, Accuracy: 0.9219\n",
      "Epoch [28/100], Step [200/254], Loss: 0.2047, Accuracy: 0.9062\n",
      "Epoch [28/100], Train Loss: 0.2259, Val Loss: 0.1887, Train Acc: 0.9184, Val Acc: 0.9331\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/254], Loss: 0.2770, Accuracy: 0.9375\n",
      "Epoch [29/100], Step [100/254], Loss: 0.2243, Accuracy: 0.9062\n",
      "Epoch [29/100], Step [200/254], Loss: 0.3235, Accuracy: 0.8750\n",
      "Epoch [29/100], Train Loss: 0.2260, Val Loss: 0.1860, Train Acc: 0.9184, Val Acc: 0.9341\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/254], Loss: 0.3043, Accuracy: 0.8906\n",
      "Epoch [30/100], Step [100/254], Loss: 0.2029, Accuracy: 0.9219\n",
      "Epoch [30/100], Step [200/254], Loss: 0.2240, Accuracy: 0.9062\n",
      "Epoch [30/100], Train Loss: 0.2315, Val Loss: 0.1868, Train Acc: 0.9174, Val Acc: 0.9336\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/254], Loss: 0.0695, Accuracy: 1.0000\n",
      "Epoch [31/100], Step [100/254], Loss: 0.1392, Accuracy: 0.9375\n",
      "Epoch [31/100], Step [200/254], Loss: 0.2717, Accuracy: 0.8906\n",
      "Epoch [31/100], Train Loss: 0.2387, Val Loss: 0.1869, Train Acc: 0.9178, Val Acc: 0.9344\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/254], Loss: 0.3027, Accuracy: 0.9062\n",
      "Epoch [32/100], Step [100/254], Loss: 0.1807, Accuracy: 0.9219\n",
      "Epoch [32/100], Step [200/254], Loss: 0.3334, Accuracy: 0.8906\n",
      "Epoch [32/100], Train Loss: 0.2218, Val Loss: 0.1829, Train Acc: 0.9208, Val Acc: 0.9334\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/254], Loss: 0.2241, Accuracy: 0.9375\n",
      "Epoch [33/100], Step [100/254], Loss: 0.1450, Accuracy: 0.9531\n",
      "Epoch [33/100], Step [200/254], Loss: 0.2340, Accuracy: 0.9219\n",
      "Epoch [33/100], Train Loss: 0.2255, Val Loss: 0.1867, Train Acc: 0.9181, Val Acc: 0.9337\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/254], Loss: 0.1935, Accuracy: 0.9062\n",
      "Epoch [34/100], Step [100/254], Loss: 0.1713, Accuracy: 0.9375\n",
      "Epoch [34/100], Step [200/254], Loss: 0.3731, Accuracy: 0.8906\n",
      "Epoch [34/100], Train Loss: 0.2179, Val Loss: 0.1743, Train Acc: 0.9220, Val Acc: 0.9372\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/254], Loss: 0.2106, Accuracy: 0.9062\n",
      "Epoch [35/100], Step [100/254], Loss: 0.2122, Accuracy: 0.8906\n",
      "Epoch [35/100], Step [200/254], Loss: 0.1858, Accuracy: 0.9375\n",
      "Epoch [35/100], Train Loss: 0.2256, Val Loss: 0.1809, Train Acc: 0.9172, Val Acc: 0.9360\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/254], Loss: 0.1696, Accuracy: 0.9531\n",
      "Epoch [36/100], Step [100/254], Loss: 0.2785, Accuracy: 0.8750\n",
      "Epoch [36/100], Step [200/254], Loss: 0.1288, Accuracy: 0.9375\n",
      "Epoch [36/100], Train Loss: 0.2230, Val Loss: 0.1855, Train Acc: 0.9208, Val Acc: 0.9354\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/254], Loss: 0.3428, Accuracy: 0.8906\n",
      "Epoch [37/100], Step [100/254], Loss: 0.2328, Accuracy: 0.9062\n",
      "Epoch [37/100], Step [200/254], Loss: 0.4557, Accuracy: 0.8594\n",
      "Epoch [37/100], Train Loss: 0.2233, Val Loss: 0.1824, Train Acc: 0.9198, Val Acc: 0.9357\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/254], Loss: 0.1314, Accuracy: 0.9531\n",
      "Epoch [38/100], Step [100/254], Loss: 0.4744, Accuracy: 0.8750\n",
      "Epoch [38/100], Step [200/254], Loss: 0.3203, Accuracy: 0.9062\n",
      "Epoch [38/100], Train Loss: 0.2145, Val Loss: 0.1815, Train Acc: 0.9224, Val Acc: 0.9362\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/254], Loss: 0.1807, Accuracy: 0.9688\n",
      "Epoch [39/100], Step [100/254], Loss: 0.2312, Accuracy: 0.9375\n",
      "Epoch [39/100], Step [200/254], Loss: 0.1459, Accuracy: 0.9531\n",
      "Epoch [39/100], Train Loss: 0.2137, Val Loss: 0.1795, Train Acc: 0.9214, Val Acc: 0.9365\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/254], Loss: 0.1725, Accuracy: 0.9375\n",
      "Epoch [40/100], Step [100/254], Loss: 0.2536, Accuracy: 0.9219\n",
      "Epoch [40/100], Step [200/254], Loss: 0.0763, Accuracy: 0.9844\n",
      "Epoch [40/100], Train Loss: 0.2136, Val Loss: 0.1759, Train Acc: 0.9227, Val Acc: 0.9364\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/254], Loss: 0.2323, Accuracy: 0.9219\n",
      "Epoch [41/100], Step [100/254], Loss: 0.2514, Accuracy: 0.9062\n",
      "Epoch [41/100], Step [200/254], Loss: 0.2117, Accuracy: 0.9062\n",
      "Epoch [41/100], Train Loss: 0.2180, Val Loss: 0.1741, Train Acc: 0.9198, Val Acc: 0.9382\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/254], Loss: 0.2485, Accuracy: 0.8906\n",
      "Epoch [42/100], Step [100/254], Loss: 0.2748, Accuracy: 0.8906\n",
      "Epoch [42/100], Step [200/254], Loss: 0.1262, Accuracy: 0.9375\n",
      "Epoch [42/100], Train Loss: 0.2099, Val Loss: 0.1730, Train Acc: 0.9251, Val Acc: 0.9386\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/254], Loss: 0.3077, Accuracy: 0.9062\n",
      "Epoch [43/100], Step [100/254], Loss: 0.1747, Accuracy: 0.9375\n",
      "Epoch [43/100], Step [200/254], Loss: 0.0947, Accuracy: 0.9531\n",
      "Epoch [43/100], Train Loss: 0.2092, Val Loss: 0.1766, Train Acc: 0.9251, Val Acc: 0.9376\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/254], Loss: 0.2116, Accuracy: 0.9375\n",
      "Epoch [44/100], Step [100/254], Loss: 0.3834, Accuracy: 0.8906\n",
      "Epoch [44/100], Step [200/254], Loss: 0.2098, Accuracy: 0.9062\n",
      "Epoch [44/100], Train Loss: 0.2184, Val Loss: 0.1772, Train Acc: 0.9225, Val Acc: 0.9359\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/254], Loss: 0.2567, Accuracy: 0.8750\n",
      "Epoch [45/100], Step [100/254], Loss: 0.2057, Accuracy: 0.9062\n",
      "Epoch [45/100], Step [200/254], Loss: 0.2504, Accuracy: 0.9531\n",
      "Epoch [45/100], Train Loss: 0.2104, Val Loss: 0.1765, Train Acc: 0.9249, Val Acc: 0.9365\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/254], Loss: 0.1762, Accuracy: 0.9375\n",
      "Epoch [46/100], Step [100/254], Loss: 0.2486, Accuracy: 0.9062\n",
      "Epoch [46/100], Step [200/254], Loss: 0.2509, Accuracy: 0.8594\n",
      "Epoch [46/100], Train Loss: 0.2138, Val Loss: 0.1770, Train Acc: 0.9216, Val Acc: 0.9371\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/254], Loss: 0.2116, Accuracy: 0.9219\n",
      "Epoch [47/100], Step [100/254], Loss: 0.1887, Accuracy: 0.9531\n",
      "Epoch [47/100], Step [200/254], Loss: 0.2195, Accuracy: 0.9062\n",
      "Epoch [47/100], Train Loss: 0.2173, Val Loss: 0.1761, Train Acc: 0.9217, Val Acc: 0.9373\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/254], Loss: 0.4047, Accuracy: 0.9062\n",
      "Epoch [48/100], Step [100/254], Loss: 0.1771, Accuracy: 0.9219\n",
      "Epoch [48/100], Step [200/254], Loss: 0.1803, Accuracy: 0.9062\n",
      "Epoch [48/100], Train Loss: 0.2142, Val Loss: 0.1748, Train Acc: 0.9227, Val Acc: 0.9375\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/254], Loss: 0.1711, Accuracy: 0.9219\n",
      "Epoch [49/100], Step [100/254], Loss: 0.2249, Accuracy: 0.9219\n",
      "Epoch [49/100], Step [200/254], Loss: 0.0999, Accuracy: 0.9688\n",
      "Epoch [49/100], Train Loss: 0.2234, Val Loss: 0.1757, Train Acc: 0.9232, Val Acc: 0.9366\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/254], Loss: 0.1709, Accuracy: 0.9375\n",
      "Epoch [50/100], Step [100/254], Loss: 0.1206, Accuracy: 0.9375\n",
      "Epoch [50/100], Step [200/254], Loss: 0.2360, Accuracy: 0.9219\n",
      "Epoch [50/100], Train Loss: 0.2180, Val Loss: 0.1756, Train Acc: 0.9235, Val Acc: 0.9388\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/254], Loss: 0.2838, Accuracy: 0.9219\n",
      "Epoch [51/100], Step [100/254], Loss: 0.2610, Accuracy: 0.9219\n",
      "Epoch [51/100], Step [200/254], Loss: 0.3355, Accuracy: 0.8750\n",
      "Epoch [51/100], Train Loss: 0.2124, Val Loss: 0.1732, Train Acc: 0.9256, Val Acc: 0.9385\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/254], Loss: 0.2986, Accuracy: 0.8906\n",
      "Epoch [52/100], Step [100/254], Loss: 0.2422, Accuracy: 0.8906\n",
      "Epoch [52/100], Step [200/254], Loss: 0.3337, Accuracy: 0.8906\n",
      "Epoch [52/100], Train Loss: 0.2123, Val Loss: 0.1761, Train Acc: 0.9257, Val Acc: 0.9381\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/254], Loss: 0.3064, Accuracy: 0.9062\n",
      "Epoch [53/100], Step [100/254], Loss: 0.1315, Accuracy: 0.9375\n",
      "Epoch [53/100], Step [200/254], Loss: 0.1582, Accuracy: 0.9219\n",
      "Epoch [53/100], Train Loss: 0.2164, Val Loss: 0.1750, Train Acc: 0.9211, Val Acc: 0.9364\n",
      "==================================================\n",
      "CPU times: user 7h 49min 12s, sys: 15min 55s, total: 8h 5min 7s\n",
      "Wall time: 1h 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.1276405303497015,\n",
       "  0.8355774374928061,\n",
       "  0.7433535173887343,\n",
       "  0.6692954608657229,\n",
       "  0.6028224855192065,\n",
       "  0.5647461573908649,\n",
       "  0.5212455495722651,\n",
       "  0.4743181393723788,\n",
       "  0.45097026387302896,\n",
       "  0.41794770081916194,\n",
       "  0.40809714277898235,\n",
       "  0.3944704887552524,\n",
       "  0.3776831647425186,\n",
       "  0.3112283064213794,\n",
       "  0.300114090077755,\n",
       "  0.280710178214734,\n",
       "  0.2751382489314699,\n",
       "  0.2804053719412154,\n",
       "  0.2740256746453563,\n",
       "  0.2653665639988081,\n",
       "  0.26069572562073157,\n",
       "  0.2555849635166915,\n",
       "  0.26115888826490385,\n",
       "  0.25767784267957283,\n",
       "  0.25278246625671236,\n",
       "  0.23992718017007422,\n",
       "  0.23527461997284663,\n",
       "  0.2259450173812119,\n",
       "  0.2260283733036105,\n",
       "  0.23145089270620364,\n",
       "  0.2387224342674017,\n",
       "  0.22182327922521614,\n",
       "  0.225536120087495,\n",
       "  0.2179368640494159,\n",
       "  0.2256453996744212,\n",
       "  0.22298474796116352,\n",
       "  0.22325870187909114,\n",
       "  0.21448368212486815,\n",
       "  0.21374717787436143,\n",
       "  0.21357419356469096,\n",
       "  0.21802236224840008,\n",
       "  0.20992092940751023,\n",
       "  0.20918118045848655,\n",
       "  0.21844068449723908,\n",
       "  0.2103805969373917,\n",
       "  0.21384724717968562,\n",
       "  0.2173093966113066,\n",
       "  0.21417737524868466,\n",
       "  0.22340706609717503,\n",
       "  0.2179886461712244,\n",
       "  0.21242324204191448,\n",
       "  0.21231844683918427,\n",
       "  0.21639402559775067],\n",
       " [tensor(3.1147, device='cuda:0'),\n",
       "  tensor(1.7087, device='cuda:0'),\n",
       "  tensor(1.0834, device='cuda:0'),\n",
       "  tensor(1.0531, device='cuda:0'),\n",
       "  tensor(0.7604, device='cuda:0'),\n",
       "  tensor(0.5888, device='cuda:0'),\n",
       "  tensor(0.6278, device='cuda:0'),\n",
       "  tensor(0.5605, device='cuda:0'),\n",
       "  tensor(0.4394, device='cuda:0'),\n",
       "  tensor(0.5336, device='cuda:0'),\n",
       "  tensor(0.5619, device='cuda:0'),\n",
       "  tensor(0.5381, device='cuda:0'),\n",
       "  tensor(0.4580, device='cuda:0'),\n",
       "  tensor(0.2500, device='cuda:0'),\n",
       "  tensor(0.2638, device='cuda:0'),\n",
       "  tensor(0.2351, device='cuda:0'),\n",
       "  tensor(0.2478, device='cuda:0'),\n",
       "  tensor(0.2319, device='cuda:0'),\n",
       "  tensor(0.2225, device='cuda:0'),\n",
       "  tensor(0.2288, device='cuda:0'),\n",
       "  tensor(0.2159, device='cuda:0'),\n",
       "  tensor(0.2313, device='cuda:0'),\n",
       "  tensor(0.2304, device='cuda:0'),\n",
       "  tensor(0.2372, device='cuda:0'),\n",
       "  tensor(0.2162, device='cuda:0'),\n",
       "  tensor(0.1912, device='cuda:0'),\n",
       "  tensor(0.1873, device='cuda:0'),\n",
       "  tensor(0.1887, device='cuda:0'),\n",
       "  tensor(0.1860, device='cuda:0'),\n",
       "  tensor(0.1868, device='cuda:0'),\n",
       "  tensor(0.1869, device='cuda:0'),\n",
       "  tensor(0.1829, device='cuda:0'),\n",
       "  tensor(0.1867, device='cuda:0'),\n",
       "  tensor(0.1743, device='cuda:0'),\n",
       "  tensor(0.1809, device='cuda:0'),\n",
       "  tensor(0.1855, device='cuda:0'),\n",
       "  tensor(0.1824, device='cuda:0'),\n",
       "  tensor(0.1815, device='cuda:0'),\n",
       "  tensor(0.1795, device='cuda:0'),\n",
       "  tensor(0.1759, device='cuda:0'),\n",
       "  tensor(0.1741, device='cuda:0'),\n",
       "  tensor(0.1730, device='cuda:0'),\n",
       "  tensor(0.1766, device='cuda:0'),\n",
       "  tensor(0.1772, device='cuda:0'),\n",
       "  tensor(0.1765, device='cuda:0'),\n",
       "  tensor(0.1770, device='cuda:0'),\n",
       "  tensor(0.1761, device='cuda:0'),\n",
       "  tensor(0.1748, device='cuda:0'),\n",
       "  tensor(0.1757, device='cuda:0'),\n",
       "  tensor(0.1756, device='cuda:0'),\n",
       "  tensor(0.1732, device='cuda:0'),\n",
       "  tensor(0.1761, device='cuda:0'),\n",
       "  tensor(0.1750, device='cuda:0')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3Elu(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 967 ms, total: 28.5 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predElu, actualElu = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9439\n"
     ]
    }
   ],
   "source": [
    "eluAcc = (torch.argmax(predElu, dim=1) == actualElu).float().mean()\n",
    "print(f\"Test Accuracy: {geluAcc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using LeakyRelu instead of hardswish in Bottleneck block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LeakyReLU\n",
    "\n",
    "\n",
    "class MobileNetV3Leaky(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"small\":\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, LeakyReLU, 2],\n",
    "                [5, 240, 40, True, LeakyReLU, 1],\n",
    "                [5, 240, 40, True, LeakyReLU, 1],\n",
    "                [5, 120, 48, True, LeakyReLU, 1],\n",
    "                [5, 144, 48, True, LeakyReLU, 1],\n",
    "                [5, 288, 96, True, LeakyReLU, 2],\n",
    "                [5, 576, 96, True, LeakyReLU, 1],\n",
    "                [5, 576, 96, True, LeakyReLU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "\n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/254], Loss: 2.3111, Accuracy: 0.0938\n",
      "Epoch [1/100], Step [100/254], Loss: 1.3709, Accuracy: 0.5312\n",
      "Epoch [1/100], Step [200/254], Loss: 1.0504, Accuracy: 0.5469\n",
      "Epoch [1/100], Train Loss: 1.2057, Val Loss: 3.2128, Train Acc: 0.5547, Val Acc: 0.0738\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 1.0695, Accuracy: 0.6094\n",
      "Epoch [2/100], Step [100/254], Loss: 0.7829, Accuracy: 0.6719\n",
      "Epoch [2/100], Step [200/254], Loss: 0.8235, Accuracy: 0.7500\n",
      "Epoch [2/100], Train Loss: 0.8952, Val Loss: 1.7690, Train Acc: 0.6846, Val Acc: 0.4367\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 1.0630, Accuracy: 0.6719\n",
      "Epoch [3/100], Step [100/254], Loss: 0.8378, Accuracy: 0.7344\n",
      "Epoch [3/100], Step [200/254], Loss: 0.6594, Accuracy: 0.7344\n",
      "Epoch [3/100], Train Loss: 0.7536, Val Loss: 1.3294, Train Acc: 0.7357, Val Acc: 0.6230\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 0.6201, Accuracy: 0.8125\n",
      "Epoch [4/100], Step [100/254], Loss: 0.7240, Accuracy: 0.7500\n",
      "Epoch [4/100], Step [200/254], Loss: 0.6340, Accuracy: 0.7969\n",
      "Epoch [4/100], Train Loss: 0.6927, Val Loss: 0.7220, Train Acc: 0.7597, Val Acc: 0.7494\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 0.6465, Accuracy: 0.7969\n",
      "Epoch [5/100], Step [100/254], Loss: 0.4902, Accuracy: 0.8438\n",
      "Epoch [5/100], Step [200/254], Loss: 0.5545, Accuracy: 0.8438\n",
      "Epoch [5/100], Train Loss: 0.6114, Val Loss: 1.0304, Train Acc: 0.7894, Val Acc: 0.6646\n",
      "==================================================\n",
      "Epoch [6/100], Step [0/254], Loss: 0.5066, Accuracy: 0.8281\n",
      "Epoch [6/100], Step [100/254], Loss: 0.3673, Accuracy: 0.8906\n",
      "Epoch [6/100], Step [200/254], Loss: 0.4252, Accuracy: 0.7969\n",
      "Epoch [6/100], Train Loss: 0.5981, Val Loss: 1.3379, Train Acc: 0.7931, Val Acc: 0.6136\n",
      "==================================================\n",
      "Epoch [7/100], Step [0/254], Loss: 0.5537, Accuracy: 0.8281\n",
      "Epoch [7/100], Step [100/254], Loss: 0.5267, Accuracy: 0.8906\n",
      "Epoch [7/100], Step [200/254], Loss: 0.4854, Accuracy: 0.7812\n",
      "Epoch [7/100], Train Loss: 0.5330, Val Loss: 0.9787, Train Acc: 0.8175, Val Acc: 0.7256\n",
      "==================================================\n",
      "Epoch [8/100], Step [0/254], Loss: 0.6593, Accuracy: 0.7812\n",
      "Epoch [8/100], Step [100/254], Loss: 0.6638, Accuracy: 0.7812\n",
      "Epoch [8/100], Step [200/254], Loss: 0.4064, Accuracy: 0.8906\n",
      "Epoch [8/100], Train Loss: 0.5201, Val Loss: 0.5801, Train Acc: 0.8211, Val Acc: 0.7982\n",
      "==================================================\n",
      "Epoch [9/100], Step [0/254], Loss: 0.4912, Accuracy: 0.7969\n",
      "Epoch [9/100], Step [100/254], Loss: 0.6044, Accuracy: 0.8125\n",
      "Epoch [9/100], Step [200/254], Loss: 0.2941, Accuracy: 0.9375\n",
      "Epoch [9/100], Train Loss: 0.4780, Val Loss: 0.9604, Train Acc: 0.8376, Val Acc: 0.7037\n",
      "==================================================\n",
      "Epoch [10/100], Step [0/254], Loss: 0.3676, Accuracy: 0.8750\n",
      "Epoch [10/100], Step [100/254], Loss: 0.5048, Accuracy: 0.7812\n",
      "Epoch [10/100], Step [200/254], Loss: 0.3557, Accuracy: 0.8438\n",
      "Epoch [10/100], Train Loss: 0.4577, Val Loss: 0.7489, Train Acc: 0.8421, Val Acc: 0.7630\n",
      "==================================================\n",
      "Epoch [11/100], Step [0/254], Loss: 0.4309, Accuracy: 0.8281\n",
      "Epoch [11/100], Step [100/254], Loss: 0.5085, Accuracy: 0.8594\n",
      "Epoch [11/100], Step [200/254], Loss: 0.4951, Accuracy: 0.8281\n",
      "Epoch [11/100], Train Loss: 0.4445, Val Loss: 0.6254, Train Acc: 0.8473, Val Acc: 0.7996\n",
      "==================================================\n",
      "Epoch [12/100], Step [0/254], Loss: 0.3644, Accuracy: 0.9062\n",
      "Epoch [12/100], Step [100/254], Loss: 0.5397, Accuracy: 0.8281\n",
      "Epoch [12/100], Step [200/254], Loss: 0.5083, Accuracy: 0.8594\n",
      "Epoch [12/100], Train Loss: 0.4093, Val Loss: 0.5424, Train Acc: 0.8627, Val Acc: 0.8221\n",
      "==================================================\n",
      "Epoch [13/100], Step [0/254], Loss: 0.6057, Accuracy: 0.7969\n",
      "Epoch [13/100], Step [100/254], Loss: 0.7096, Accuracy: 0.8125\n",
      "Epoch [13/100], Step [200/254], Loss: 0.2657, Accuracy: 0.9062\n",
      "Epoch [13/100], Train Loss: 0.4183, Val Loss: 0.5682, Train Acc: 0.8570, Val Acc: 0.8029\n",
      "==================================================\n",
      "Epoch [14/100], Step [0/254], Loss: 0.4981, Accuracy: 0.8281\n",
      "Epoch [14/100], Step [100/254], Loss: 0.2506, Accuracy: 0.9219\n",
      "Epoch [14/100], Step [200/254], Loss: 0.2735, Accuracy: 0.9219\n",
      "Epoch [14/100], Train Loss: 0.3941, Val Loss: 0.5487, Train Acc: 0.8647, Val Acc: 0.8262\n",
      "==================================================\n",
      "Epoch [15/100], Step [0/254], Loss: 0.5179, Accuracy: 0.8125\n",
      "Epoch [15/100], Step [100/254], Loss: 0.3152, Accuracy: 0.9062\n",
      "Epoch [15/100], Step [200/254], Loss: 0.4653, Accuracy: 0.8281\n",
      "Epoch [15/100], Train Loss: 0.3893, Val Loss: 0.5028, Train Acc: 0.8666, Val Acc: 0.8163\n",
      "==================================================\n",
      "Epoch [16/100], Step [0/254], Loss: 0.4321, Accuracy: 0.8906\n",
      "Epoch [16/100], Step [100/254], Loss: 0.2420, Accuracy: 0.9375\n",
      "Epoch [16/100], Step [200/254], Loss: 0.3613, Accuracy: 0.8594\n",
      "Epoch [16/100], Train Loss: 0.3745, Val Loss: 0.4175, Train Acc: 0.8729, Val Acc: 0.8629\n",
      "==================================================\n",
      "Epoch [17/100], Step [0/254], Loss: 0.2979, Accuracy: 0.9062\n",
      "Epoch [17/100], Step [100/254], Loss: 0.2809, Accuracy: 0.9219\n",
      "Epoch [17/100], Step [200/254], Loss: 0.6519, Accuracy: 0.8438\n",
      "Epoch [17/100], Train Loss: 0.3646, Val Loss: 0.4305, Train Acc: 0.8768, Val Acc: 0.8521\n",
      "==================================================\n",
      "Epoch [18/100], Step [0/254], Loss: 0.3759, Accuracy: 0.8750\n",
      "Epoch [18/100], Step [100/254], Loss: 0.3665, Accuracy: 0.8281\n",
      "Epoch [18/100], Step [200/254], Loss: 0.3763, Accuracy: 0.8750\n",
      "Epoch [18/100], Train Loss: 0.3508, Val Loss: 0.3355, Train Acc: 0.8802, Val Acc: 0.8807\n",
      "==================================================\n",
      "Epoch [19/100], Step [0/254], Loss: 0.4173, Accuracy: 0.8750\n",
      "Epoch [19/100], Step [100/254], Loss: 0.2842, Accuracy: 0.8906\n",
      "Epoch [19/100], Step [200/254], Loss: 0.4420, Accuracy: 0.8594\n",
      "Epoch [19/100], Train Loss: 0.3345, Val Loss: 0.2982, Train Acc: 0.8881, Val Acc: 0.8948\n",
      "==================================================\n",
      "Epoch [20/100], Step [0/254], Loss: 0.1546, Accuracy: 0.9531\n",
      "Epoch [20/100], Step [100/254], Loss: 0.1851, Accuracy: 0.9531\n",
      "Epoch [20/100], Step [200/254], Loss: 0.2796, Accuracy: 0.8906\n",
      "Epoch [20/100], Train Loss: 0.3186, Val Loss: 0.4981, Train Acc: 0.8890, Val Acc: 0.8318\n",
      "==================================================\n",
      "Epoch [21/100], Step [0/254], Loss: 0.4099, Accuracy: 0.8125\n",
      "Epoch [21/100], Step [100/254], Loss: 0.2101, Accuracy: 0.9219\n",
      "Epoch [21/100], Step [200/254], Loss: 0.1988, Accuracy: 0.9219\n",
      "Epoch [21/100], Train Loss: 0.3236, Val Loss: 0.5892, Train Acc: 0.8897, Val Acc: 0.8146\n",
      "==================================================\n",
      "Epoch [22/100], Step [0/254], Loss: 0.4395, Accuracy: 0.8594\n",
      "Epoch [22/100], Step [100/254], Loss: 0.3004, Accuracy: 0.8750\n",
      "Epoch [22/100], Step [200/254], Loss: 0.2574, Accuracy: 0.9219\n",
      "Epoch [22/100], Train Loss: 0.3316, Val Loss: 0.4711, Train Acc: 0.8866, Val Acc: 0.8362\n",
      "==================================================\n",
      "Epoch [23/100], Step [0/254], Loss: 0.3787, Accuracy: 0.9219\n",
      "Epoch [23/100], Step [100/254], Loss: 0.2961, Accuracy: 0.8594\n",
      "Epoch [23/100], Step [200/254], Loss: 0.1440, Accuracy: 0.9844\n",
      "Epoch [23/100], Train Loss: 0.2984, Val Loss: 0.3644, Train Acc: 0.8951, Val Acc: 0.8812\n",
      "==================================================\n",
      "Epoch [24/100], Step [0/254], Loss: 0.2700, Accuracy: 0.9062\n",
      "Epoch [24/100], Step [100/254], Loss: 0.2980, Accuracy: 0.8906\n",
      "Epoch [24/100], Step [200/254], Loss: 0.1696, Accuracy: 0.9375\n",
      "Epoch [24/100], Train Loss: 0.2432, Val Loss: 0.1848, Train Acc: 0.9168, Val Acc: 0.9363\n",
      "==================================================\n",
      "Epoch [25/100], Step [0/254], Loss: 0.1817, Accuracy: 0.9219\n",
      "Epoch [25/100], Step [100/254], Loss: 0.1564, Accuracy: 0.9531\n",
      "Epoch [25/100], Step [200/254], Loss: 0.2763, Accuracy: 0.8750\n",
      "Epoch [25/100], Train Loss: 0.2260, Val Loss: 0.2070, Train Acc: 0.9231, Val Acc: 0.9285\n",
      "==================================================\n",
      "Epoch [26/100], Step [0/254], Loss: 0.3162, Accuracy: 0.8906\n",
      "Epoch [26/100], Step [100/254], Loss: 0.1358, Accuracy: 0.9219\n",
      "Epoch [26/100], Step [200/254], Loss: 0.1373, Accuracy: 0.9375\n",
      "Epoch [26/100], Train Loss: 0.2192, Val Loss: 0.1896, Train Acc: 0.9218, Val Acc: 0.9335\n",
      "==================================================\n",
      "Epoch [27/100], Step [0/254], Loss: 0.1841, Accuracy: 0.9375\n",
      "Epoch [27/100], Step [100/254], Loss: 0.3059, Accuracy: 0.8750\n",
      "Epoch [27/100], Step [200/254], Loss: 0.1474, Accuracy: 0.9531\n",
      "Epoch [27/100], Train Loss: 0.2163, Val Loss: 0.1755, Train Acc: 0.9230, Val Acc: 0.9384\n",
      "==================================================\n",
      "Epoch [28/100], Step [0/254], Loss: 0.3076, Accuracy: 0.8906\n",
      "Epoch [28/100], Step [100/254], Loss: 0.2720, Accuracy: 0.9062\n",
      "Epoch [28/100], Step [200/254], Loss: 0.1348, Accuracy: 0.9219\n",
      "Epoch [28/100], Train Loss: 0.2133, Val Loss: 0.1695, Train Acc: 0.9252, Val Acc: 0.9399\n",
      "==================================================\n",
      "Epoch [29/100], Step [0/254], Loss: 0.2204, Accuracy: 0.9375\n",
      "Epoch [29/100], Step [100/254], Loss: 0.2261, Accuracy: 0.9062\n",
      "Epoch [29/100], Step [200/254], Loss: 0.1837, Accuracy: 0.9062\n",
      "Epoch [29/100], Train Loss: 0.2238, Val Loss: 0.1743, Train Acc: 0.9242, Val Acc: 0.9391\n",
      "==================================================\n",
      "Epoch [30/100], Step [0/254], Loss: 0.0666, Accuracy: 0.9844\n",
      "Epoch [30/100], Step [100/254], Loss: 0.0843, Accuracy: 0.9844\n",
      "Epoch [30/100], Step [200/254], Loss: 0.2663, Accuracy: 0.8906\n",
      "Epoch [30/100], Train Loss: 0.2202, Val Loss: 0.1735, Train Acc: 0.9220, Val Acc: 0.9385\n",
      "==================================================\n",
      "Epoch [31/100], Step [0/254], Loss: 0.0705, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [100/254], Loss: 0.1209, Accuracy: 0.9688\n",
      "Epoch [31/100], Step [200/254], Loss: 0.3148, Accuracy: 0.8906\n",
      "Epoch [31/100], Train Loss: 0.2016, Val Loss: 0.1932, Train Acc: 0.9287, Val Acc: 0.9293\n",
      "==================================================\n",
      "Epoch [32/100], Step [0/254], Loss: 0.1375, Accuracy: 0.9531\n",
      "Epoch [32/100], Step [100/254], Loss: 0.2801, Accuracy: 0.9219\n",
      "Epoch [32/100], Step [200/254], Loss: 0.2449, Accuracy: 0.8906\n",
      "Epoch [32/100], Train Loss: 0.2047, Val Loss: 0.1838, Train Acc: 0.9295, Val Acc: 0.9355\n",
      "==================================================\n",
      "Epoch [33/100], Step [0/254], Loss: 0.2390, Accuracy: 0.9375\n",
      "Epoch [33/100], Step [100/254], Loss: 0.1931, Accuracy: 0.9375\n",
      "Epoch [33/100], Step [200/254], Loss: 0.1735, Accuracy: 0.9375\n",
      "Epoch [33/100], Train Loss: 0.1945, Val Loss: 0.1515, Train Acc: 0.9322, Val Acc: 0.9463\n",
      "==================================================\n",
      "Epoch [34/100], Step [0/254], Loss: 0.2189, Accuracy: 0.9062\n",
      "Epoch [34/100], Step [100/254], Loss: 0.1525, Accuracy: 0.9375\n",
      "Epoch [34/100], Step [200/254], Loss: 0.1420, Accuracy: 0.9688\n",
      "Epoch [34/100], Train Loss: 0.1920, Val Loss: 0.1522, Train Acc: 0.9349, Val Acc: 0.9458\n",
      "==================================================\n",
      "Epoch [35/100], Step [0/254], Loss: 0.2254, Accuracy: 0.9062\n",
      "Epoch [35/100], Step [100/254], Loss: 0.2875, Accuracy: 0.8906\n",
      "Epoch [35/100], Step [200/254], Loss: 0.2019, Accuracy: 0.8906\n",
      "Epoch [35/100], Train Loss: 0.1823, Val Loss: 0.1467, Train Acc: 0.9369, Val Acc: 0.9485\n",
      "==================================================\n",
      "Epoch [36/100], Step [0/254], Loss: 0.1443, Accuracy: 0.9219\n",
      "Epoch [36/100], Step [100/254], Loss: 0.3465, Accuracy: 0.8750\n",
      "Epoch [36/100], Step [200/254], Loss: 0.1395, Accuracy: 0.9531\n",
      "Epoch [36/100], Train Loss: 0.1891, Val Loss: 0.1438, Train Acc: 0.9305, Val Acc: 0.9490\n",
      "==================================================\n",
      "Epoch [37/100], Step [0/254], Loss: 0.2197, Accuracy: 0.9375\n",
      "Epoch [37/100], Step [100/254], Loss: 0.1711, Accuracy: 0.9375\n",
      "Epoch [37/100], Step [200/254], Loss: 0.1195, Accuracy: 0.9688\n",
      "Epoch [37/100], Train Loss: 0.1860, Val Loss: 0.1435, Train Acc: 0.9369, Val Acc: 0.9499\n",
      "==================================================\n",
      "Epoch [38/100], Step [0/254], Loss: 0.2419, Accuracy: 0.9062\n",
      "Epoch [38/100], Step [100/254], Loss: 0.2167, Accuracy: 0.9219\n",
      "Epoch [38/100], Step [200/254], Loss: 0.2812, Accuracy: 0.9062\n",
      "Epoch [38/100], Train Loss: 0.1779, Val Loss: 0.1460, Train Acc: 0.9365, Val Acc: 0.9496\n",
      "==================================================\n",
      "Epoch [39/100], Step [0/254], Loss: 0.1464, Accuracy: 0.9375\n",
      "Epoch [39/100], Step [100/254], Loss: 0.1881, Accuracy: 0.9219\n",
      "Epoch [39/100], Step [200/254], Loss: 0.0566, Accuracy: 1.0000\n",
      "Epoch [39/100], Train Loss: 0.1821, Val Loss: 0.1426, Train Acc: 0.9355, Val Acc: 0.9490\n",
      "==================================================\n",
      "Epoch [40/100], Step [0/254], Loss: 0.0510, Accuracy: 1.0000\n",
      "Epoch [40/100], Step [100/254], Loss: 0.2750, Accuracy: 0.9219\n",
      "Epoch [40/100], Step [200/254], Loss: 0.2625, Accuracy: 0.9062\n",
      "Epoch [40/100], Train Loss: 0.1774, Val Loss: 0.1425, Train Acc: 0.9368, Val Acc: 0.9502\n",
      "==================================================\n",
      "Epoch [41/100], Step [0/254], Loss: 0.1495, Accuracy: 0.9219\n",
      "Epoch [41/100], Step [100/254], Loss: 0.1126, Accuracy: 0.9375\n",
      "Epoch [41/100], Step [200/254], Loss: 0.2176, Accuracy: 0.9375\n",
      "Epoch [41/100], Train Loss: 0.1776, Val Loss: 0.1376, Train Acc: 0.9366, Val Acc: 0.9544\n",
      "==================================================\n",
      "Epoch [42/100], Step [0/254], Loss: 0.2438, Accuracy: 0.9062\n",
      "Epoch [42/100], Step [100/254], Loss: 0.1939, Accuracy: 0.9219\n",
      "Epoch [42/100], Step [200/254], Loss: 0.1386, Accuracy: 0.9531\n",
      "Epoch [42/100], Train Loss: 0.1808, Val Loss: 0.1403, Train Acc: 0.9364, Val Acc: 0.9490\n",
      "==================================================\n",
      "Epoch [43/100], Step [0/254], Loss: 0.1414, Accuracy: 0.9375\n",
      "Epoch [43/100], Step [100/254], Loss: 0.1338, Accuracy: 0.9531\n",
      "Epoch [43/100], Step [200/254], Loss: 0.1161, Accuracy: 0.9844\n",
      "Epoch [43/100], Train Loss: 0.1842, Val Loss: 0.1402, Train Acc: 0.9333, Val Acc: 0.9504\n",
      "==================================================\n",
      "Epoch [44/100], Step [0/254], Loss: 0.3275, Accuracy: 0.9219\n",
      "Epoch [44/100], Step [100/254], Loss: 0.2280, Accuracy: 0.8906\n",
      "Epoch [44/100], Step [200/254], Loss: 0.1741, Accuracy: 0.9531\n",
      "Epoch [44/100], Train Loss: 0.1881, Val Loss: 0.1382, Train Acc: 0.9357, Val Acc: 0.9513\n",
      "==================================================\n",
      "Epoch [45/100], Step [0/254], Loss: 0.0757, Accuracy: 0.9844\n",
      "Epoch [45/100], Step [100/254], Loss: 0.1362, Accuracy: 0.9688\n",
      "Epoch [45/100], Step [200/254], Loss: 0.1619, Accuracy: 0.9375\n",
      "Epoch [45/100], Train Loss: 0.1686, Val Loss: 0.1398, Train Acc: 0.9393, Val Acc: 0.9515\n",
      "==================================================\n",
      "Epoch [46/100], Step [0/254], Loss: 0.1858, Accuracy: 0.9375\n",
      "Epoch [46/100], Step [100/254], Loss: 0.0893, Accuracy: 0.9688\n",
      "Epoch [46/100], Step [200/254], Loss: 0.1642, Accuracy: 0.9375\n",
      "Epoch [46/100], Train Loss: 0.1718, Val Loss: 0.1405, Train Acc: 0.9382, Val Acc: 0.9510\n",
      "==================================================\n",
      "Epoch [47/100], Step [0/254], Loss: 0.0490, Accuracy: 0.9844\n",
      "Epoch [47/100], Step [100/254], Loss: 0.3099, Accuracy: 0.8750\n",
      "Epoch [47/100], Step [200/254], Loss: 0.0837, Accuracy: 0.9688\n",
      "Epoch [47/100], Train Loss: 0.1717, Val Loss: 0.1388, Train Acc: 0.9388, Val Acc: 0.9522\n",
      "==================================================\n",
      "Epoch [48/100], Step [0/254], Loss: 0.0975, Accuracy: 0.9688\n",
      "Epoch [48/100], Step [100/254], Loss: 0.0966, Accuracy: 0.9688\n",
      "Epoch [48/100], Step [200/254], Loss: 0.1644, Accuracy: 0.9219\n",
      "Epoch [48/100], Train Loss: 0.1787, Val Loss: 0.1345, Train Acc: 0.9371, Val Acc: 0.9517\n",
      "==================================================\n",
      "Epoch [49/100], Step [0/254], Loss: 0.1469, Accuracy: 0.9531\n",
      "Epoch [49/100], Step [100/254], Loss: 0.1956, Accuracy: 0.9062\n",
      "Epoch [49/100], Step [200/254], Loss: 0.2159, Accuracy: 0.9219\n",
      "Epoch [49/100], Train Loss: 0.1781, Val Loss: 0.1347, Train Acc: 0.9377, Val Acc: 0.9547\n",
      "==================================================\n",
      "Epoch [50/100], Step [0/254], Loss: 0.2045, Accuracy: 0.9219\n",
      "Epoch [50/100], Step [100/254], Loss: 0.0443, Accuracy: 1.0000\n",
      "Epoch [50/100], Step [200/254], Loss: 0.0775, Accuracy: 1.0000\n",
      "Epoch [50/100], Train Loss: 0.1794, Val Loss: 0.1393, Train Acc: 0.9392, Val Acc: 0.9522\n",
      "==================================================\n",
      "Epoch [51/100], Step [0/254], Loss: 0.2228, Accuracy: 0.9219\n",
      "Epoch [51/100], Step [100/254], Loss: 0.3645, Accuracy: 0.8438\n",
      "Epoch [51/100], Step [200/254], Loss: 0.1540, Accuracy: 0.9531\n",
      "Epoch [51/100], Train Loss: 0.1658, Val Loss: 0.1340, Train Acc: 0.9416, Val Acc: 0.9528\n",
      "==================================================\n",
      "Epoch [52/100], Step [0/254], Loss: 0.0738, Accuracy: 0.9844\n",
      "Epoch [52/100], Step [100/254], Loss: 0.1531, Accuracy: 0.9375\n",
      "Epoch [52/100], Step [200/254], Loss: 0.2800, Accuracy: 0.8906\n",
      "Epoch [52/100], Train Loss: 0.1678, Val Loss: 0.1340, Train Acc: 0.9422, Val Acc: 0.9525\n",
      "==================================================\n",
      "Epoch [53/100], Step [0/254], Loss: 0.1674, Accuracy: 0.9375\n",
      "Epoch [53/100], Step [100/254], Loss: 0.1662, Accuracy: 0.9219\n",
      "Epoch [53/100], Step [200/254], Loss: 0.2098, Accuracy: 0.9375\n",
      "Epoch [53/100], Train Loss: 0.1821, Val Loss: 0.1361, Train Acc: 0.9350, Val Acc: 0.9511\n",
      "==================================================\n",
      "Epoch [54/100], Step [0/254], Loss: 0.0669, Accuracy: 0.9844\n",
      "Epoch [54/100], Step [100/254], Loss: 0.2195, Accuracy: 0.9219\n",
      "Epoch [54/100], Step [200/254], Loss: 0.1780, Accuracy: 0.9375\n",
      "Epoch [54/100], Train Loss: 0.1718, Val Loss: 0.1382, Train Acc: 0.9413, Val Acc: 0.9510\n",
      "==================================================\n",
      "Epoch [55/100], Step [0/254], Loss: 0.1487, Accuracy: 0.9531\n",
      "Epoch [55/100], Step [100/254], Loss: 0.1263, Accuracy: 0.9844\n",
      "Epoch [55/100], Step [200/254], Loss: 0.1389, Accuracy: 0.9688\n",
      "Epoch [55/100], Train Loss: 0.1736, Val Loss: 0.1384, Train Acc: 0.9384, Val Acc: 0.9513\n",
      "==================================================\n",
      "Epoch [56/100], Step [0/254], Loss: 0.2921, Accuracy: 0.9062\n",
      "Epoch [56/100], Step [100/254], Loss: 0.1791, Accuracy: 0.9219\n",
      "Epoch [56/100], Step [200/254], Loss: 0.2380, Accuracy: 0.8906\n",
      "Epoch [56/100], Train Loss: 0.1722, Val Loss: 0.1395, Train Acc: 0.9394, Val Acc: 0.9515\n",
      "==================================================\n",
      "Epoch [57/100], Step [0/254], Loss: 0.1115, Accuracy: 0.9531\n",
      "Epoch [57/100], Step [100/254], Loss: 0.0891, Accuracy: 0.9688\n",
      "Epoch [57/100], Step [200/254], Loss: 0.1123, Accuracy: 0.9375\n",
      "Epoch [57/100], Train Loss: 0.1683, Val Loss: 0.1379, Train Acc: 0.9392, Val Acc: 0.9505\n",
      "==================================================\n",
      "Epoch [58/100], Step [0/254], Loss: 0.1559, Accuracy: 0.9531\n",
      "Epoch [58/100], Step [100/254], Loss: 0.1359, Accuracy: 0.9844\n",
      "Epoch [58/100], Step [200/254], Loss: 0.2199, Accuracy: 0.9219\n",
      "Epoch [58/100], Train Loss: 0.1718, Val Loss: 0.1342, Train Acc: 0.9384, Val Acc: 0.9539\n",
      "==================================================\n",
      "Epoch [59/100], Step [0/254], Loss: 0.1237, Accuracy: 0.9688\n",
      "Epoch [59/100], Step [100/254], Loss: 0.4212, Accuracy: 0.8438\n",
      "Epoch [59/100], Step [200/254], Loss: 0.1432, Accuracy: 0.9219\n",
      "Epoch [59/100], Train Loss: 0.1746, Val Loss: 0.1353, Train Acc: 0.9393, Val Acc: 0.9524\n",
      "==================================================\n",
      "Epoch [60/100], Step [0/254], Loss: 0.0992, Accuracy: 0.9844\n",
      "Epoch [60/100], Step [100/254], Loss: 0.2592, Accuracy: 0.9219\n",
      "Epoch [60/100], Step [200/254], Loss: 0.2190, Accuracy: 0.9375\n",
      "Epoch [60/100], Train Loss: 0.1742, Val Loss: 0.1351, Train Acc: 0.9406, Val Acc: 0.9520\n",
      "==================================================\n",
      "Epoch [61/100], Step [0/254], Loss: 0.1775, Accuracy: 0.9219\n",
      "Epoch [61/100], Step [100/254], Loss: 0.2388, Accuracy: 0.9219\n",
      "Epoch [61/100], Step [200/254], Loss: 0.1016, Accuracy: 0.9531\n",
      "Epoch [61/100], Train Loss: 0.1697, Val Loss: 0.1382, Train Acc: 0.9387, Val Acc: 0.9522\n",
      "==================================================\n",
      "Epoch [62/100], Step [0/254], Loss: 0.1001, Accuracy: 0.9688\n",
      "Epoch [62/100], Step [100/254], Loss: 0.2302, Accuracy: 0.9375\n",
      "Epoch [62/100], Step [200/254], Loss: 0.1097, Accuracy: 0.9688\n",
      "Epoch [62/100], Train Loss: 0.1672, Val Loss: 0.1354, Train Acc: 0.9400, Val Acc: 0.9514\n",
      "==================================================\n",
      "CPU times: user 8h 33min 9s, sys: 17min 10s, total: 8h 50min 19s\n",
      "Wall time: 1h 7min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.2056729523685035,\n",
       "  0.895162730705081,\n",
       "  0.7536280679655826,\n",
       "  0.6927459298625706,\n",
       "  0.6114184709749823,\n",
       "  0.5981067278253751,\n",
       "  0.5330351925506367,\n",
       "  0.5200614942692396,\n",
       "  0.477996087977736,\n",
       "  0.457701325181901,\n",
       "  0.4444837455205091,\n",
       "  0.4092987003758198,\n",
       "  0.4183402805347142,\n",
       "  0.39405642344257025,\n",
       "  0.3893335371857553,\n",
       "  0.374477186892915,\n",
       "  0.36461322707688715,\n",
       "  0.35076466009138135,\n",
       "  0.3344695196435677,\n",
       "  0.31863156644495455,\n",
       "  0.32358062842230156,\n",
       "  0.3316278682625669,\n",
       "  0.29842899860007555,\n",
       "  0.24323792086811516,\n",
       "  0.2259903264591309,\n",
       "  0.21919267994212352,\n",
       "  0.21633716216530857,\n",
       "  0.21330135246724125,\n",
       "  0.22382246666243227,\n",
       "  0.22015280203204454,\n",
       "  0.20155607059773967,\n",
       "  0.20470878434932138,\n",
       "  0.1945146644106648,\n",
       "  0.19198901816381245,\n",
       "  0.18227503074729068,\n",
       "  0.18910906997721966,\n",
       "  0.18601405010448666,\n",
       "  0.1779126077890396,\n",
       "  0.182119106345876,\n",
       "  0.1773536583394047,\n",
       "  0.17758769885908196,\n",
       "  0.1808204355495652,\n",
       "  0.18418877277024617,\n",
       "  0.18806795258162998,\n",
       "  0.16862907791583556,\n",
       "  0.17179076923570763,\n",
       "  0.17172512393738107,\n",
       "  0.17874216588991365,\n",
       "  0.17807126998549372,\n",
       "  0.17944532373230757,\n",
       "  0.16581445772523487,\n",
       "  0.1677955910416804,\n",
       "  0.18207237480838936,\n",
       "  0.17177681315426283,\n",
       "  0.1736246963860247,\n",
       "  0.17220130586248683,\n",
       "  0.1682878003169702,\n",
       "  0.17183894627382906,\n",
       "  0.17460886515148982,\n",
       "  0.17421528899411517,\n",
       "  0.1696980660561267,\n",
       "  0.16723302680647045],\n",
       " [tensor(3.2128, device='cuda:0'),\n",
       "  tensor(1.7690, device='cuda:0'),\n",
       "  tensor(1.3294, device='cuda:0'),\n",
       "  tensor(0.7220, device='cuda:0'),\n",
       "  tensor(1.0304, device='cuda:0'),\n",
       "  tensor(1.3379, device='cuda:0'),\n",
       "  tensor(0.9787, device='cuda:0'),\n",
       "  tensor(0.5801, device='cuda:0'),\n",
       "  tensor(0.9604, device='cuda:0'),\n",
       "  tensor(0.7489, device='cuda:0'),\n",
       "  tensor(0.6254, device='cuda:0'),\n",
       "  tensor(0.5424, device='cuda:0'),\n",
       "  tensor(0.5682, device='cuda:0'),\n",
       "  tensor(0.5487, device='cuda:0'),\n",
       "  tensor(0.5028, device='cuda:0'),\n",
       "  tensor(0.4175, device='cuda:0'),\n",
       "  tensor(0.4305, device='cuda:0'),\n",
       "  tensor(0.3355, device='cuda:0'),\n",
       "  tensor(0.2982, device='cuda:0'),\n",
       "  tensor(0.4981, device='cuda:0'),\n",
       "  tensor(0.5892, device='cuda:0'),\n",
       "  tensor(0.4711, device='cuda:0'),\n",
       "  tensor(0.3644, device='cuda:0'),\n",
       "  tensor(0.1848, device='cuda:0'),\n",
       "  tensor(0.2070, device='cuda:0'),\n",
       "  tensor(0.1896, device='cuda:0'),\n",
       "  tensor(0.1755, device='cuda:0'),\n",
       "  tensor(0.1695, device='cuda:0'),\n",
       "  tensor(0.1743, device='cuda:0'),\n",
       "  tensor(0.1735, device='cuda:0'),\n",
       "  tensor(0.1932, device='cuda:0'),\n",
       "  tensor(0.1838, device='cuda:0'),\n",
       "  tensor(0.1515, device='cuda:0'),\n",
       "  tensor(0.1522, device='cuda:0'),\n",
       "  tensor(0.1467, device='cuda:0'),\n",
       "  tensor(0.1438, device='cuda:0'),\n",
       "  tensor(0.1435, device='cuda:0'),\n",
       "  tensor(0.1460, device='cuda:0'),\n",
       "  tensor(0.1426, device='cuda:0'),\n",
       "  tensor(0.1425, device='cuda:0'),\n",
       "  tensor(0.1376, device='cuda:0'),\n",
       "  tensor(0.1403, device='cuda:0'),\n",
       "  tensor(0.1402, device='cuda:0'),\n",
       "  tensor(0.1382, device='cuda:0'),\n",
       "  tensor(0.1398, device='cuda:0'),\n",
       "  tensor(0.1405, device='cuda:0'),\n",
       "  tensor(0.1388, device='cuda:0'),\n",
       "  tensor(0.1345, device='cuda:0'),\n",
       "  tensor(0.1347, device='cuda:0'),\n",
       "  tensor(0.1393, device='cuda:0'),\n",
       "  tensor(0.1340, device='cuda:0'),\n",
       "  tensor(0.1340, device='cuda:0'),\n",
       "  tensor(0.1361, device='cuda:0'),\n",
       "  tensor(0.1382, device='cuda:0'),\n",
       "  tensor(0.1384, device='cuda:0'),\n",
       "  tensor(0.1395, device='cuda:0'),\n",
       "  tensor(0.1379, device='cuda:0'),\n",
       "  tensor(0.1342, device='cuda:0'),\n",
       "  tensor(0.1353, device='cuda:0'),\n",
       "  tensor(0.1351, device='cuda:0'),\n",
       "  tensor(0.1382, device='cuda:0'),\n",
       "  tensor(0.1354, device='cuda:0')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3Leaky(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 814 ms, total: 26.3 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predLeaky, actualLeaky = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9378\n"
     ]
    }
   ],
   "source": [
    "predAcc = (torch.argmax(predLeaky, dim=1) == actualLeaky).float().mean()\n",
    "print(f\"Test Accuracy: {predAcc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using Hard-ELU instead of hardswish in bottleneck block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardElu(x, alpha=1.0, inplace=False):\n",
    "    safe_x = torch.where(x < 0.0, x, torch.zeros_like(x))\n",
    "\n",
    "    if inplace:\n",
    "        x[:] = torch.where(x < 0.0, alpha * safe_x / (1 - safe_x), x)\n",
    "        return x\n",
    "    else:\n",
    "        return torch.where(x < 0.0, alpha * safe_x / (1 - safe_x), x)\n",
    "\n",
    "\n",
    "class HardELU(nn.Module):\n",
    "    def __init__(self, alpha=1.0, inplace=False):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hardElu(x, self.alpha, self.inplace)\n",
    "\n",
    "\n",
    "class MobileNetV3HardElu(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chn,\n",
    "        se_reduction=4,\n",
    "        mode=\"small\",\n",
    "        num_classes=10,\n",
    "        bn_eps=0.001,\n",
    "        bn_momentum=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == \"small\":\n",
    "            # MobileNetV3-Small\n",
    "            layers_config = [\n",
    "                [3, 16, 16, True, ReLU, 2],\n",
    "                [3, 72, 24, False, ReLU, 2],\n",
    "                [3, 88, 24, False, ReLU, 1],\n",
    "                [5, 96, 40, True, HardELU, 2],\n",
    "                [5, 240, 40, True, HardELU, 1],\n",
    "                [5, 240, 40, True, HardELU, 1],\n",
    "                [5, 120, 48, True, HardELU, 1],\n",
    "                [5, 144, 48, True, HardELU, 1],\n",
    "                [5, 288, 96, True, HardELU, 2],\n",
    "                [5, 576, 96, True, HardELU, 1],\n",
    "                [5, 576, 96, True, HardELU, 1],\n",
    "            ]\n",
    "            last_channel = 1280\n",
    "            # final layer\n",
    "            self.final_layers = [\n",
    "                ConvBlock(\n",
    "                    layers_config[-1][2], 576, activation=Hardswish, kernel_size=1\n",
    "                ),\n",
    "                SqueezeAndExcite(576, se_reduction),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            ]\n",
    "\n",
    "        # Initial layers\n",
    "        self.features: list[nn.Module] = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        #         print(dir(self.features[-1]))\n",
    "\n",
    "        # Build main blocks\n",
    "\n",
    "        input_channel = 16\n",
    "        for kernel, exp, out, se, act, stride in layers_config:\n",
    "            self.features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        self.start = nn.Sequential(*self.features)\n",
    "        self.final = nn.Sequential(*self.final_layers)\n",
    "\n",
    "        # modify batchnorm layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eps = bn_eps\n",
    "                m.momentum = bn_momentum\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/100], Step [0/254], Loss: 2.3101, Accuracy: 0.1094\n",
      "Epoch [1/100], Step [100/254], Loss: 1.0525, Accuracy: 0.5781\n",
      "Epoch [1/100], Step [200/254], Loss: 0.9817, Accuracy: 0.6562\n",
      "Epoch [1/100], Train Loss: 1.1655, Val Loss: 2.9838, Train Acc: 0.5738, Val Acc: 0.1106\n",
      "==================================================\n",
      "Epoch [2/100], Step [0/254], Loss: 0.9359, Accuracy: 0.6719\n",
      "Epoch [2/100], Step [100/254], Loss: 0.8906, Accuracy: 0.6719\n",
      "Epoch [2/100], Step [200/254], Loss: 0.8871, Accuracy: 0.6094\n",
      "Epoch [2/100], Train Loss: 0.8236, Val Loss: 2.6079, Train Acc: 0.7084, Val Acc: 0.3681\n",
      "==================================================\n",
      "Epoch [3/100], Step [0/254], Loss: 1.1288, Accuracy: 0.5938\n",
      "Epoch [3/100], Step [100/254], Loss: 0.7040, Accuracy: 0.7344\n",
      "Epoch [3/100], Step [200/254], Loss: 0.4498, Accuracy: 0.8281\n",
      "Epoch [3/100], Train Loss: 0.7301, Val Loss: 1.0607, Train Acc: 0.7396, Val Acc: 0.6685\n",
      "==================================================\n",
      "Epoch [4/100], Step [0/254], Loss: 0.7512, Accuracy: 0.7031\n",
      "Epoch [4/100], Step [100/254], Loss: 0.5588, Accuracy: 0.7500\n",
      "Epoch [4/100], Step [200/254], Loss: 0.5570, Accuracy: 0.7812\n",
      "Epoch [4/100], Train Loss: 0.6334, Val Loss: 0.8009, Train Acc: 0.7734, Val Acc: 0.7589\n",
      "==================================================\n",
      "Epoch [5/100], Step [0/254], Loss: 0.6437, Accuracy: 0.7969\n",
      "Epoch [5/100], Step [100/254], Loss: 0.6632, Accuracy: 0.7812\n",
      "Epoch [5/100], Step [200/254], Loss: 0.5056, Accuracy: 0.7969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:15\u001b[0m\n",
      "Cell \u001b[0;32mIn[13], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)\u001b[0m\n\u001b[1;32m     67\u001b[0m trainLosses\u001b[38;5;241m.\u001b[39mappend(trainLoss)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m pred, actual \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m valLoss \u001b[38;5;241m=\u001b[39m criterion(pred, actual)\n\u001b[1;32m     72\u001b[0m valAcc \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m actual)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(model, device, dataLoader)\u001b[0m\n\u001b[1;32m     16\u001b[0m actual \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataLoader:\n\u001b[1;32m     20\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     61\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/_type_conversion.py:39\u001b[0m, in \u001b[0;36mToImage._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m, inpt: Union[torch\u001b[38;5;241m.\u001b[39mTensor, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray], params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_type_conversion.py:16\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(inpt)\u001b[0m\n\u001b[1;32m     14\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39matleast_3d(inpt))\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m---> 16\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inpt, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m inpt\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/torchvision/transforms/functional.py:209\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(nppic)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    210\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MobileNetV3HardElu(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predHardElu, actualHardElu = infer(model, device, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predAcc = (torch.argmax(predHardElu, dim=1) == actualHardElu).float().mean()\n",
    "print(f\"Test Accuracy: {predAcc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Sobel and Hog augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_chn=3, se_reduction=4, mode=\"large\", num_classes=10, hybrid=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        MobileNetV3 with optional hybrid (Sobel+HOG) mode\n",
    "        Args:\n",
    "            in_chn (int): Input channels (3 for RGB, 5 for hybrid mode)\n",
    "            se_reduction (int): SE block reduction ratio\n",
    "            mode (str): 'large' or 'small' architecture\n",
    "            num_classes (int): Output classes\n",
    "            hybrid (bool): Enable hybrid Sobel+HOG mode\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hybrid = hybrid\n",
    "        self.mode = mode\n",
    "        self.se_reduction = se_reduction\n",
    "        # Initialize feature extractors for hybrid mode\n",
    "        if self.hybrid:\n",
    "\n",
    "            self.sobel = (\n",
    "                SobelFeatureExtractor()\n",
    "            )  # Outputs 5 channels (RGB+SobelX+SobelY)\n",
    "            self.hog_layer = HOGLayer()  # HOG feature extractor (Dense layer)\n",
    "\n",
    "        # Architecture configuration\n",
    "        self.layers_config, last_channel = self._get_config(mode)\n",
    "\n",
    "        # Build core MobileNetV3 components\n",
    "        self.features = self._build_features(in_chn)\n",
    "        self.final = self._build_final_layers(last_channel, num_classes)\n",
    "\n",
    "        # Hybrid-specific components\n",
    "        if self.hybrid:\n",
    "            self.hog_fc = nn.Linear(512, num_classes)  # 256 MobileNet + 256 HOG\n",
    "\n",
    "    def _get_config(self, mode):\n",
    "        \"\"\"Get layer configuration and final channel count\"\"\"\n",
    "        if mode == \"large\":\n",
    "            return (\n",
    "                [\n",
    "                    [3, 16, 16, False, ReLU, 1],\n",
    "                    [3, 64, 24, False, ReLU, 2],\n",
    "                    [3, 72, 24, False, ReLU, 1],\n",
    "                    [5, 72, 40, True, ReLU, 2],\n",
    "                    [5, 120, 40, True, ReLU, 1],\n",
    "                    [5, 120, 40, True, ReLU, 1],\n",
    "                    [3, 240, 80, False, Hardswish, 2],\n",
    "                    [3, 200, 80, False, Hardswish, 1],\n",
    "                    [3, 184, 80, False, Hardswish, 1],\n",
    "                    [3, 184, 80, False, Hardswish, 1],\n",
    "                    [3, 480, 112, True, Hardswish, 1],\n",
    "                    [3, 672, 112, True, Hardswish, 1],\n",
    "                    [5, 672, 160, True, Hardswish, 2],\n",
    "                    [5, 960, 160, True, Hardswish, 1],\n",
    "                    [5, 960, 160, True, Hardswish, 1],\n",
    "                ],\n",
    "                1280,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                [\n",
    "                    [3, 16, 16, True, ReLU, 2],\n",
    "                    [3, 72, 24, False, ReLU, 2],\n",
    "                    [3, 88, 24, False, ReLU, 1],\n",
    "                    [5, 96, 40, True, Hardswish, 2],\n",
    "                    [5, 240, 40, True, Hardswish, 1],\n",
    "                    [5, 240, 40, True, Hardswish, 1],\n",
    "                    [5, 120, 48, True, Hardswish, 1],\n",
    "                    [5, 144, 48, True, Hardswish, 1],\n",
    "                    [5, 288, 96, True, Hardswish, 2],\n",
    "                    [5, 576, 96, True, Hardswish, 1],\n",
    "                    [5, 576, 96, True, Hardswish, 1],\n",
    "                ],\n",
    "                1024,\n",
    "            )  # return 2 thing config list and neuron in the last chanbel\n",
    "\n",
    "    def _build_features(self, in_chn):\n",
    "        \"\"\"Construct initial convolutional blocks\"\"\"\n",
    "        features = [\n",
    "            ConvBlock(in_chn, 16, activation=Hardswish, kernel_size=3, stride=2)\n",
    "        ]\n",
    "        input_channel = 16\n",
    "\n",
    "        # create the bottleneck block based on the config choose\n",
    "        for kernel, exp, out, se, act, stride in self.layers_config:\n",
    "            features.append(\n",
    "                BottleNeck(\n",
    "                    in_chn=input_channel,\n",
    "                    out_chn=out,\n",
    "                    expansion_channel=exp,\n",
    "                    activation=act,\n",
    "                    kernel=kernel,\n",
    "                    se_flag=se,\n",
    "                    stride=stride,\n",
    "                    se_reduction=self.se_reduction,\n",
    "                )\n",
    "            )\n",
    "            input_channel = out\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def _build_final_layers(self, last_channel, num_classes):\n",
    "        \"\"\"Construct final classification blocks\"\"\"\n",
    "        if self.hybrid:\n",
    "\n",
    "            return nn.Sequential(\n",
    "                ConvBlock(\n",
    "                    self.layers_config[-1][2],\n",
    "                    960 if self.mode == \"large\" else 576,\n",
    "                    activation=Hardswish,\n",
    "                    kernel_size=1,\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960 if self.mode == \"large\" else 576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, 256),\n",
    "            )  # Reduced features for hybrid combination\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                ConvBlock(\n",
    "                    self.layers_config[-1][2],\n",
    "                    960 if self.mode == \"large\" else 576,\n",
    "                    activation=Hardswish,\n",
    "                    kernel_size=1,\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(960 if self.mode == \"large\" else 576, last_channel, 1),\n",
    "                Hardswish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            )  # if we are not using hybrid approach we can directly output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hybrid:\n",
    "            # Process Sobel-augmented image\n",
    "            sob_x = self.sobel(x)  # (batch, 5, H, W)\n",
    "            features = self.features(sob_x)\n",
    "            mobilenet_out = self.final(features)  # (batch, 256)\n",
    "\n",
    "            # Process HOG features\n",
    "            hog_features = self.hog_layer(x)  # (batch, 256)\n",
    "\n",
    "            # Combine features\n",
    "            combined = torch.cat([mobilenet_out, hog_features], dim=1)\n",
    "            return self.hog_fc(combined)  # (batch, num_classes)\n",
    "        else:\n",
    "            # Standard processing\n",
    "            x = self.features(x)\n",
    "            return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = MobileNetV3(\n",
    "    in_chn=3,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, device, trainLoader, valLoader, criterion, optimizer, n_epochs, earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dka-aki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
